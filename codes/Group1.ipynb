{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pfaTzbbQyxmW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66565,
     "status": "ok",
     "timestamp": 1721114293650,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "pfaTzbbQyxmW",
    "outputId": "42312318-40f3-4089-ccb6-00682e1d96af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') # 코랩에 구글드라이브 연동해서 안에 파일들을 사용할 수 있게끔 해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "z1s8IaNIy9om",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1721114302360,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "z1s8IaNIy9om",
    "outputId": "b072db18-180a-494d-ed2c-22247598adf8"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'drive/MyDrive/Multimodal_Fusion_Ratings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdrive/MyDrive/Multimodal_Fusion_Ratings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Multimodal_Fusion_Ratings'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('drive/MyDrive/Multimodal_Fusion_Ratings')\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1762b9",
   "metadata": {
    "id": "ac1762b9"
   },
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f58490d",
   "metadata": {
    "id": "4f58490d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/fc/a5/4d82be566f069d7a9a702dcdf6f9106df0e0b042e738043c0cc7ddd7e3f6/pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ce3aad-94e6-4846-b84e-9efc948b3ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/e7/54/0c1c068542cee73d8863336e974fc881e608d0170f3af15d0c0f28644531/pip-24.1.2-py3-none-any.whl.metadata\n",
      "  Downloading pip-24.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.1.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-24.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oafKhZhDzB_7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 1940,
     "status": "ok",
     "timestamp": 1721114320013,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "oafKhZhDzB_7",
    "outputId": "a2230e83-e822-435e-ec3e-1b8964526cbc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>transcript</th>\n",
       "      <th>CAPEI</th>\n",
       "      <th>bm</th>\n",
       "      <th>evm</th>\n",
       "      <th>pe_op_basic</th>\n",
       "      <th>pe_op_dil</th>\n",
       "      <th>pe_exi</th>\n",
       "      <th>pe_inc</th>\n",
       "      <th>ps</th>\n",
       "      <th>...</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>rect_turn</th>\n",
       "      <th>pay_turn</th>\n",
       "      <th>sale_invcap</th>\n",
       "      <th>sale_equity</th>\n",
       "      <th>rd_sale</th>\n",
       "      <th>adv_sale</th>\n",
       "      <th>staff_sale</th>\n",
       "      <th>accrual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BBB</td>\n",
       "      <td>On the earnings call for AIR on 2017-05-31, th...</td>\n",
       "      <td>36.554125</td>\n",
       "      <td>0.773976</td>\n",
       "      <td>12.839974</td>\n",
       "      <td>26.074627</td>\n",
       "      <td>26.469697</td>\n",
       "      <td>25.318841</td>\n",
       "      <td>25.503650</td>\n",
       "      <td>0.745404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682116</td>\n",
       "      <td>1.098638</td>\n",
       "      <td>6.298297</td>\n",
       "      <td>7.836994</td>\n",
       "      <td>1.575769</td>\n",
       "      <td>1.848036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBB</td>\n",
       "      <td>AIR reported strong quarterly earnings, exceed...</td>\n",
       "      <td>29.819586</td>\n",
       "      <td>0.702109</td>\n",
       "      <td>11.728077</td>\n",
       "      <td>12.382716</td>\n",
       "      <td>12.589958</td>\n",
       "      <td>13.995349</td>\n",
       "      <td>-273.545455</td>\n",
       "      <td>0.537637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679062</td>\n",
       "      <td>1.263504</td>\n",
       "      <td>7.719933</td>\n",
       "      <td>8.508922</td>\n",
       "      <td>1.751227</td>\n",
       "      <td>2.121500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCC</td>\n",
       "      <td>During the earnings call on 2021-05-31, AIR re...</td>\n",
       "      <td>27.840428</td>\n",
       "      <td>0.663635</td>\n",
       "      <td>13.604304</td>\n",
       "      <td>61.397059</td>\n",
       "      <td>62.313433</td>\n",
       "      <td>90.760870</td>\n",
       "      <td>298.214286</td>\n",
       "      <td>0.899017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959270</td>\n",
       "      <td>0.923419</td>\n",
       "      <td>7.189918</td>\n",
       "      <td>6.856839</td>\n",
       "      <td>1.268180</td>\n",
       "      <td>1.809227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.021715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BBB</td>\n",
       "      <td>AIR reported strong Q1 earnings with revenue u...</td>\n",
       "      <td>27.336809</td>\n",
       "      <td>0.573127</td>\n",
       "      <td>11.099339</td>\n",
       "      <td>18.090253</td>\n",
       "      <td>18.355311</td>\n",
       "      <td>19.806324</td>\n",
       "      <td>19.728346</td>\n",
       "      <td>0.909511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555582</td>\n",
       "      <td>1.178007</td>\n",
       "      <td>6.189351</td>\n",
       "      <td>9.404594</td>\n",
       "      <td>1.534240</td>\n",
       "      <td>1.832487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A+</td>\n",
       "      <td>During the earnings call, AAL reported strong ...</td>\n",
       "      <td>13.187465</td>\n",
       "      <td>0.164853</td>\n",
       "      <td>5.946422</td>\n",
       "      <td>9.151229</td>\n",
       "      <td>9.185958</td>\n",
       "      <td>11.232019</td>\n",
       "      <td>11.232019</td>\n",
       "      <td>0.587852</td>\n",
       "      <td>...</td>\n",
       "      <td>11.923173</td>\n",
       "      <td>0.787441</td>\n",
       "      <td>25.808812</td>\n",
       "      <td>15.082781</td>\n",
       "      <td>1.561108</td>\n",
       "      <td>10.176231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.304206</td>\n",
       "      <td>-0.074970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ratings                                         transcript      CAPEI  \\\n",
       "0     BBB  On the earnings call for AIR on 2017-05-31, th...  36.554125   \n",
       "1     BBB  AIR reported strong quarterly earnings, exceed...  29.819586   \n",
       "2     CCC  During the earnings call on 2021-05-31, AIR re...  27.840428   \n",
       "3     BBB  AIR reported strong Q1 earnings with revenue u...  27.336809   \n",
       "4      A+  During the earnings call, AAL reported strong ...  13.187465   \n",
       "\n",
       "         bm        evm  pe_op_basic  pe_op_dil     pe_exi      pe_inc  \\\n",
       "0  0.773976  12.839974    26.074627  26.469697  25.318841   25.503650   \n",
       "1  0.702109  11.728077    12.382716  12.589958  13.995349 -273.545455   \n",
       "2  0.663635  13.604304    61.397059  62.313433  90.760870  298.214286   \n",
       "3  0.573127  11.099339    18.090253  18.355311  19.806324   19.728346   \n",
       "4  0.164853   5.946422     9.151229   9.185958  11.232019   11.232019   \n",
       "\n",
       "         ps  ...   de_ratio   at_turn  rect_turn   pay_turn  sale_invcap  \\\n",
       "0  0.745404  ...   0.682116  1.098638   6.298297   7.836994     1.575769   \n",
       "1  0.537637  ...   0.679062  1.263504   7.719933   8.508922     1.751227   \n",
       "2  0.899017  ...   0.959270  0.923419   7.189918   6.856839     1.268180   \n",
       "3  0.909511  ...   0.555582  1.178007   6.189351   9.404594     1.534240   \n",
       "4  0.587852  ...  11.923173  0.787441  25.808812  15.082781     1.561108   \n",
       "\n",
       "   sale_equity  rd_sale  adv_sale  staff_sale   accrual  \n",
       "0     1.848036      0.0  0.000000    0.000000  0.008605  \n",
       "1     2.121500      0.0  0.000000    0.000000  0.012750  \n",
       "2     1.809227      0.0  0.000000    0.000000 -0.021715  \n",
       "3     1.832487      0.0  0.000000    0.000000  0.044935  \n",
       "4    10.176231      0.0  0.002887    0.304206 -0.074970  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수집한 데이터 load\n",
    "\n",
    "import pandas as pd\n",
    "data_main=pd.read_csv(\"data.csv\")\n",
    "data_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e8775",
   "metadata": {
    "id": "ce1e8775"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7lxY_HtDvh",
   "metadata": {
    "executionInfo": {
     "elapsed": 3820,
     "status": "ok",
     "timestamp": 1721114360724,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "1d7lxY_HtDvh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 05:50:06.844260: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-17 05:50:06.844329: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-17 05:50:06.844372: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-17 05:50:06.854171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 05:50:08.587297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22455 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# data_main=pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)\n",
    "\n",
    "# Set multiple gpus\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26cf70e7",
   "metadata": {
    "id": "26cf70e7"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525b478a",
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1721114391955,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "525b478a"
   },
   "outputs": [],
   "source": [
    "#change of the rating classes to new classes\n",
    "import numpy as np\n",
    "\n",
    "lbl_first = data_main[\"ratings\"].values\n",
    "lbl_final = lbl_first.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08332416",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721114394063,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "08332416"
   },
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(lbl_final, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacece2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1721114396359,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "eacece2c",
    "outputId": "e77ef4e6-2024-41db-cbdf-c2da02f31bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: A+, Count: 324\n",
      "Value: A-, Count: 94\n",
      "Value: AA, Count: 65\n",
      "Value: AAA, Count: 11\n",
      "Value: B, Count: 1786\n",
      "Value: B+, Count: 48\n",
      "Value: B-, Count: 42\n",
      "Value: BBB, Count: 6122\n",
      "Value: BBB+, Count: 1\n",
      "Value: C, Count: 17\n",
      "Value: CCC, Count: 2805\n",
      "Value: D, Count: 30\n"
     ]
    }
   ],
   "source": [
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-f2eJ3gUtGOZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1721114830521,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "-f2eJ3gUtGOZ"
   },
   "outputs": [],
   "source": [
    "# 각 class의 비중을 다소 균등하게 조절하기 위해 신규 class로 묶어주기\n",
    "dict_trans_class={0: ['AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-'],\n",
    "                  1: ['BBB+', 'BBB'],\n",
    "                  2: ['BBB-', 'BB+', 'BB', 'BB-', 'B+', 'B', 'B-'],\n",
    "                  3: ['CCC', 'CCC-', 'C', 'D']\n",
    "                  }\n",
    "\n",
    "# dict_trans_class={1:[1,2,3,4,5],\n",
    "#                  10:[10,11,12],\n",
    "#                   13:[13,14,15],\n",
    "#                   16:[16,17,18,19,20,21,22]\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "SpXR1-M1tHNa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1721114831443,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "SpXR1-M1tHNa",
    "outputId": "9666fa32-1159-4087-f01f-d81f2df57b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 ... 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "# 해당하는 기존 class를 신규 class로 변경\n",
    "for class_main_idx in dict_trans_class:\n",
    "    lst_class = dict_trans_class[class_main_idx]\n",
    "    for item in lst_class:\n",
    "        lbl_final = np.where(lbl_first == item, class_main_idx, lbl_final)\n",
    "\n",
    "print(lbl_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ncMnWFMPtI1B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1721114838853,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "ncMnWFMPtI1B",
    "outputId": "053fbb22-ef6b-48f8-b477-06c7b167da89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[1 1 3 ... 2 3 1]\n",
      "[[0 494]\n",
      " [1 6123]\n",
      " [2 1876]\n",
      " [3 2852]]\n"
     ]
    }
   ],
   "source": [
    "# 신규 class 별 숫자를 확인\n",
    "lbl_final_rest = lbl_final.copy()\n",
    "uni_class = np.unique(lbl_final)\n",
    "print(uni_class)\n",
    "for idx, item in enumerate(uni_class):\n",
    "    lbl_final_rest=np.where(lbl_final == item, idx, lbl_final_rest)\n",
    "print(lbl_final_rest)\n",
    "unique, counts = np.unique(lbl_final_rest, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Gd4S-3kKtKcB",
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1721114885863,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "Gd4S-3kKtKcB"
   },
   "outputs": [],
   "source": [
    "# 더 진행하기 전에 기존 데이터 카피를 만들어주고, text를 토큰으로 받기\n",
    "data_pre = data_main.copy()\n",
    "data_token = data_pre[\"transcript\"].values # type = np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "xTmdtjX4zgLm",
   "metadata": {
    "id": "xTmdtjX4zgLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0 (from clean-text)\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy<7.0,>=6.0 (from clean-text)\n",
      "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy<7.0,>=6.0->clean-text)\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=8cbad4e7f1f7c413b319a7e6a0aa278915fb138d692de66b9b543209fa9a264e\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/22/e5/b69726d5e1a19795ecd3b3e7464b16c0f1d019aa94ff1c8578\n",
      "Successfully built emoji\n",
      "Installing collected packages: wcwidth, emoji, ftfy, clean-text\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.2.0 wcwidth-0.2.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "D5ajCJX0cTgc",
   "metadata": {
    "id": "D5ajCJX0cTgc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.8.1 regex-2024.5.15 tqdm-4.66.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "417f0d01-f71d-4a2b-a385-4fe7f10b8c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.1 scipy-1.14.0 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9kv5cUBftMGK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6105,
     "status": "ok",
     "timestamp": 1721114932594,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "9kv5cUBftMGK",
    "outputId": "90d7d004-d995-41b7-8d6a-4ea3d38b49ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "##########################Preprocessing\n",
    "\n",
    "# import necessary packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "zXZ1Jg0P1ZEN",
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1721114962995,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "zXZ1Jg0P1ZEN"
   },
   "outputs": [],
   "source": [
    "# get lemmatized review\n",
    "def get_lemmatized_text(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "# get stemmed review\n",
    "def get_stemmed_text(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "p_PwR3tWtY8Z",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1721114963324,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "p_PwR3tWtY8Z"
   },
   "outputs": [],
   "source": [
    "# remove all stopwords in english review\n",
    "def remove_stop_words(corpus):\n",
    "    english_stop_words = stopwords.words('english')\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split()\n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "qgOhnTl-teRj",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1721114964377,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "qgOhnTl-teRj"
   },
   "outputs": [],
   "source": [
    "# preprocess review\n",
    "def preprocess_txt(reviews):\n",
    "    REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    NO_SPACE = \"\"\n",
    "    SPACE = \" \"\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, reviews.lower())]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74wjSj720Llv",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721114965770,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "74wjSj720Llv"
   },
   "outputs": [],
   "source": [
    "# clean review by cleantext libaray\n",
    "def clean_text(sent):\n",
    "    clean_sent=clean(sent,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=True,                     # lowercase text\n",
    "        no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "        no_numbers=False,               # replace all numbers with a special token\n",
    "        no_digits=True,                # replace all digits with a special token\n",
    "        no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "        no_punct=True,                 # remove punctuations\n",
    "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "        lang=\"en\" )\n",
    "    return clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7Lji7MPT0Nsw",
   "metadata": {
    "executionInfo": {
     "elapsed": 16103,
     "status": "ok",
     "timestamp": 1721114994423,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "7Lji7MPT0Nsw"
   },
   "outputs": [],
   "source": [
    "# 위에서 만든 전처리 함수들을 텍스트에 적용\n",
    "data_pre[\"transcript\"] = data_pre[\"transcript\"].apply(lambda x:clean_text(x))\n",
    "data_pre[\"transcript\"] = data_pre[\"transcript\"].apply(lambda x:preprocess_txt(x))\n",
    "data_pre[\"transcript\"] = data_pre[\"transcript\"].apply(lambda x:remove_stop_words(x))\n",
    "data_pre[\"transcript\"] = data_pre[\"transcript\"].apply(lambda x:get_stemmed_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "915f53c0",
   "metadata": {
    "id": "915f53c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras-Preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from Keras-Preprocessing) (1.26.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
      "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Keras-Preprocessing\n",
      "Successfully installed Keras-Preprocessing-1.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76OIy8v11FI3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1736,
     "status": "ok",
     "timestamp": 1721115022320,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "76OIy8v11FI3"
   },
   "outputs": [],
   "source": [
    "#############################Tokenization -> 정리된 텍스트 데이터를 tokenizer를 이용해 숫자로 변형 후 패딩으로 길이 맞춰주기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "text = data_token # data_token 은 string_values 열의 값을 np array로 받은 것\n",
    "data_token_re = data_token.copy()\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "MAX_NB_WORDS=500000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_token_re)\n",
    "data_seq = tokenizer.texts_to_sequences(data_token_re)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 5000\n",
    "data_seq_pad = pad_sequences(data_seq, padding='pre', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "_p4586zd1I0A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1721115433105,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "_p4586zd1I0A",
    "outputId": "475adb68-3ff2-4a61-cae2-f99c43833939"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11345, 56)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################Numeric part\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "lbl_binary = to_categorical(lbl_final_rest).astype(int)\n",
    "\n",
    "cat = data_main.loc[:, ['ratings']] # categorical 데이터 추출\n",
    "dum = pd.get_dummies(cat, drop_first=True) # dummy 생성\n",
    "dum = dum.astype(int)\n",
    "# 4:101:market, 102:110:bond info, 111:157:finRatio\n",
    "# num = data_main[list(data_main.iloc[:,2:47])] # numerical 데이터 추출\n",
    "num = data_main.iloc[:,2:47] # numerical 데이터 추출\n",
    "\n",
    "# data_meta=num.drop('dater',axis=1)\n",
    "data_meta = num.copy()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data_meta_normal=data_meta.copy()\n",
    "sce=MinMaxScaler()\n",
    "for item_col in data_meta_normal.columns: # minmaxscaler 를 통해 numerical 컬럼들을 normailize\n",
    "    data_meta_normal[item_col]=sce.fit_transform(data_meta[item_col].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "data_meta_normal=pd.concat([dum,data_meta_normal],axis=1) # categorical(더미) 와 numerical 컬럼들을 합친다\n",
    "\n",
    "data_meta_normal[\"lbl\"]=lbl_final_rest # lbl 컬럼 추가 (전에 만들었던 lbl_final_rest 사용)\n",
    "\n",
    "\n",
    "meta_normal=data_meta_normal.values[:,:-1] # meta_normal을 np.array로 받기\n",
    "meta_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "yFaEFba7_abo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1721115444831,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "yFaEFba7_abo",
    "outputId": "3758fa36-20ed-42e5-e81b-9df0cbaa51e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "train shape: (9076, 5000)\n",
      "test shape: (2269, 5000)\n",
      "x_train_txt shape: (9076,)\n",
      "x_train_meta shape: (9076, 56, 1)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import  to_categorical\n",
    "lbl_binary=to_categorical(lbl_final_rest).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train split mode\n",
    "trian_Percentage=0.80;\n",
    "\n",
    "# padding 진행한 data_seq (string data) 를 train/test split\n",
    "x_train, x_test, lbl_train, lbl_test=train_test_split(data_seq_pad,\n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "# tokenized 데이터를 train/test split\n",
    "x_train_txt, x_test_txt, lbl_train, lbl_test=train_test_split(data_token,\n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "# 병합한 meta data를 train/test split\n",
    "x_train_meta, x_test_meta, lbl_train, lbl_test=train_test_split(meta_normal,\n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "\n",
    "# CNN layer 에 넣기위해 2D -> 3D로 reshape (마지막에 1 추가)\n",
    "x_train_meta_re=x_train_meta.reshape(x_train_meta.shape[0],\n",
    "                                  x_train_meta.shape[1],1)\n",
    "x_test_meta_re=x_test_meta.reshape(x_test_meta.shape[0],\n",
    "                                x_test_meta.shape[1],1)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"train shape: {0}\".format(x_train.shape))\n",
    "print(\"test shape: {0}\".format(x_test.shape))\n",
    "print(\"x_train_txt shape: {0}\".format(x_train_txt.shape))\n",
    "print(\"x_train_meta shape: {0}\".format(x_train_meta_re.shape))\n",
    "print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46bf33bc",
   "metadata": {
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1721115449840,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "46bf33bc"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_train_meta = x_train_meta.astype(np.float32)\n",
    "x_test_meta = x_test_meta.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b_pbiAUJCXx9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1721115453626,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "b_pbiAUJCXx9",
    "outputId": "a92b4274-853d-4829-dfe9-87d9e35f9563"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11345, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "KX5KbhbIzhhx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1721115502473,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "KX5KbhbIzhhx",
    "outputId": "d03f4eeb-c6a2-486e-de0e-4b4c9d3ede0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6.233516483516484,\n",
       " 1: 0.468801652892562,\n",
       " 2: 1.492763157894737,\n",
       " 3: 0.9647108843537415}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=500\n",
    "n_class=lbl_binary.shape[1]\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights_val = class_weight.compute_class_weight(class_weight='balanced',    # 데이터가 balanced 되지 않아서, 더 잘 훈련하기 위한 가중치 생성\n",
    "                                                 classes=np.unique(np.argmax(lbl_test,axis=1)),\n",
    "                                                 y=np.argmax(lbl_test,axis=1))\n",
    "\n",
    "class_weight=dict()\n",
    "for idx,val in enumerate(class_weights_val):\n",
    "    class_weight[idx]=val\n",
    "\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "YffDvxN4_tI1",
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1721115525516,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "YffDvxN4_tI1"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'),\n",
    "      keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "deKVGi0q_yyW",
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1721115686129,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "deKVGi0q_yyW"
   },
   "outputs": [],
   "source": [
    "######################Model lstm\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def Create_LSTM(data,nb_words,maxlen,embedding_dim,data_meta):\n",
    "\n",
    "\n",
    "\n",
    "#meta input\n",
    "  input_meta = keras.layers.Input(shape=data_meta.shape[1:],dtype='float64',name=\"input_meta\")\n",
    "  layer_meta = keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta.1\")(input_meta)\n",
    "  layer_meta = keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "\n",
    "  layer_meta = keras.layers.LSTM(32)(layer_meta) # return_sequences=False\n",
    "\n",
    "\n",
    "  # txt input\n",
    "  word_input_cnn = keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")\n",
    "  em_layer_cnn = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data = keras.layers.MaxPool1D(pool_size=(2),name=\"maxpool1\")(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data = keras.layers.Conv1D(filters=32,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.1)(layer_data)\n",
    "\n",
    "  layer_data = keras.layers.Conv1D(filters=16,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_3\")(layer_data)\n",
    "\n",
    "  layer_data = keras.layers.MaxPool1D(pool_size=(2),name=\"maxpool2\")(layer_data)\n",
    "  layer_data = keras.layers.LSTM(32)(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.5)(layer_data)\n",
    "\n",
    "\n",
    "  # hybrid\n",
    "  layer_last = keras.layers.Concatenate(axis=-1)([layer_meta,layer_data])\n",
    "\n",
    "  layer_last = keras.layers.Flatten()(layer_last)\n",
    "  layer_last = keras.layers.Dense(32,activation=\"relu\",name=\"dens_1\")(layer_last)\n",
    "  layer_last = keras.layers.Dropout(0.5)(layer_last)\n",
    "  output = keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.00001)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=METRICS)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nzkDq8b0OqAd",
   "metadata": {
    "executionInfo": {
     "elapsed": 1789,
     "status": "ok",
     "timestamp": 1721115690749,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "nzkDq8b0OqAd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 10:20:42.831396: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 위에 만든 Create_LSTM 함수를 사용하여 모델 생성 후 컴파일\n",
    "LSTM_embedding=Create_LSTM(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta=x_train_meta_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "GA7J9ixwO0RJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 39229,
     "status": "error",
     "timestamp": 1721115734724,
     "user": {
      "displayName": "Dohyung Kim",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "GA7J9ixwO0RJ",
    "outputId": "0349f89a-fd9f-48c8-9e44-91a73fe59dcd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8922 - tp: 1433.0000 - fp: 1341.0000 - tn: 25887.0000 - fn: 7643.0000 - accuracy: 0.4950 - precision: 0.5166 - recall: 0.1579 - auc: 0.7836 - prc: 0.4902 - val_loss: 1.2939 - val_tp: 79.0000 - val_fp: 239.0000 - val_tn: 6568.0000 - val_fn: 2190.0000 - val_accuracy: 0.3975 - val_precision: 0.2484 - val_recall: 0.0348 - val_auc: 0.6636 - val_prc: 0.3446\n",
      "Epoch 2/200\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.8913 - tp: 1511.0000 - fp: 1407.0000 - tn: 25821.0000 - fn: 7565.0000 - accuracy: 0.5004 - precision: 0.5178 - recall: 0.1665 - auc: 0.7836 - prc: 0.4919 - val_loss: 1.3032 - val_tp: 81.0000 - val_fp: 259.0000 - val_tn: 6548.0000 - val_fn: 2188.0000 - val_accuracy: 0.3984 - val_precision: 0.2382 - val_recall: 0.0357 - val_auc: 0.6613 - val_prc: 0.3420\n",
      "Epoch 3/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.8995 - tp: 1495.0000 - fp: 1456.0000 - tn: 25772.0000 - fn: 7581.0000 - accuracy: 0.5017 - precision: 0.5066 - recall: 0.1647 - auc: 0.7821 - prc: 0.4876 - val_loss: 1.2957 - val_tp: 81.0000 - val_fp: 245.0000 - val_tn: 6562.0000 - val_fn: 2188.0000 - val_accuracy: 0.4006 - val_precision: 0.2485 - val_recall: 0.0357 - val_auc: 0.6649 - val_prc: 0.3461\n",
      "Epoch 4/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.8875 - tp: 1546.0000 - fp: 1377.0000 - tn: 25851.0000 - fn: 7530.0000 - accuracy: 0.5131 - precision: 0.5289 - recall: 0.1703 - auc: 0.7907 - prc: 0.5002 - val_loss: 1.2987 - val_tp: 85.0000 - val_fp: 258.0000 - val_tn: 6549.0000 - val_fn: 2184.0000 - val_accuracy: 0.3993 - val_precision: 0.2478 - val_recall: 0.0375 - val_auc: 0.6648 - val_prc: 0.3462\n",
      "Epoch 5/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.8840 - tp: 1571.0000 - fp: 1385.0000 - tn: 25843.0000 - fn: 7505.0000 - accuracy: 0.5127 - precision: 0.5315 - recall: 0.1731 - auc: 0.7907 - prc: 0.5020 - val_loss: 1.2971 - val_tp: 100.0000 - val_fp: 257.0000 - val_tn: 6550.0000 - val_fn: 2169.0000 - val_accuracy: 0.3997 - val_precision: 0.2801 - val_recall: 0.0441 - val_auc: 0.6657 - val_prc: 0.3479\n",
      "Epoch 6/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8833 - tp: 1583.0000 - fp: 1431.0000 - tn: 25797.0000 - fn: 7493.0000 - accuracy: 0.5102 - precision: 0.5252 - recall: 0.1744 - auc: 0.7894 - prc: 0.4997 - val_loss: 1.2896 - val_tp: 102.0000 - val_fp: 246.0000 - val_tn: 6561.0000 - val_fn: 2167.0000 - val_accuracy: 0.4055 - val_precision: 0.2931 - val_recall: 0.0450 - val_auc: 0.6697 - val_prc: 0.3525\n",
      "Epoch 7/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8821 - tp: 1574.0000 - fp: 1397.0000 - tn: 25831.0000 - fn: 7502.0000 - accuracy: 0.5110 - precision: 0.5298 - recall: 0.1734 - auc: 0.7915 - prc: 0.5040 - val_loss: 1.2907 - val_tp: 106.0000 - val_fp: 256.0000 - val_tn: 6551.0000 - val_fn: 2163.0000 - val_accuracy: 0.4086 - val_precision: 0.2928 - val_recall: 0.0467 - val_auc: 0.6692 - val_prc: 0.3521\n",
      "Epoch 8/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8645 - tp: 1715.0000 - fp: 1352.0000 - tn: 25876.0000 - fn: 7361.0000 - accuracy: 0.5325 - precision: 0.5592 - recall: 0.1890 - auc: 0.8025 - prc: 0.5210 - val_loss: 1.2971 - val_tp: 113.0000 - val_fp: 268.0000 - val_tn: 6539.0000 - val_fn: 2156.0000 - val_accuracy: 0.4072 - val_precision: 0.2966 - val_recall: 0.0498 - val_auc: 0.6684 - val_prc: 0.3513\n",
      "Epoch 9/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8645 - tp: 1720.0000 - fp: 1449.0000 - tn: 25779.0000 - fn: 7356.0000 - accuracy: 0.5279 - precision: 0.5428 - recall: 0.1895 - auc: 0.8001 - prc: 0.5170 - val_loss: 1.2957 - val_tp: 111.0000 - val_fp: 263.0000 - val_tn: 6544.0000 - val_fn: 2158.0000 - val_accuracy: 0.4090 - val_precision: 0.2968 - val_recall: 0.0489 - val_auc: 0.6699 - val_prc: 0.3533\n",
      "Epoch 10/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8550 - tp: 1826.0000 - fp: 1364.0000 - tn: 25864.0000 - fn: 7250.0000 - accuracy: 0.5378 - precision: 0.5724 - recall: 0.2012 - auc: 0.8059 - prc: 0.5310 - val_loss: 1.2942 - val_tp: 111.0000 - val_fp: 265.0000 - val_tn: 6542.0000 - val_fn: 2158.0000 - val_accuracy: 0.4099 - val_precision: 0.2952 - val_recall: 0.0489 - val_auc: 0.6716 - val_prc: 0.3554\n",
      "Epoch 11/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8529 - tp: 1862.0000 - fp: 1443.0000 - tn: 25785.0000 - fn: 7214.0000 - accuracy: 0.5445 - precision: 0.5634 - recall: 0.2052 - auc: 0.8071 - prc: 0.5287 - val_loss: 1.2955 - val_tp: 117.0000 - val_fp: 270.0000 - val_tn: 6537.0000 - val_fn: 2152.0000 - val_accuracy: 0.4086 - val_precision: 0.3023 - val_recall: 0.0516 - val_auc: 0.6719 - val_prc: 0.3562\n",
      "Epoch 12/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8474 - tp: 1892.0000 - fp: 1422.0000 - tn: 25806.0000 - fn: 7184.0000 - accuracy: 0.5389 - precision: 0.5709 - recall: 0.2085 - auc: 0.8077 - prc: 0.5320 - val_loss: 1.2925 - val_tp: 118.0000 - val_fp: 264.0000 - val_tn: 6543.0000 - val_fn: 2151.0000 - val_accuracy: 0.4147 - val_precision: 0.3089 - val_recall: 0.0520 - val_auc: 0.6746 - val_prc: 0.3597\n",
      "Epoch 13/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8591 - tp: 1956.0000 - fp: 1474.0000 - tn: 25754.0000 - fn: 7120.0000 - accuracy: 0.5340 - precision: 0.5703 - recall: 0.2155 - auc: 0.8063 - prc: 0.5262 - val_loss: 1.2967 - val_tp: 122.0000 - val_fp: 273.0000 - val_tn: 6534.0000 - val_fn: 2147.0000 - val_accuracy: 0.4147 - val_precision: 0.3089 - val_recall: 0.0538 - val_auc: 0.6739 - val_prc: 0.3593\n",
      "Epoch 14/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.8488 - tp: 1985.0000 - fp: 1528.0000 - tn: 25700.0000 - fn: 7091.0000 - accuracy: 0.5436 - precision: 0.5650 - recall: 0.2187 - auc: 0.8105 - prc: 0.5326 - val_loss: 1.3040 - val_tp: 122.0000 - val_fp: 286.0000 - val_tn: 6521.0000 - val_fn: 2147.0000 - val_accuracy: 0.4103 - val_precision: 0.2990 - val_recall: 0.0538 - val_auc: 0.6723 - val_prc: 0.3577\n",
      "Epoch 15/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.8561 - tp: 1985.0000 - fp: 1540.0000 - tn: 25688.0000 - fn: 7091.0000 - accuracy: 0.5398 - precision: 0.5631 - recall: 0.2187 - auc: 0.8077 - prc: 0.5299 - val_loss: 1.3107 - val_tp: 118.0000 - val_fp: 299.0000 - val_tn: 6508.0000 - val_fn: 2151.0000 - val_accuracy: 0.4099 - val_precision: 0.2830 - val_recall: 0.0520 - val_auc: 0.6715 - val_prc: 0.3564\n",
      "Epoch 16/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8456 - tp: 1965.0000 - fp: 1566.0000 - tn: 25662.0000 - fn: 7111.0000 - accuracy: 0.5410 - precision: 0.5565 - recall: 0.2165 - auc: 0.8083 - prc: 0.5276 - val_loss: 1.2989 - val_tp: 118.0000 - val_fp: 281.0000 - val_tn: 6526.0000 - val_fn: 2151.0000 - val_accuracy: 0.4156 - val_precision: 0.2957 - val_recall: 0.0520 - val_auc: 0.6761 - val_prc: 0.3619\n",
      "Epoch 17/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8321 - tp: 2138.0000 - fp: 1513.0000 - tn: 25715.0000 - fn: 6938.0000 - accuracy: 0.5580 - precision: 0.5856 - recall: 0.2356 - auc: 0.8189 - prc: 0.5490 - val_loss: 1.2967 - val_tp: 122.0000 - val_fp: 279.0000 - val_tn: 6528.0000 - val_fn: 2147.0000 - val_accuracy: 0.4160 - val_precision: 0.3042 - val_recall: 0.0538 - val_auc: 0.6780 - val_prc: 0.3644\n",
      "Epoch 18/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8441 - tp: 2177.0000 - fp: 1538.0000 - tn: 25690.0000 - fn: 6899.0000 - accuracy: 0.5606 - precision: 0.5860 - recall: 0.2399 - auc: 0.8170 - prc: 0.5452 - val_loss: 1.3035 - val_tp: 136.0000 - val_fp: 294.0000 - val_tn: 6513.0000 - val_fn: 2133.0000 - val_accuracy: 0.4152 - val_precision: 0.3163 - val_recall: 0.0599 - val_auc: 0.6755 - val_prc: 0.3620\n",
      "Epoch 19/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8351 - tp: 2148.0000 - fp: 1515.0000 - tn: 25713.0000 - fn: 6928.0000 - accuracy: 0.5567 - precision: 0.5864 - recall: 0.2367 - auc: 0.8164 - prc: 0.5445 - val_loss: 1.3067 - val_tp: 121.0000 - val_fp: 294.0000 - val_tn: 6513.0000 - val_fn: 2148.0000 - val_accuracy: 0.4138 - val_precision: 0.2916 - val_recall: 0.0533 - val_auc: 0.6770 - val_prc: 0.3634\n",
      "Epoch 20/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8289 - tp: 2264.0000 - fp: 1505.0000 - tn: 25723.0000 - fn: 6812.0000 - accuracy: 0.5610 - precision: 0.6007 - recall: 0.2494 - auc: 0.8216 - prc: 0.5557 - val_loss: 1.2967 - val_tp: 131.0000 - val_fp: 282.0000 - val_tn: 6525.0000 - val_fn: 2138.0000 - val_accuracy: 0.4213 - val_precision: 0.3172 - val_recall: 0.0577 - val_auc: 0.6802 - val_prc: 0.3675\n",
      "Epoch 21/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8264 - tp: 2273.0000 - fp: 1524.0000 - tn: 25704.0000 - fn: 6803.0000 - accuracy: 0.5658 - precision: 0.5986 - recall: 0.2504 - auc: 0.8198 - prc: 0.5540 - val_loss: 1.3013 - val_tp: 142.0000 - val_fp: 295.0000 - val_tn: 6512.0000 - val_fn: 2127.0000 - val_accuracy: 0.4169 - val_precision: 0.3249 - val_recall: 0.0626 - val_auc: 0.6790 - val_prc: 0.3666\n",
      "Epoch 22/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8204 - tp: 2350.0000 - fp: 1472.0000 - tn: 25756.0000 - fn: 6726.0000 - accuracy: 0.5744 - precision: 0.6149 - recall: 0.2589 - auc: 0.8272 - prc: 0.5649 - val_loss: 1.2984 - val_tp: 145.0000 - val_fp: 290.0000 - val_tn: 6517.0000 - val_fn: 2124.0000 - val_accuracy: 0.4178 - val_precision: 0.3333 - val_recall: 0.0639 - val_auc: 0.6805 - val_prc: 0.3690\n",
      "Epoch 23/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.8116 - tp: 2449.0000 - fp: 1525.0000 - tn: 25703.0000 - fn: 6627.0000 - accuracy: 0.5726 - precision: 0.6163 - recall: 0.2698 - auc: 0.8284 - prc: 0.5689 - val_loss: 1.2974 - val_tp: 144.0000 - val_fp: 290.0000 - val_tn: 6517.0000 - val_fn: 2125.0000 - val_accuracy: 0.4204 - val_precision: 0.3318 - val_recall: 0.0635 - val_auc: 0.6822 - val_prc: 0.3713\n",
      "Epoch 24/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8165 - tp: 2483.0000 - fp: 1516.0000 - tn: 25712.0000 - fn: 6593.0000 - accuracy: 0.5790 - precision: 0.6209 - recall: 0.2736 - auc: 0.8311 - prc: 0.5716 - val_loss: 1.3058 - val_tp: 153.0000 - val_fp: 316.0000 - val_tn: 6491.0000 - val_fn: 2116.0000 - val_accuracy: 0.4152 - val_precision: 0.3262 - val_recall: 0.0674 - val_auc: 0.6802 - val_prc: 0.3695\n",
      "Epoch 25/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.8201 - tp: 2496.0000 - fp: 1564.0000 - tn: 25664.0000 - fn: 6580.0000 - accuracy: 0.5708 - precision: 0.6148 - recall: 0.2750 - auc: 0.8270 - prc: 0.5657 - val_loss: 1.3042 - val_tp: 165.0000 - val_fp: 309.0000 - val_tn: 6498.0000 - val_fn: 2104.0000 - val_accuracy: 0.4182 - val_precision: 0.3481 - val_recall: 0.0727 - val_auc: 0.6817 - val_prc: 0.3721\n",
      "Epoch 26/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.8004 - tp: 2653.0000 - fp: 1508.0000 - tn: 25720.0000 - fn: 6423.0000 - accuracy: 0.5870 - precision: 0.6376 - recall: 0.2923 - auc: 0.8362 - prc: 0.5817 - val_loss: 1.3052 - val_tp: 234.0000 - val_fp: 322.0000 - val_tn: 6485.0000 - val_fn: 2035.0000 - val_accuracy: 0.4178 - val_precision: 0.4209 - val_recall: 0.1031 - val_auc: 0.6826 - val_prc: 0.3736\n",
      "Epoch 27/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.7930 - tp: 2660.0000 - fp: 1505.0000 - tn: 25723.0000 - fn: 6416.0000 - accuracy: 0.5874 - precision: 0.6387 - recall: 0.2931 - auc: 0.8381 - prc: 0.5861 - val_loss: 1.3167 - val_tp: 241.0000 - val_fp: 344.0000 - val_tn: 6463.0000 - val_fn: 2028.0000 - val_accuracy: 0.4108 - val_precision: 0.4120 - val_recall: 0.1062 - val_auc: 0.6798 - val_prc: 0.3711\n",
      "Epoch 28/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7945 - tp: 2716.0000 - fp: 1576.0000 - tn: 25652.0000 - fn: 6360.0000 - accuracy: 0.5876 - precision: 0.6328 - recall: 0.2993 - auc: 0.8374 - prc: 0.5862 - val_loss: 1.3067 - val_tp: 309.0000 - val_fp: 364.0000 - val_tn: 6443.0000 - val_fn: 1960.0000 - val_accuracy: 0.4160 - val_precision: 0.4591 - val_recall: 0.1362 - val_auc: 0.6839 - val_prc: 0.3767\n",
      "Epoch 29/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7984 - tp: 2705.0000 - fp: 1615.0000 - tn: 25613.0000 - fn: 6371.0000 - accuracy: 0.5793 - precision: 0.6262 - recall: 0.2980 - auc: 0.8336 - prc: 0.5751 - val_loss: 1.3027 - val_tp: 352.0000 - val_fp: 413.0000 - val_tn: 6394.0000 - val_fn: 1917.0000 - val_accuracy: 0.4204 - val_precision: 0.4601 - val_recall: 0.1551 - val_auc: 0.6865 - val_prc: 0.3801\n",
      "Epoch 30/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7983 - tp: 2806.0000 - fp: 1494.0000 - tn: 25734.0000 - fn: 6270.0000 - accuracy: 0.5954 - precision: 0.6526 - recall: 0.3092 - auc: 0.8410 - prc: 0.5917 - val_loss: 1.2997 - val_tp: 393.0000 - val_fp: 440.0000 - val_tn: 6367.0000 - val_fn: 1876.0000 - val_accuracy: 0.4240 - val_precision: 0.4718 - val_recall: 0.1732 - val_auc: 0.6885 - val_prc: 0.3828\n",
      "Epoch 31/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.7858 - tp: 2840.0000 - fp: 1538.0000 - tn: 25690.0000 - fn: 6236.0000 - accuracy: 0.5939 - precision: 0.6487 - recall: 0.3129 - auc: 0.8407 - prc: 0.5948 - val_loss: 1.3046 - val_tp: 416.0000 - val_fp: 457.0000 - val_tn: 6350.0000 - val_fn: 1853.0000 - val_accuracy: 0.4266 - val_precision: 0.4765 - val_recall: 0.1833 - val_auc: 0.6884 - val_prc: 0.3836\n",
      "Epoch 32/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7859 - tp: 2870.0000 - fp: 1519.0000 - tn: 25709.0000 - fn: 6206.0000 - accuracy: 0.5982 - precision: 0.6539 - recall: 0.3162 - auc: 0.8433 - prc: 0.5968 - val_loss: 1.3127 - val_tp: 438.0000 - val_fp: 483.0000 - val_tn: 6324.0000 - val_fn: 1831.0000 - val_accuracy: 0.4209 - val_precision: 0.4756 - val_recall: 0.1930 - val_auc: 0.6878 - val_prc: 0.3833\n",
      "Epoch 33/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7721 - tp: 2996.0000 - fp: 1553.0000 - tn: 25675.0000 - fn: 6080.0000 - accuracy: 0.6054 - precision: 0.6586 - recall: 0.3301 - auc: 0.8478 - prc: 0.6076 - val_loss: 1.2930 - val_tp: 463.0000 - val_fp: 484.0000 - val_tn: 6323.0000 - val_fn: 1806.0000 - val_accuracy: 0.4332 - val_precision: 0.4889 - val_recall: 0.2041 - val_auc: 0.6936 - val_prc: 0.3918\n",
      "Epoch 34/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.7716 - tp: 3104.0000 - fp: 1515.0000 - tn: 25713.0000 - fn: 5972.0000 - accuracy: 0.6089 - precision: 0.6720 - recall: 0.3420 - auc: 0.8508 - prc: 0.6154 - val_loss: 1.3072 - val_tp: 484.0000 - val_fp: 514.0000 - val_tn: 6293.0000 - val_fn: 1785.0000 - val_accuracy: 0.4275 - val_precision: 0.4850 - val_recall: 0.2133 - val_auc: 0.6915 - val_prc: 0.3898\n",
      "Epoch 35/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7679 - tp: 3083.0000 - fp: 1521.0000 - tn: 25707.0000 - fn: 5993.0000 - accuracy: 0.6158 - precision: 0.6696 - recall: 0.3397 - auc: 0.8510 - prc: 0.6152 - val_loss: 1.2984 - val_tp: 509.0000 - val_fp: 505.0000 - val_tn: 6302.0000 - val_fn: 1760.0000 - val_accuracy: 0.4262 - val_precision: 0.5020 - val_recall: 0.2243 - val_auc: 0.6948 - val_prc: 0.3952\n",
      "Epoch 36/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.7693 - tp: 3140.0000 - fp: 1567.0000 - tn: 25661.0000 - fn: 5936.0000 - accuracy: 0.6092 - precision: 0.6671 - recall: 0.3460 - auc: 0.8519 - prc: 0.6156 - val_loss: 1.3010 - val_tp: 521.0000 - val_fp: 509.0000 - val_tn: 6298.0000 - val_fn: 1748.0000 - val_accuracy: 0.4266 - val_precision: 0.5058 - val_recall: 0.2296 - val_auc: 0.6952 - val_prc: 0.3965\n",
      "Epoch 37/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.7613 - tp: 3187.0000 - fp: 1600.0000 - tn: 25628.0000 - fn: 5889.0000 - accuracy: 0.6135 - precision: 0.6658 - recall: 0.3511 - auc: 0.8536 - prc: 0.6194 - val_loss: 1.3009 - val_tp: 553.0000 - val_fp: 532.0000 - val_tn: 6275.0000 - val_fn: 1716.0000 - val_accuracy: 0.4288 - val_precision: 0.5097 - val_recall: 0.2437 - val_auc: 0.6976 - val_prc: 0.4010\n",
      "Epoch 38/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.7500 - tp: 3358.0000 - fp: 1519.0000 - tn: 25709.0000 - fn: 5718.0000 - accuracy: 0.6277 - precision: 0.6885 - recall: 0.3700 - auc: 0.8615 - prc: 0.6373 - val_loss: 1.2984 - val_tp: 564.0000 - val_fp: 524.0000 - val_tn: 6283.0000 - val_fn: 1705.0000 - val_accuracy: 0.4323 - val_precision: 0.5184 - val_recall: 0.2486 - val_auc: 0.6996 - val_prc: 0.4043\n",
      "Epoch 39/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7483 - tp: 3375.0000 - fp: 1515.0000 - tn: 25713.0000 - fn: 5701.0000 - accuracy: 0.6194 - precision: 0.6902 - recall: 0.3719 - auc: 0.8596 - prc: 0.6363 - val_loss: 1.2918 - val_tp: 601.0000 - val_fp: 541.0000 - val_tn: 6266.0000 - val_fn: 1668.0000 - val_accuracy: 0.4385 - val_precision: 0.5263 - val_recall: 0.2649 - val_auc: 0.7040 - val_prc: 0.4121\n",
      "Epoch 40/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7415 - tp: 3454.0000 - fp: 1514.0000 - tn: 25714.0000 - fn: 5622.0000 - accuracy: 0.6254 - precision: 0.6952 - recall: 0.3806 - auc: 0.8630 - prc: 0.6416 - val_loss: 1.2933 - val_tp: 612.0000 - val_fp: 553.0000 - val_tn: 6254.0000 - val_fn: 1657.0000 - val_accuracy: 0.4420 - val_precision: 0.5253 - val_recall: 0.2697 - val_auc: 0.7054 - val_prc: 0.4147\n",
      "Epoch 41/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.7375 - tp: 3534.0000 - fp: 1485.0000 - tn: 25743.0000 - fn: 5542.0000 - accuracy: 0.6321 - precision: 0.7041 - recall: 0.3894 - auc: 0.8678 - prc: 0.6538 - val_loss: 1.2821 - val_tp: 636.0000 - val_fp: 554.0000 - val_tn: 6253.0000 - val_fn: 1633.0000 - val_accuracy: 0.4473 - val_precision: 0.5345 - val_recall: 0.2803 - val_auc: 0.7100 - val_prc: 0.4233\n",
      "Epoch 42/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.7316 - tp: 3634.0000 - fp: 1510.0000 - tn: 25718.0000 - fn: 5442.0000 - accuracy: 0.6397 - precision: 0.7065 - recall: 0.4004 - auc: 0.8687 - prc: 0.6555 - val_loss: 1.2801 - val_tp: 653.0000 - val_fp: 556.0000 - val_tn: 6251.0000 - val_fn: 1616.0000 - val_accuracy: 0.4504 - val_precision: 0.5401 - val_recall: 0.2878 - val_auc: 0.7132 - val_prc: 0.4300\n",
      "Epoch 43/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7272 - tp: 3637.0000 - fp: 1477.0000 - tn: 25751.0000 - fn: 5439.0000 - accuracy: 0.6384 - precision: 0.7112 - recall: 0.4007 - auc: 0.8692 - prc: 0.6570 - val_loss: 1.2781 - val_tp: 666.0000 - val_fp: 545.0000 - val_tn: 6262.0000 - val_fn: 1603.0000 - val_accuracy: 0.4513 - val_precision: 0.5500 - val_recall: 0.2935 - val_auc: 0.7153 - val_prc: 0.4338\n",
      "Epoch 44/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.7239 - tp: 3731.0000 - fp: 1454.0000 - tn: 25774.0000 - fn: 5345.0000 - accuracy: 0.6513 - precision: 0.7196 - recall: 0.4111 - auc: 0.8713 - prc: 0.6647 - val_loss: 1.2777 - val_tp: 678.0000 - val_fp: 536.0000 - val_tn: 6271.0000 - val_fn: 1591.0000 - val_accuracy: 0.4557 - val_precision: 0.5585 - val_recall: 0.2988 - val_auc: 0.7171 - val_prc: 0.4357\n",
      "Epoch 45/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.7191 - tp: 3792.0000 - fp: 1461.0000 - tn: 25767.0000 - fn: 5284.0000 - accuracy: 0.6491 - precision: 0.7219 - recall: 0.4178 - auc: 0.8754 - prc: 0.6716 - val_loss: 1.2667 - val_tp: 679.0000 - val_fp: 507.0000 - val_tn: 6300.0000 - val_fn: 1590.0000 - val_accuracy: 0.4570 - val_precision: 0.5725 - val_recall: 0.2993 - val_auc: 0.7209 - val_prc: 0.4425\n",
      "Epoch 46/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7095 - tp: 3951.0000 - fp: 1416.0000 - tn: 25812.0000 - fn: 5125.0000 - accuracy: 0.6631 - precision: 0.7362 - recall: 0.4353 - auc: 0.8818 - prc: 0.6885 - val_loss: 1.2613 - val_tp: 707.0000 - val_fp: 475.0000 - val_tn: 6332.0000 - val_fn: 1562.0000 - val_accuracy: 0.4614 - val_precision: 0.5981 - val_recall: 0.3116 - val_auc: 0.7251 - val_prc: 0.4497\n",
      "Epoch 47/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.7055 - tp: 3994.0000 - fp: 1446.0000 - tn: 25782.0000 - fn: 5082.0000 - accuracy: 0.6611 - precision: 0.7342 - recall: 0.4401 - auc: 0.8834 - prc: 0.6892 - val_loss: 1.2439 - val_tp: 727.0000 - val_fp: 504.0000 - val_tn: 6303.0000 - val_fn: 1542.0000 - val_accuracy: 0.4680 - val_precision: 0.5906 - val_recall: 0.3204 - val_auc: 0.7328 - val_prc: 0.4660\n",
      "Epoch 48/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.6897 - tp: 4121.0000 - fp: 1421.0000 - tn: 25807.0000 - fn: 4955.0000 - accuracy: 0.6701 - precision: 0.7436 - recall: 0.4541 - auc: 0.8885 - prc: 0.7038 - val_loss: 1.2665 - val_tp: 713.0000 - val_fp: 505.0000 - val_tn: 6302.0000 - val_fn: 1556.0000 - val_accuracy: 0.4689 - val_precision: 0.5854 - val_recall: 0.3142 - val_auc: 0.7261 - val_prc: 0.4459\n",
      "Epoch 49/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.6793 - tp: 4215.0000 - fp: 1364.0000 - tn: 25864.0000 - fn: 4861.0000 - accuracy: 0.6781 - precision: 0.7555 - recall: 0.4644 - auc: 0.8919 - prc: 0.7118 - val_loss: 1.2263 - val_tp: 767.0000 - val_fp: 444.0000 - val_tn: 6363.0000 - val_fn: 1502.0000 - val_accuracy: 0.4839 - val_precision: 0.6334 - val_recall: 0.3380 - val_auc: 0.7429 - val_prc: 0.4824\n",
      "Epoch 50/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.6681 - tp: 4303.0000 - fp: 1343.0000 - tn: 25885.0000 - fn: 4773.0000 - accuracy: 0.6954 - precision: 0.7621 - recall: 0.4741 - auc: 0.8987 - prc: 0.7298 - val_loss: 1.2114 - val_tp: 782.0000 - val_fp: 432.0000 - val_tn: 6375.0000 - val_fn: 1487.0000 - val_accuracy: 0.4910 - val_precision: 0.6442 - val_recall: 0.3446 - val_auc: 0.7503 - val_prc: 0.4954\n",
      "Epoch 51/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.6485 - tp: 4520.0000 - fp: 1297.0000 - tn: 25931.0000 - fn: 4556.0000 - accuracy: 0.6994 - precision: 0.7770 - recall: 0.4980 - auc: 0.9027 - prc: 0.7435 - val_loss: 1.2119 - val_tp: 818.0000 - val_fp: 488.0000 - val_tn: 6319.0000 - val_fn: 1451.0000 - val_accuracy: 0.5042 - val_precision: 0.6263 - val_recall: 0.3605 - val_auc: 0.7540 - val_prc: 0.4924\n",
      "Epoch 52/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.6308 - tp: 4645.0000 - fp: 1184.0000 - tn: 26044.0000 - fn: 4431.0000 - accuracy: 0.7162 - precision: 0.7969 - recall: 0.5118 - auc: 0.9105 - prc: 0.7597 - val_loss: 1.1873 - val_tp: 838.0000 - val_fp: 457.0000 - val_tn: 6350.0000 - val_fn: 1431.0000 - val_accuracy: 0.5165 - val_precision: 0.6471 - val_recall: 0.3693 - val_auc: 0.7653 - val_prc: 0.5130\n",
      "Epoch 53/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.6263 - tp: 4659.0000 - fp: 1217.0000 - tn: 26011.0000 - fn: 4417.0000 - accuracy: 0.7106 - precision: 0.7929 - recall: 0.5133 - auc: 0.9112 - prc: 0.7637 - val_loss: 1.1603 - val_tp: 854.0000 - val_fp: 418.0000 - val_tn: 6389.0000 - val_fn: 1415.0000 - val_accuracy: 0.5311 - val_precision: 0.6714 - val_recall: 0.3764 - val_auc: 0.7766 - val_prc: 0.5347\n",
      "Epoch 54/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.6092 - tp: 4809.0000 - fp: 1245.0000 - tn: 25983.0000 - fn: 4267.0000 - accuracy: 0.7215 - precision: 0.7944 - recall: 0.5299 - auc: 0.9163 - prc: 0.7752 - val_loss: 1.1673 - val_tp: 892.0000 - val_fp: 490.0000 - val_tn: 6317.0000 - val_fn: 1377.0000 - val_accuracy: 0.5465 - val_precision: 0.6454 - val_recall: 0.3931 - val_auc: 0.7772 - val_prc: 0.5225\n",
      "Epoch 55/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.5984 - tp: 4972.0000 - fp: 1173.0000 - tn: 26055.0000 - fn: 4104.0000 - accuracy: 0.7374 - precision: 0.8091 - recall: 0.5478 - auc: 0.9221 - prc: 0.7920 - val_loss: 1.1384 - val_tp: 910.0000 - val_fp: 448.0000 - val_tn: 6359.0000 - val_fn: 1359.0000 - val_accuracy: 0.5602 - val_precision: 0.6701 - val_recall: 0.4011 - val_auc: 0.7883 - val_prc: 0.5443\n",
      "Epoch 56/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.5808 - tp: 5047.0000 - fp: 1139.0000 - tn: 26089.0000 - fn: 4029.0000 - accuracy: 0.7384 - precision: 0.8159 - recall: 0.5561 - auc: 0.9260 - prc: 0.8044 - val_loss: 1.1193 - val_tp: 932.0000 - val_fp: 412.0000 - val_tn: 6395.0000 - val_fn: 1337.0000 - val_accuracy: 0.5813 - val_precision: 0.6935 - val_recall: 0.4108 - val_auc: 0.7982 - val_prc: 0.5601\n",
      "Epoch 57/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.5707 - tp: 5253.0000 - fp: 1121.0000 - tn: 26107.0000 - fn: 3823.0000 - accuracy: 0.7478 - precision: 0.8241 - recall: 0.5788 - auc: 0.9304 - prc: 0.8138 - val_loss: 1.0930 - val_tp: 944.0000 - val_fp: 391.0000 - val_tn: 6416.0000 - val_fn: 1325.0000 - val_accuracy: 0.5835 - val_precision: 0.7071 - val_recall: 0.4160 - val_auc: 0.8063 - val_prc: 0.5790\n",
      "Epoch 58/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.5530 - tp: 5341.0000 - fp: 1110.0000 - tn: 26118.0000 - fn: 3735.0000 - accuracy: 0.7543 - precision: 0.8279 - recall: 0.5885 - auc: 0.9340 - prc: 0.8248 - val_loss: 1.0898 - val_tp: 1015.0000 - val_fp: 423.0000 - val_tn: 6384.0000 - val_fn: 1254.0000 - val_accuracy: 0.5941 - val_precision: 0.7058 - val_recall: 0.4473 - val_auc: 0.8088 - val_prc: 0.5749\n",
      "Epoch 59/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.5403 - tp: 5502.0000 - fp: 1109.0000 - tn: 26119.0000 - fn: 3574.0000 - accuracy: 0.7618 - precision: 0.8322 - recall: 0.6062 - auc: 0.9368 - prc: 0.8290 - val_loss: 1.0786 - val_tp: 1065.0000 - val_fp: 479.0000 - val_tn: 6328.0000 - val_fn: 1204.0000 - val_accuracy: 0.5963 - val_precision: 0.6898 - val_recall: 0.4694 - val_auc: 0.8125 - val_prc: 0.5794\n",
      "Epoch 60/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.5275 - tp: 5602.0000 - fp: 1098.0000 - tn: 26130.0000 - fn: 3474.0000 - accuracy: 0.7685 - precision: 0.8361 - recall: 0.6172 - auc: 0.9396 - prc: 0.8375 - val_loss: 1.0616 - val_tp: 1096.0000 - val_fp: 623.0000 - val_tn: 6184.0000 - val_fn: 1173.0000 - val_accuracy: 0.5994 - val_precision: 0.6376 - val_recall: 0.4830 - val_auc: 0.8179 - val_prc: 0.5889\n",
      "Epoch 61/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.5205 - tp: 5580.0000 - fp: 1093.0000 - tn: 26135.0000 - fn: 3496.0000 - accuracy: 0.7682 - precision: 0.8362 - recall: 0.6148 - auc: 0.9412 - prc: 0.8422 - val_loss: 1.0481 - val_tp: 1115.0000 - val_fp: 639.0000 - val_tn: 6168.0000 - val_fn: 1154.0000 - val_accuracy: 0.6064 - val_precision: 0.6357 - val_recall: 0.4914 - val_auc: 0.8217 - val_prc: 0.5985\n",
      "Epoch 62/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.5112 - tp: 5692.0000 - fp: 1131.0000 - tn: 26097.0000 - fn: 3384.0000 - accuracy: 0.7699 - precision: 0.8342 - recall: 0.6271 - auc: 0.9420 - prc: 0.8429 - val_loss: 1.0585 - val_tp: 1149.0000 - val_fp: 667.0000 - val_tn: 6140.0000 - val_fn: 1120.0000 - val_accuracy: 0.6020 - val_precision: 0.6327 - val_recall: 0.5064 - val_auc: 0.8202 - val_prc: 0.5838\n",
      "Epoch 63/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.5072 - tp: 5795.0000 - fp: 1121.0000 - tn: 26107.0000 - fn: 3281.0000 - accuracy: 0.7710 - precision: 0.8379 - recall: 0.6385 - auc: 0.9440 - prc: 0.8476 - val_loss: 1.0532 - val_tp: 1156.0000 - val_fp: 701.0000 - val_tn: 6106.0000 - val_fn: 1113.0000 - val_accuracy: 0.6033 - val_precision: 0.6225 - val_recall: 0.5095 - val_auc: 0.8221 - val_prc: 0.5839\n",
      "Epoch 64/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.4833 - tp: 5862.0000 - fp: 1153.0000 - tn: 26075.0000 - fn: 3214.0000 - accuracy: 0.7738 - precision: 0.8356 - recall: 0.6459 - auc: 0.9465 - prc: 0.8551 - val_loss: 1.0349 - val_tp: 1173.0000 - val_fp: 714.0000 - val_tn: 6093.0000 - val_fn: 1096.0000 - val_accuracy: 0.6060 - val_precision: 0.6216 - val_recall: 0.5170 - val_auc: 0.8270 - val_prc: 0.5960\n",
      "Epoch 65/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.4794 - tp: 5950.0000 - fp: 1167.0000 - tn: 26061.0000 - fn: 3126.0000 - accuracy: 0.7810 - precision: 0.8360 - recall: 0.6556 - auc: 0.9470 - prc: 0.8551 - val_loss: 1.0278 - val_tp: 1184.0000 - val_fp: 722.0000 - val_tn: 6085.0000 - val_fn: 1085.0000 - val_accuracy: 0.6064 - val_precision: 0.6212 - val_recall: 0.5218 - val_auc: 0.8292 - val_prc: 0.5987\n",
      "Epoch 66/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.4765 - tp: 6000.0000 - fp: 1119.0000 - tn: 26109.0000 - fn: 3076.0000 - accuracy: 0.7839 - precision: 0.8428 - recall: 0.6611 - auc: 0.9476 - prc: 0.8576 - val_loss: 1.0136 - val_tp: 1201.0000 - val_fp: 726.0000 - val_tn: 6081.0000 - val_fn: 1068.0000 - val_accuracy: 0.6113 - val_precision: 0.6232 - val_recall: 0.5293 - val_auc: 0.8330 - val_prc: 0.6074\n",
      "Epoch 67/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.4817 - tp: 6087.0000 - fp: 1148.0000 - tn: 26080.0000 - fn: 2989.0000 - accuracy: 0.7877 - precision: 0.8413 - recall: 0.6707 - auc: 0.9491 - prc: 0.8597 - val_loss: 1.0169 - val_tp: 1215.0000 - val_fp: 747.0000 - val_tn: 6060.0000 - val_fn: 1054.0000 - val_accuracy: 0.6073 - val_precision: 0.6193 - val_recall: 0.5355 - val_auc: 0.8328 - val_prc: 0.6008\n",
      "Epoch 68/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.4618 - tp: 6116.0000 - fp: 1182.0000 - tn: 26046.0000 - fn: 2960.0000 - accuracy: 0.7845 - precision: 0.8380 - recall: 0.6739 - auc: 0.9498 - prc: 0.8615 - val_loss: 1.0141 - val_tp: 1231.0000 - val_fp: 753.0000 - val_tn: 6054.0000 - val_fn: 1038.0000 - val_accuracy: 0.6100 - val_precision: 0.6205 - val_recall: 0.5425 - val_auc: 0.8339 - val_prc: 0.6034\n",
      "Epoch 69/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.4553 - tp: 6157.0000 - fp: 1179.0000 - tn: 26049.0000 - fn: 2919.0000 - accuracy: 0.7874 - precision: 0.8393 - recall: 0.6784 - auc: 0.9509 - prc: 0.8652 - val_loss: 1.0016 - val_tp: 1225.0000 - val_fp: 769.0000 - val_tn: 6038.0000 - val_fn: 1044.0000 - val_accuracy: 0.6073 - val_precision: 0.6143 - val_recall: 0.5399 - val_auc: 0.8374 - val_prc: 0.6106\n",
      "Epoch 70/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.4537 - tp: 6268.0000 - fp: 1178.0000 - tn: 26050.0000 - fn: 2808.0000 - accuracy: 0.7950 - precision: 0.8418 - recall: 0.6906 - auc: 0.9537 - prc: 0.8741 - val_loss: 1.0028 - val_tp: 1241.0000 - val_fp: 780.0000 - val_tn: 6027.0000 - val_fn: 1028.0000 - val_accuracy: 0.6082 - val_precision: 0.6141 - val_recall: 0.5469 - val_auc: 0.8374 - val_prc: 0.6071\n",
      "Epoch 71/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.4450 - tp: 6293.0000 - fp: 1200.0000 - tn: 26028.0000 - fn: 2783.0000 - accuracy: 0.7915 - precision: 0.8399 - recall: 0.6934 - auc: 0.9528 - prc: 0.8694 - val_loss: 0.9986 - val_tp: 1251.0000 - val_fp: 785.0000 - val_tn: 6022.0000 - val_fn: 1018.0000 - val_accuracy: 0.6082 - val_precision: 0.6144 - val_recall: 0.5513 - val_auc: 0.8387 - val_prc: 0.6080\n",
      "Epoch 72/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.4293 - tp: 6399.0000 - fp: 1148.0000 - tn: 26080.0000 - fn: 2677.0000 - accuracy: 0.8010 - precision: 0.8479 - recall: 0.7050 - auc: 0.9558 - prc: 0.8776 - val_loss: 0.9885 - val_tp: 1241.0000 - val_fp: 789.0000 - val_tn: 6018.0000 - val_fn: 1028.0000 - val_accuracy: 0.6073 - val_precision: 0.6113 - val_recall: 0.5469 - val_auc: 0.8414 - val_prc: 0.6149\n",
      "Epoch 73/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.4233 - tp: 6432.0000 - fp: 1139.0000 - tn: 26089.0000 - fn: 2644.0000 - accuracy: 0.8049 - precision: 0.8496 - recall: 0.7087 - auc: 0.9569 - prc: 0.8804 - val_loss: 0.9855 - val_tp: 1263.0000 - val_fp: 800.0000 - val_tn: 6007.0000 - val_fn: 1006.0000 - val_accuracy: 0.6100 - val_precision: 0.6122 - val_recall: 0.5566 - val_auc: 0.8428 - val_prc: 0.6156\n",
      "Epoch 74/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.4246 - tp: 6361.0000 - fp: 1203.0000 - tn: 26025.0000 - fn: 2715.0000 - accuracy: 0.7935 - precision: 0.8410 - recall: 0.7009 - auc: 0.9543 - prc: 0.8736 - val_loss: 0.9819 - val_tp: 1292.0000 - val_fp: 786.0000 - val_tn: 6021.0000 - val_fn: 977.0000 - val_accuracy: 0.6139 - val_precision: 0.6218 - val_recall: 0.5694 - val_auc: 0.8444 - val_prc: 0.6164\n",
      "Epoch 75/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.4161 - tp: 6530.0000 - fp: 1163.0000 - tn: 26065.0000 - fn: 2546.0000 - accuracy: 0.8041 - precision: 0.8488 - recall: 0.7195 - auc: 0.9581 - prc: 0.8840 - val_loss: 0.9763 - val_tp: 1301.0000 - val_fp: 791.0000 - val_tn: 6016.0000 - val_fn: 968.0000 - val_accuracy: 0.6139 - val_precision: 0.6219 - val_recall: 0.5734 - val_auc: 0.8464 - val_prc: 0.6217\n",
      "Epoch 76/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.4128 - tp: 6593.0000 - fp: 1167.0000 - tn: 26061.0000 - fn: 2483.0000 - accuracy: 0.8099 - precision: 0.8496 - recall: 0.7264 - auc: 0.9586 - prc: 0.8843 - val_loss: 0.9570 - val_tp: 1266.0000 - val_fp: 812.0000 - val_tn: 5995.0000 - val_fn: 1003.0000 - val_accuracy: 0.5932 - val_precision: 0.6092 - val_recall: 0.5580 - val_auc: 0.8501 - val_prc: 0.6373\n",
      "Epoch 77/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3982 - tp: 6666.0000 - fp: 1114.0000 - tn: 26114.0000 - fn: 2410.0000 - accuracy: 0.8130 - precision: 0.8568 - recall: 0.7345 - auc: 0.9612 - prc: 0.8926 - val_loss: 0.9648 - val_tp: 1316.0000 - val_fp: 803.0000 - val_tn: 6004.0000 - val_fn: 953.0000 - val_accuracy: 0.6179 - val_precision: 0.6210 - val_recall: 0.5800 - val_auc: 0.8504 - val_prc: 0.6346\n",
      "Epoch 78/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.4052 - tp: 6618.0000 - fp: 1234.0000 - tn: 25994.0000 - fn: 2458.0000 - accuracy: 0.8059 - precision: 0.8428 - recall: 0.7292 - auc: 0.9589 - prc: 0.8861 - val_loss: 0.9631 - val_tp: 1328.0000 - val_fp: 796.0000 - val_tn: 6011.0000 - val_fn: 941.0000 - val_accuracy: 0.6175 - val_precision: 0.6252 - val_recall: 0.5853 - val_auc: 0.8514 - val_prc: 0.6323\n",
      "Epoch 79/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3948 - tp: 6676.0000 - fp: 1143.0000 - tn: 26085.0000 - fn: 2400.0000 - accuracy: 0.8163 - precision: 0.8538 - recall: 0.7356 - auc: 0.9607 - prc: 0.8921 - val_loss: 0.9600 - val_tp: 1336.0000 - val_fp: 788.0000 - val_tn: 6019.0000 - val_fn: 933.0000 - val_accuracy: 0.6188 - val_precision: 0.6290 - val_recall: 0.5888 - val_auc: 0.8525 - val_prc: 0.6331\n",
      "Epoch 80/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3895 - tp: 6785.0000 - fp: 1119.0000 - tn: 26109.0000 - fn: 2291.0000 - accuracy: 0.8210 - precision: 0.8584 - recall: 0.7476 - auc: 0.9627 - prc: 0.8958 - val_loss: 0.9612 - val_tp: 1326.0000 - val_fp: 807.0000 - val_tn: 6000.0000 - val_fn: 943.0000 - val_accuracy: 0.6161 - val_precision: 0.6217 - val_recall: 0.5844 - val_auc: 0.8517 - val_prc: 0.6269\n",
      "Epoch 81/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3878 - tp: 6840.0000 - fp: 1146.0000 - tn: 26082.0000 - fn: 2236.0000 - accuracy: 0.8197 - precision: 0.8565 - recall: 0.7536 - auc: 0.9633 - prc: 0.8978 - val_loss: 0.9547 - val_tp: 1342.0000 - val_fp: 802.0000 - val_tn: 6005.0000 - val_fn: 927.0000 - val_accuracy: 0.6205 - val_precision: 0.6259 - val_recall: 0.5914 - val_auc: 0.8544 - val_prc: 0.6357\n",
      "Epoch 82/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3786 - tp: 6932.0000 - fp: 1111.0000 - tn: 26117.0000 - fn: 2144.0000 - accuracy: 0.8271 - precision: 0.8619 - recall: 0.7638 - auc: 0.9649 - prc: 0.9025 - val_loss: 0.9605 - val_tp: 1351.0000 - val_fp: 795.0000 - val_tn: 6012.0000 - val_fn: 918.0000 - val_accuracy: 0.6227 - val_precision: 0.6295 - val_recall: 0.5954 - val_auc: 0.8541 - val_prc: 0.6323\n",
      "Epoch 83/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3733 - tp: 6967.0000 - fp: 1070.0000 - tn: 26158.0000 - fn: 2109.0000 - accuracy: 0.8343 - precision: 0.8669 - recall: 0.7676 - auc: 0.9661 - prc: 0.9050 - val_loss: 0.9346 - val_tp: 1358.0000 - val_fp: 805.0000 - val_tn: 6002.0000 - val_fn: 911.0000 - val_accuracy: 0.6210 - val_precision: 0.6278 - val_recall: 0.5985 - val_auc: 0.8603 - val_prc: 0.6549\n",
      "Epoch 84/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3661 - tp: 6934.0000 - fp: 1166.0000 - tn: 26062.0000 - fn: 2142.0000 - accuracy: 0.8226 - precision: 0.8560 - recall: 0.7640 - auc: 0.9649 - prc: 0.9028 - val_loss: 0.9399 - val_tp: 1368.0000 - val_fp: 782.0000 - val_tn: 6025.0000 - val_fn: 901.0000 - val_accuracy: 0.6245 - val_precision: 0.6363 - val_recall: 0.6029 - val_auc: 0.8604 - val_prc: 0.6514\n",
      "Epoch 85/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3587 - tp: 6954.0000 - fp: 1109.0000 - tn: 26119.0000 - fn: 2122.0000 - accuracy: 0.8316 - precision: 0.8625 - recall: 0.7662 - auc: 0.9671 - prc: 0.9081 - val_loss: 0.9420 - val_tp: 1355.0000 - val_fp: 810.0000 - val_tn: 5997.0000 - val_fn: 914.0000 - val_accuracy: 0.6175 - val_precision: 0.6259 - val_recall: 0.5972 - val_auc: 0.8585 - val_prc: 0.6410\n",
      "Epoch 86/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3606 - tp: 7010.0000 - fp: 1113.0000 - tn: 26115.0000 - fn: 2066.0000 - accuracy: 0.8322 - precision: 0.8630 - recall: 0.7724 - auc: 0.9671 - prc: 0.9090 - val_loss: 0.9279 - val_tp: 1380.0000 - val_fp: 795.0000 - val_tn: 6012.0000 - val_fn: 889.0000 - val_accuracy: 0.6289 - val_precision: 0.6345 - val_recall: 0.6082 - val_auc: 0.8641 - val_prc: 0.6607\n",
      "Epoch 87/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3492 - tp: 7141.0000 - fp: 1059.0000 - tn: 26169.0000 - fn: 1935.0000 - accuracy: 0.8431 - precision: 0.8709 - recall: 0.7868 - auc: 0.9685 - prc: 0.9132 - val_loss: 0.9166 - val_tp: 1384.0000 - val_fp: 793.0000 - val_tn: 6014.0000 - val_fn: 885.0000 - val_accuracy: 0.6271 - val_precision: 0.6357 - val_recall: 0.6100 - val_auc: 0.8671 - val_prc: 0.6719\n",
      "Epoch 88/200\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.3462 - tp: 7138.0000 - fp: 1073.0000 - tn: 26155.0000 - fn: 1938.0000 - accuracy: 0.8432 - precision: 0.8693 - recall: 0.7865 - auc: 0.9685 - prc: 0.9118 - val_loss: 0.9135 - val_tp: 1385.0000 - val_fp: 791.0000 - val_tn: 6016.0000 - val_fn: 884.0000 - val_accuracy: 0.6276 - val_precision: 0.6365 - val_recall: 0.6104 - val_auc: 0.8680 - val_prc: 0.6717\n",
      "Epoch 89/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3461 - tp: 7206.0000 - fp: 1048.0000 - tn: 26180.0000 - fn: 1870.0000 - accuracy: 0.8455 - precision: 0.8730 - recall: 0.7940 - auc: 0.9702 - prc: 0.9174 - val_loss: 0.9141 - val_tp: 1391.0000 - val_fp: 786.0000 - val_tn: 6021.0000 - val_fn: 878.0000 - val_accuracy: 0.6346 - val_precision: 0.6390 - val_recall: 0.6130 - val_auc: 0.8697 - val_prc: 0.6809\n",
      "Epoch 90/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3399 - tp: 7165.0000 - fp: 1063.0000 - tn: 26165.0000 - fn: 1911.0000 - accuracy: 0.8457 - precision: 0.8708 - recall: 0.7894 - auc: 0.9696 - prc: 0.9149 - val_loss: 0.9134 - val_tp: 1404.0000 - val_fp: 777.0000 - val_tn: 6030.0000 - val_fn: 865.0000 - val_accuracy: 0.6346 - val_precision: 0.6437 - val_recall: 0.6188 - val_auc: 0.8701 - val_prc: 0.6789\n",
      "Epoch 91/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.3366 - tp: 7260.0000 - fp: 1019.0000 - tn: 26209.0000 - fn: 1816.0000 - accuracy: 0.8502 - precision: 0.8769 - recall: 0.7999 - auc: 0.9712 - prc: 0.9193 - val_loss: 0.9048 - val_tp: 1405.0000 - val_fp: 785.0000 - val_tn: 6022.0000 - val_fn: 864.0000 - val_accuracy: 0.6382 - val_precision: 0.6416 - val_recall: 0.6192 - val_auc: 0.8719 - val_prc: 0.6811\n",
      "Epoch 92/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3318 - tp: 7299.0000 - fp: 999.0000 - tn: 26229.0000 - fn: 1777.0000 - accuracy: 0.8522 - precision: 0.8796 - recall: 0.8042 - auc: 0.9716 - prc: 0.9209 - val_loss: 0.9035 - val_tp: 1413.0000 - val_fp: 780.0000 - val_tn: 6027.0000 - val_fn: 856.0000 - val_accuracy: 0.6390 - val_precision: 0.6443 - val_recall: 0.6227 - val_auc: 0.8726 - val_prc: 0.6793\n",
      "Epoch 93/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3351 - tp: 7344.0000 - fp: 999.0000 - tn: 26229.0000 - fn: 1732.0000 - accuracy: 0.8567 - precision: 0.8803 - recall: 0.8092 - auc: 0.9730 - prc: 0.9238 - val_loss: 0.9138 - val_tp: 1418.0000 - val_fp: 769.0000 - val_tn: 6038.0000 - val_fn: 851.0000 - val_accuracy: 0.6413 - val_precision: 0.6484 - val_recall: 0.6249 - val_auc: 0.8715 - val_prc: 0.6761\n",
      "Epoch 94/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3160 - tp: 7341.0000 - fp: 1018.0000 - tn: 26210.0000 - fn: 1735.0000 - accuracy: 0.8559 - precision: 0.8782 - recall: 0.8088 - auc: 0.9735 - prc: 0.9263 - val_loss: 0.8933 - val_tp: 1435.0000 - val_fp: 771.0000 - val_tn: 6036.0000 - val_fn: 834.0000 - val_accuracy: 0.6452 - val_precision: 0.6505 - val_recall: 0.6324 - val_auc: 0.8769 - val_prc: 0.6947\n",
      "Epoch 95/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3146 - tp: 7412.0000 - fp: 971.0000 - tn: 26257.0000 - fn: 1664.0000 - accuracy: 0.8619 - precision: 0.8842 - recall: 0.8167 - auc: 0.9750 - prc: 0.9295 - val_loss: 0.8880 - val_tp: 1426.0000 - val_fp: 767.0000 - val_tn: 6040.0000 - val_fn: 843.0000 - val_accuracy: 0.6487 - val_precision: 0.6503 - val_recall: 0.6285 - val_auc: 0.8783 - val_prc: 0.6977\n",
      "Epoch 96/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3169 - tp: 7446.0000 - fp: 993.0000 - tn: 26235.0000 - fn: 1630.0000 - accuracy: 0.8599 - precision: 0.8823 - recall: 0.8204 - auc: 0.9745 - prc: 0.9285 - val_loss: 0.8908 - val_tp: 1444.0000 - val_fp: 759.0000 - val_tn: 6048.0000 - val_fn: 825.0000 - val_accuracy: 0.6457 - val_precision: 0.6555 - val_recall: 0.6364 - val_auc: 0.8784 - val_prc: 0.6954\n",
      "Epoch 97/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3003 - tp: 7494.0000 - fp: 935.0000 - tn: 26293.0000 - fn: 1582.0000 - accuracy: 0.8671 - precision: 0.8891 - recall: 0.8257 - auc: 0.9764 - prc: 0.9342 - val_loss: 0.8925 - val_tp: 1431.0000 - val_fp: 767.0000 - val_tn: 6040.0000 - val_fn: 838.0000 - val_accuracy: 0.6448 - val_precision: 0.6510 - val_recall: 0.6307 - val_auc: 0.8773 - val_prc: 0.6875\n",
      "Epoch 98/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.3076 - tp: 7598.0000 - fp: 897.0000 - tn: 26331.0000 - fn: 1478.0000 - accuracy: 0.8762 - precision: 0.8944 - recall: 0.8372 - auc: 0.9769 - prc: 0.9361 - val_loss: 0.8826 - val_tp: 1447.0000 - val_fp: 753.0000 - val_tn: 6054.0000 - val_fn: 822.0000 - val_accuracy: 0.6514 - val_precision: 0.6577 - val_recall: 0.6377 - val_auc: 0.8815 - val_prc: 0.7042\n",
      "Epoch 99/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.3076 - tp: 7557.0000 - fp: 911.0000 - tn: 26317.0000 - fn: 1519.0000 - accuracy: 0.8708 - precision: 0.8924 - recall: 0.8326 - auc: 0.9774 - prc: 0.9365 - val_loss: 0.8807 - val_tp: 1454.0000 - val_fp: 749.0000 - val_tn: 6058.0000 - val_fn: 815.0000 - val_accuracy: 0.6518 - val_precision: 0.6600 - val_recall: 0.6408 - val_auc: 0.8829 - val_prc: 0.7102\n",
      "Epoch 100/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.3031 - tp: 7549.0000 - fp: 956.0000 - tn: 26272.0000 - fn: 1527.0000 - accuracy: 0.8690 - precision: 0.8876 - recall: 0.8318 - auc: 0.9767 - prc: 0.9349 - val_loss: 0.8872 - val_tp: 1452.0000 - val_fp: 755.0000 - val_tn: 6052.0000 - val_fn: 817.0000 - val_accuracy: 0.6536 - val_precision: 0.6579 - val_recall: 0.6399 - val_auc: 0.8814 - val_prc: 0.7011\n",
      "Epoch 101/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2884 - tp: 7617.0000 - fp: 906.0000 - tn: 26322.0000 - fn: 1459.0000 - accuracy: 0.8746 - precision: 0.8937 - recall: 0.8392 - auc: 0.9785 - prc: 0.9400 - val_loss: 0.8953 - val_tp: 1449.0000 - val_fp: 762.0000 - val_tn: 6045.0000 - val_fn: 820.0000 - val_accuracy: 0.6518 - val_precision: 0.6554 - val_recall: 0.6386 - val_auc: 0.8795 - val_prc: 0.6919\n",
      "Epoch 102/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2867 - tp: 7704.0000 - fp: 856.0000 - tn: 26372.0000 - fn: 1372.0000 - accuracy: 0.8833 - precision: 0.9000 - recall: 0.8488 - auc: 0.9801 - prc: 0.9441 - val_loss: 0.8720 - val_tp: 1473.0000 - val_fp: 739.0000 - val_tn: 6068.0000 - val_fn: 796.0000 - val_accuracy: 0.6624 - val_precision: 0.6659 - val_recall: 0.6492 - val_auc: 0.8866 - val_prc: 0.7205\n",
      "Epoch 103/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2834 - tp: 7704.0000 - fp: 867.0000 - tn: 26361.0000 - fn: 1372.0000 - accuracy: 0.8820 - precision: 0.8988 - recall: 0.8488 - auc: 0.9805 - prc: 0.9450 - val_loss: 0.8719 - val_tp: 1480.0000 - val_fp: 736.0000 - val_tn: 6071.0000 - val_fn: 789.0000 - val_accuracy: 0.6602 - val_precision: 0.6679 - val_recall: 0.6523 - val_auc: 0.8865 - val_prc: 0.7173\n",
      "Epoch 104/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2739 - tp: 7716.0000 - fp: 846.0000 - tn: 26382.0000 - fn: 1360.0000 - accuracy: 0.8855 - precision: 0.9012 - recall: 0.8502 - auc: 0.9800 - prc: 0.9438 - val_loss: 0.8739 - val_tp: 1481.0000 - val_fp: 735.0000 - val_tn: 6072.0000 - val_fn: 788.0000 - val_accuracy: 0.6646 - val_precision: 0.6683 - val_recall: 0.6527 - val_auc: 0.8864 - val_prc: 0.7132\n",
      "Epoch 105/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2772 - tp: 7759.0000 - fp: 821.0000 - tn: 26407.0000 - fn: 1317.0000 - accuracy: 0.8865 - precision: 0.9043 - recall: 0.8549 - auc: 0.9812 - prc: 0.9478 - val_loss: 0.8651 - val_tp: 1486.0000 - val_fp: 729.0000 - val_tn: 6078.0000 - val_fn: 783.0000 - val_accuracy: 0.6646 - val_precision: 0.6709 - val_recall: 0.6549 - val_auc: 0.8887 - val_prc: 0.7199\n",
      "Epoch 106/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2622 - tp: 7799.0000 - fp: 806.0000 - tn: 26422.0000 - fn: 1277.0000 - accuracy: 0.8913 - precision: 0.9063 - recall: 0.8593 - auc: 0.9816 - prc: 0.9484 - val_loss: 0.8639 - val_tp: 1501.0000 - val_fp: 711.0000 - val_tn: 6096.0000 - val_fn: 768.0000 - val_accuracy: 0.6739 - val_precision: 0.6786 - val_recall: 0.6615 - val_auc: 0.8903 - val_prc: 0.7290\n",
      "Epoch 107/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2608 - tp: 7811.0000 - fp: 826.0000 - tn: 26402.0000 - fn: 1265.0000 - accuracy: 0.8900 - precision: 0.9044 - recall: 0.8606 - auc: 0.9822 - prc: 0.9501 - val_loss: 0.8538 - val_tp: 1498.0000 - val_fp: 713.0000 - val_tn: 6094.0000 - val_fn: 771.0000 - val_accuracy: 0.6761 - val_precision: 0.6775 - val_recall: 0.6602 - val_auc: 0.8925 - val_prc: 0.7324\n",
      "Epoch 108/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.2675 - tp: 7811.0000 - fp: 803.0000 - tn: 26425.0000 - fn: 1265.0000 - accuracy: 0.8901 - precision: 0.9068 - recall: 0.8606 - auc: 0.9823 - prc: 0.9503 - val_loss: 0.8539 - val_tp: 1502.0000 - val_fp: 716.0000 - val_tn: 6091.0000 - val_fn: 767.0000 - val_accuracy: 0.6708 - val_precision: 0.6772 - val_recall: 0.6620 - val_auc: 0.8922 - val_prc: 0.7282\n",
      "Epoch 109/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2563 - tp: 7854.0000 - fp: 758.0000 - tn: 26470.0000 - fn: 1222.0000 - accuracy: 0.8924 - precision: 0.9120 - recall: 0.8654 - auc: 0.9840 - prc: 0.9548 - val_loss: 0.8483 - val_tp: 1505.0000 - val_fp: 709.0000 - val_tn: 6098.0000 - val_fn: 764.0000 - val_accuracy: 0.6774 - val_precision: 0.6798 - val_recall: 0.6633 - val_auc: 0.8939 - val_prc: 0.7326\n",
      "Epoch 110/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.2521 - tp: 7912.0000 - fp: 761.0000 - tn: 26467.0000 - fn: 1164.0000 - accuracy: 0.8984 - precision: 0.9123 - recall: 0.8717 - auc: 0.9843 - prc: 0.9566 - val_loss: 0.8439 - val_tp: 1519.0000 - val_fp: 705.0000 - val_tn: 6102.0000 - val_fn: 750.0000 - val_accuracy: 0.6805 - val_precision: 0.6830 - val_recall: 0.6695 - val_auc: 0.8952 - val_prc: 0.7359\n",
      "Epoch 111/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.2484 - tp: 7948.0000 - fp: 763.0000 - tn: 26465.0000 - fn: 1128.0000 - accuracy: 0.8993 - precision: 0.9124 - recall: 0.8757 - auc: 0.9850 - prc: 0.9584 - val_loss: 0.8537 - val_tp: 1511.0000 - val_fp: 707.0000 - val_tn: 6100.0000 - val_fn: 758.0000 - val_accuracy: 0.6787 - val_precision: 0.6812 - val_recall: 0.6659 - val_auc: 0.8931 - val_prc: 0.7280\n",
      "Epoch 112/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.2452 - tp: 7911.0000 - fp: 757.0000 - tn: 26471.0000 - fn: 1165.0000 - accuracy: 0.8957 - precision: 0.9127 - recall: 0.8716 - auc: 0.9847 - prc: 0.9570 - val_loss: 0.8416 - val_tp: 1542.0000 - val_fp: 687.0000 - val_tn: 6120.0000 - val_fn: 727.0000 - val_accuracy: 0.6880 - val_precision: 0.6918 - val_recall: 0.6796 - val_auc: 0.8973 - val_prc: 0.7425\n",
      "Epoch 113/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2398 - tp: 7978.0000 - fp: 718.0000 - tn: 26510.0000 - fn: 1098.0000 - accuracy: 0.9037 - precision: 0.9174 - recall: 0.8790 - auc: 0.9860 - prc: 0.9608 - val_loss: 0.8387 - val_tp: 1541.0000 - val_fp: 678.0000 - val_tn: 6129.0000 - val_fn: 728.0000 - val_accuracy: 0.6871 - val_precision: 0.6945 - val_recall: 0.6792 - val_auc: 0.8982 - val_prc: 0.7445\n",
      "Epoch 114/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2401 - tp: 7997.0000 - fp: 690.0000 - tn: 26538.0000 - fn: 1079.0000 - accuracy: 0.9057 - precision: 0.9206 - recall: 0.8811 - auc: 0.9852 - prc: 0.9594 - val_loss: 0.8266 - val_tp: 1553.0000 - val_fp: 673.0000 - val_tn: 6134.0000 - val_fn: 716.0000 - val_accuracy: 0.6928 - val_precision: 0.6977 - val_recall: 0.6844 - val_auc: 0.9008 - val_prc: 0.7510\n",
      "Epoch 115/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2371 - tp: 7995.0000 - fp: 703.0000 - tn: 26525.0000 - fn: 1081.0000 - accuracy: 0.9044 - precision: 0.9192 - recall: 0.8809 - auc: 0.9865 - prc: 0.9627 - val_loss: 0.8276 - val_tp: 1549.0000 - val_fp: 674.0000 - val_tn: 6133.0000 - val_fn: 720.0000 - val_accuracy: 0.6941 - val_precision: 0.6968 - val_recall: 0.6827 - val_auc: 0.9006 - val_prc: 0.7491\n",
      "Epoch 116/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2333 - tp: 7998.0000 - fp: 701.0000 - tn: 26527.0000 - fn: 1078.0000 - accuracy: 0.9058 - precision: 0.9194 - recall: 0.8812 - auc: 0.9868 - prc: 0.9630 - val_loss: 0.8089 - val_tp: 1578.0000 - val_fp: 650.0000 - val_tn: 6157.0000 - val_fn: 691.0000 - val_accuracy: 0.7043 - val_precision: 0.7083 - val_recall: 0.6955 - val_auc: 0.9051 - val_prc: 0.7645\n",
      "Epoch 117/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.2327 - tp: 8016.0000 - fp: 695.0000 - tn: 26533.0000 - fn: 1060.0000 - accuracy: 0.9081 - precision: 0.9202 - recall: 0.8832 - auc: 0.9867 - prc: 0.9631 - val_loss: 0.8171 - val_tp: 1570.0000 - val_fp: 657.0000 - val_tn: 6150.0000 - val_fn: 699.0000 - val_accuracy: 0.6999 - val_precision: 0.7050 - val_recall: 0.6919 - val_auc: 0.9035 - val_prc: 0.7565\n",
      "Epoch 118/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2289 - tp: 8061.0000 - fp: 665.0000 - tn: 26563.0000 - fn: 1015.0000 - accuracy: 0.9095 - precision: 0.9238 - recall: 0.8882 - auc: 0.9874 - prc: 0.9647 - val_loss: 0.8054 - val_tp: 1586.0000 - val_fp: 632.0000 - val_tn: 6175.0000 - val_fn: 683.0000 - val_accuracy: 0.7109 - val_precision: 0.7151 - val_recall: 0.6990 - val_auc: 0.9070 - val_prc: 0.7718\n",
      "Epoch 119/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2155 - tp: 8098.0000 - fp: 642.0000 - tn: 26586.0000 - fn: 978.0000 - accuracy: 0.9154 - precision: 0.9265 - recall: 0.8922 - auc: 0.9880 - prc: 0.9669 - val_loss: 0.8013 - val_tp: 1590.0000 - val_fp: 629.0000 - val_tn: 6178.0000 - val_fn: 679.0000 - val_accuracy: 0.7131 - val_precision: 0.7165 - val_recall: 0.7007 - val_auc: 0.9079 - val_prc: 0.7749\n",
      "Epoch 120/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2200 - tp: 8119.0000 - fp: 618.0000 - tn: 26610.0000 - fn: 957.0000 - accuracy: 0.9141 - precision: 0.9293 - recall: 0.8946 - auc: 0.9888 - prc: 0.9694 - val_loss: 0.8057 - val_tp: 1586.0000 - val_fp: 635.0000 - val_tn: 6172.0000 - val_fn: 683.0000 - val_accuracy: 0.7091 - val_precision: 0.7141 - val_recall: 0.6990 - val_auc: 0.9072 - val_prc: 0.7671\n",
      "Epoch 121/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2215 - tp: 8127.0000 - fp: 650.0000 - tn: 26578.0000 - fn: 949.0000 - accuracy: 0.9137 - precision: 0.9259 - recall: 0.8954 - auc: 0.9887 - prc: 0.9681 - val_loss: 0.7938 - val_tp: 1598.0000 - val_fp: 633.0000 - val_tn: 6174.0000 - val_fn: 671.0000 - val_accuracy: 0.7113 - val_precision: 0.7163 - val_recall: 0.7043 - val_auc: 0.9101 - val_prc: 0.7773\n",
      "Epoch 122/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.2196 - tp: 8127.0000 - fp: 616.0000 - tn: 26612.0000 - fn: 949.0000 - accuracy: 0.9163 - precision: 0.9295 - recall: 0.8954 - auc: 0.9884 - prc: 0.9675 - val_loss: 0.7932 - val_tp: 1599.0000 - val_fp: 628.0000 - val_tn: 6179.0000 - val_fn: 670.0000 - val_accuracy: 0.7149 - val_precision: 0.7180 - val_recall: 0.7047 - val_auc: 0.9101 - val_prc: 0.7756\n",
      "Epoch 123/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.2076 - tp: 8184.0000 - fp: 616.0000 - tn: 26612.0000 - fn: 892.0000 - accuracy: 0.9171 - precision: 0.9300 - recall: 0.9017 - auc: 0.9895 - prc: 0.9704 - val_loss: 0.8015 - val_tp: 1602.0000 - val_fp: 633.0000 - val_tn: 6174.0000 - val_fn: 667.0000 - val_accuracy: 0.7149 - val_precision: 0.7168 - val_recall: 0.7060 - val_auc: 0.9093 - val_prc: 0.7763\n",
      "Epoch 124/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.2069 - tp: 8167.0000 - fp: 622.0000 - tn: 26606.0000 - fn: 909.0000 - accuracy: 0.9186 - precision: 0.9292 - recall: 0.8998 - auc: 0.9901 - prc: 0.9727 - val_loss: 0.7908 - val_tp: 1605.0000 - val_fp: 628.0000 - val_tn: 6179.0000 - val_fn: 664.0000 - val_accuracy: 0.7131 - val_precision: 0.7188 - val_recall: 0.7074 - val_auc: 0.9108 - val_prc: 0.7756\n",
      "Epoch 125/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.2054 - tp: 8221.0000 - fp: 578.0000 - tn: 26650.0000 - fn: 855.0000 - accuracy: 0.9234 - precision: 0.9343 - recall: 0.9058 - auc: 0.9897 - prc: 0.9718 - val_loss: 0.7894 - val_tp: 1606.0000 - val_fp: 627.0000 - val_tn: 6180.0000 - val_fn: 663.0000 - val_accuracy: 0.7149 - val_precision: 0.7192 - val_recall: 0.7078 - val_auc: 0.9111 - val_prc: 0.7762\n",
      "Epoch 126/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1987 - tp: 8213.0000 - fp: 594.0000 - tn: 26634.0000 - fn: 863.0000 - accuracy: 0.9221 - precision: 0.9326 - recall: 0.9049 - auc: 0.9908 - prc: 0.9736 - val_loss: 0.7906 - val_tp: 1617.0000 - val_fp: 618.0000 - val_tn: 6189.0000 - val_fn: 652.0000 - val_accuracy: 0.7215 - val_precision: 0.7235 - val_recall: 0.7126 - val_auc: 0.9118 - val_prc: 0.7796\n",
      "Epoch 127/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1983 - tp: 8250.0000 - fp: 564.0000 - tn: 26664.0000 - fn: 826.0000 - accuracy: 0.9251 - precision: 0.9360 - recall: 0.9090 - auc: 0.9905 - prc: 0.9730 - val_loss: 0.7862 - val_tp: 1631.0000 - val_fp: 620.0000 - val_tn: 6187.0000 - val_fn: 638.0000 - val_accuracy: 0.7241 - val_precision: 0.7246 - val_recall: 0.7188 - val_auc: 0.9126 - val_prc: 0.7797\n",
      "Epoch 128/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1961 - tp: 8255.0000 - fp: 583.0000 - tn: 26645.0000 - fn: 821.0000 - accuracy: 0.9242 - precision: 0.9340 - recall: 0.9095 - auc: 0.9907 - prc: 0.9741 - val_loss: 0.7755 - val_tp: 1625.0000 - val_fp: 609.0000 - val_tn: 6198.0000 - val_fn: 644.0000 - val_accuracy: 0.7259 - val_precision: 0.7274 - val_recall: 0.7162 - val_auc: 0.9147 - val_prc: 0.7854\n",
      "Epoch 129/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1840 - tp: 8278.0000 - fp: 542.0000 - tn: 26686.0000 - fn: 798.0000 - accuracy: 0.9283 - precision: 0.9385 - recall: 0.9121 - auc: 0.9918 - prc: 0.9773 - val_loss: 0.7577 - val_tp: 1648.0000 - val_fp: 587.0000 - val_tn: 6220.0000 - val_fn: 621.0000 - val_accuracy: 0.7338 - val_precision: 0.7374 - val_recall: 0.7263 - val_auc: 0.9182 - val_prc: 0.7967\n",
      "Epoch 130/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1890 - tp: 8256.0000 - fp: 555.0000 - tn: 26673.0000 - fn: 820.0000 - accuracy: 0.9256 - precision: 0.9370 - recall: 0.9097 - auc: 0.9913 - prc: 0.9756 - val_loss: 0.7621 - val_tp: 1644.0000 - val_fp: 585.0000 - val_tn: 6222.0000 - val_fn: 625.0000 - val_accuracy: 0.7320 - val_precision: 0.7376 - val_recall: 0.7245 - val_auc: 0.9177 - val_prc: 0.7939\n",
      "Epoch 131/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.1837 - tp: 8317.0000 - fp: 531.0000 - tn: 26697.0000 - fn: 759.0000 - accuracy: 0.9301 - precision: 0.9400 - recall: 0.9164 - auc: 0.9922 - prc: 0.9785 - val_loss: 0.7554 - val_tp: 1652.0000 - val_fp: 579.0000 - val_tn: 6228.0000 - val_fn: 617.0000 - val_accuracy: 0.7373 - val_precision: 0.7405 - val_recall: 0.7281 - val_auc: 0.9189 - val_prc: 0.7976\n",
      "Epoch 132/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1829 - tp: 8371.0000 - fp: 479.0000 - tn: 26749.0000 - fn: 705.0000 - accuracy: 0.9366 - precision: 0.9459 - recall: 0.9223 - auc: 0.9927 - prc: 0.9797 - val_loss: 0.7536 - val_tp: 1656.0000 - val_fp: 582.0000 - val_tn: 6225.0000 - val_fn: 613.0000 - val_accuracy: 0.7369 - val_precision: 0.7399 - val_recall: 0.7298 - val_auc: 0.9195 - val_prc: 0.7986\n",
      "Epoch 133/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1740 - tp: 8388.0000 - fp: 465.0000 - tn: 26763.0000 - fn: 688.0000 - accuracy: 0.9386 - precision: 0.9475 - recall: 0.9242 - auc: 0.9931 - prc: 0.9808 - val_loss: 0.7456 - val_tp: 1664.0000 - val_fp: 572.0000 - val_tn: 6235.0000 - val_fn: 605.0000 - val_accuracy: 0.7395 - val_precision: 0.7442 - val_recall: 0.7334 - val_auc: 0.9212 - val_prc: 0.8033\n",
      "Epoch 134/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1733 - tp: 8331.0000 - fp: 503.0000 - tn: 26725.0000 - fn: 745.0000 - accuracy: 0.9329 - precision: 0.9431 - recall: 0.9179 - auc: 0.9927 - prc: 0.9797 - val_loss: 0.7535 - val_tp: 1659.0000 - val_fp: 583.0000 - val_tn: 6224.0000 - val_fn: 610.0000 - val_accuracy: 0.7369 - val_precision: 0.7400 - val_recall: 0.7312 - val_auc: 0.9198 - val_prc: 0.7982\n",
      "Epoch 135/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1737 - tp: 8352.0000 - fp: 507.0000 - tn: 26721.0000 - fn: 724.0000 - accuracy: 0.9342 - precision: 0.9428 - recall: 0.9202 - auc: 0.9932 - prc: 0.9810 - val_loss: 0.7479 - val_tp: 1672.0000 - val_fp: 576.0000 - val_tn: 6231.0000 - val_fn: 597.0000 - val_accuracy: 0.7404 - val_precision: 0.7438 - val_recall: 0.7369 - val_auc: 0.9209 - val_prc: 0.8028\n",
      "Epoch 136/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1726 - tp: 8394.0000 - fp: 457.0000 - tn: 26771.0000 - fn: 682.0000 - accuracy: 0.9372 - precision: 0.9484 - recall: 0.9249 - auc: 0.9937 - prc: 0.9822 - val_loss: 0.7239 - val_tp: 1692.0000 - val_fp: 548.0000 - val_tn: 6259.0000 - val_fn: 577.0000 - val_accuracy: 0.7519 - val_precision: 0.7554 - val_recall: 0.7457 - val_auc: 0.9261 - val_prc: 0.8181\n",
      "Epoch 137/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1700 - tp: 8414.0000 - fp: 452.0000 - tn: 26776.0000 - fn: 662.0000 - accuracy: 0.9397 - precision: 0.9490 - recall: 0.9271 - auc: 0.9938 - prc: 0.9827 - val_loss: 0.7412 - val_tp: 1679.0000 - val_fp: 562.0000 - val_tn: 6245.0000 - val_fn: 590.0000 - val_accuracy: 0.7453 - val_precision: 0.7492 - val_recall: 0.7400 - val_auc: 0.9227 - val_prc: 0.8072\n",
      "Epoch 138/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1702 - tp: 8420.0000 - fp: 463.0000 - tn: 26765.0000 - fn: 656.0000 - accuracy: 0.9404 - precision: 0.9479 - recall: 0.9277 - auc: 0.9939 - prc: 0.9827 - val_loss: 0.7663 - val_tp: 1655.0000 - val_fp: 588.0000 - val_tn: 6219.0000 - val_fn: 614.0000 - val_accuracy: 0.7320 - val_precision: 0.7379 - val_recall: 0.7294 - val_auc: 0.9189 - val_prc: 0.7944\n",
      "Epoch 139/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.1682 - tp: 8386.0000 - fp: 496.0000 - tn: 26732.0000 - fn: 690.0000 - accuracy: 0.9368 - precision: 0.9442 - recall: 0.9240 - auc: 0.9933 - prc: 0.9813 - val_loss: 0.7326 - val_tp: 1691.0000 - val_fp: 554.0000 - val_tn: 6253.0000 - val_fn: 578.0000 - val_accuracy: 0.7501 - val_precision: 0.7532 - val_recall: 0.7453 - val_auc: 0.9249 - val_prc: 0.8134\n",
      "Epoch 140/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1610 - tp: 8460.0000 - fp: 443.0000 - tn: 26785.0000 - fn: 616.0000 - accuracy: 0.9424 - precision: 0.9502 - recall: 0.9321 - auc: 0.9941 - prc: 0.9837 - val_loss: 0.7378 - val_tp: 1690.0000 - val_fp: 554.0000 - val_tn: 6253.0000 - val_fn: 579.0000 - val_accuracy: 0.7492 - val_precision: 0.7531 - val_recall: 0.7448 - val_auc: 0.9242 - val_prc: 0.8103\n",
      "Epoch 141/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1557 - tp: 8441.0000 - fp: 435.0000 - tn: 26793.0000 - fn: 635.0000 - accuracy: 0.9426 - precision: 0.9510 - recall: 0.9300 - auc: 0.9944 - prc: 0.9842 - val_loss: 0.7171 - val_tp: 1704.0000 - val_fp: 534.0000 - val_tn: 6273.0000 - val_fn: 565.0000 - val_accuracy: 0.7567 - val_precision: 0.7614 - val_recall: 0.7510 - val_auc: 0.9278 - val_prc: 0.8215\n",
      "Epoch 142/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1502 - tp: 8490.0000 - fp: 390.0000 - tn: 26838.0000 - fn: 586.0000 - accuracy: 0.9478 - precision: 0.9561 - recall: 0.9354 - auc: 0.9955 - prc: 0.9875 - val_loss: 0.7122 - val_tp: 1710.0000 - val_fp: 530.0000 - val_tn: 6277.0000 - val_fn: 559.0000 - val_accuracy: 0.7585 - val_precision: 0.7634 - val_recall: 0.7536 - val_auc: 0.9292 - val_prc: 0.8246\n",
      "Epoch 143/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1587 - tp: 8459.0000 - fp: 444.0000 - tn: 26784.0000 - fn: 617.0000 - accuracy: 0.9433 - precision: 0.9501 - recall: 0.9320 - auc: 0.9945 - prc: 0.9844 - val_loss: 0.7172 - val_tp: 1708.0000 - val_fp: 532.0000 - val_tn: 6275.0000 - val_fn: 561.0000 - val_accuracy: 0.7585 - val_precision: 0.7625 - val_recall: 0.7528 - val_auc: 0.9282 - val_prc: 0.8222\n",
      "Epoch 144/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1525 - tp: 8456.0000 - fp: 419.0000 - tn: 26809.0000 - fn: 620.0000 - accuracy: 0.9434 - precision: 0.9528 - recall: 0.9317 - auc: 0.9945 - prc: 0.9844 - val_loss: 0.7195 - val_tp: 1706.0000 - val_fp: 534.0000 - val_tn: 6273.0000 - val_fn: 563.0000 - val_accuracy: 0.7576 - val_precision: 0.7616 - val_recall: 0.7519 - val_auc: 0.9283 - val_prc: 0.8207\n",
      "Epoch 145/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1526 - tp: 8489.0000 - fp: 396.0000 - tn: 26832.0000 - fn: 587.0000 - accuracy: 0.9452 - precision: 0.9554 - recall: 0.9353 - auc: 0.9949 - prc: 0.9861 - val_loss: 0.7213 - val_tp: 1706.0000 - val_fp: 538.0000 - val_tn: 6269.0000 - val_fn: 563.0000 - val_accuracy: 0.7550 - val_precision: 0.7602 - val_recall: 0.7519 - val_auc: 0.9280 - val_prc: 0.8194\n",
      "Epoch 146/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1500 - tp: 8482.0000 - fp: 396.0000 - tn: 26832.0000 - fn: 594.0000 - accuracy: 0.9461 - precision: 0.9554 - recall: 0.9346 - auc: 0.9955 - prc: 0.9872 - val_loss: 0.7159 - val_tp: 1713.0000 - val_fp: 525.0000 - val_tn: 6282.0000 - val_fn: 556.0000 - val_accuracy: 0.7602 - val_precision: 0.7654 - val_recall: 0.7550 - val_auc: 0.9293 - val_prc: 0.8252\n",
      "Epoch 147/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1544 - tp: 8480.0000 - fp: 411.0000 - tn: 26817.0000 - fn: 596.0000 - accuracy: 0.9455 - precision: 0.9538 - recall: 0.9343 - auc: 0.9950 - prc: 0.9861 - val_loss: 0.7111 - val_tp: 1716.0000 - val_fp: 523.0000 - val_tn: 6284.0000 - val_fn: 553.0000 - val_accuracy: 0.7611 - val_precision: 0.7664 - val_recall: 0.7563 - val_auc: 0.9303 - val_prc: 0.8269\n",
      "Epoch 148/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1427 - tp: 8532.0000 - fp: 381.0000 - tn: 26847.0000 - fn: 544.0000 - accuracy: 0.9510 - precision: 0.9573 - recall: 0.9401 - auc: 0.9954 - prc: 0.9869 - val_loss: 0.7331 - val_tp: 1697.0000 - val_fp: 548.0000 - val_tn: 6259.0000 - val_fn: 572.0000 - val_accuracy: 0.7536 - val_precision: 0.7559 - val_recall: 0.7479 - val_auc: 0.9268 - val_prc: 0.8155\n",
      "Epoch 149/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1447 - tp: 8512.0000 - fp: 390.0000 - tn: 26838.0000 - fn: 564.0000 - accuracy: 0.9478 - precision: 0.9562 - recall: 0.9379 - auc: 0.9954 - prc: 0.9871 - val_loss: 0.6929 - val_tp: 1730.0000 - val_fp: 503.0000 - val_tn: 6304.0000 - val_fn: 539.0000 - val_accuracy: 0.7708 - val_precision: 0.7747 - val_recall: 0.7625 - val_auc: 0.9337 - val_prc: 0.8371\n",
      "Epoch 150/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.1428 - tp: 8497.0000 - fp: 405.0000 - tn: 26823.0000 - fn: 579.0000 - accuracy: 0.9477 - precision: 0.9545 - recall: 0.9362 - auc: 0.9954 - prc: 0.9875 - val_loss: 0.6911 - val_tp: 1736.0000 - val_fp: 507.0000 - val_tn: 6300.0000 - val_fn: 533.0000 - val_accuracy: 0.7695 - val_precision: 0.7740 - val_recall: 0.7651 - val_auc: 0.9338 - val_prc: 0.8363\n",
      "Epoch 151/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1401 - tp: 8548.0000 - fp: 360.0000 - tn: 26868.0000 - fn: 528.0000 - accuracy: 0.9534 - precision: 0.9596 - recall: 0.9418 - auc: 0.9956 - prc: 0.9881 - val_loss: 0.6860 - val_tp: 1745.0000 - val_fp: 503.0000 - val_tn: 6304.0000 - val_fn: 524.0000 - val_accuracy: 0.7721 - val_precision: 0.7762 - val_recall: 0.7691 - val_auc: 0.9348 - val_prc: 0.8392\n",
      "Epoch 152/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1323 - tp: 8597.0000 - fp: 338.0000 - tn: 26890.0000 - fn: 479.0000 - accuracy: 0.9569 - precision: 0.9622 - recall: 0.9472 - auc: 0.9965 - prc: 0.9898 - val_loss: 0.6847 - val_tp: 1748.0000 - val_fp: 490.0000 - val_tn: 6317.0000 - val_fn: 521.0000 - val_accuracy: 0.7766 - val_precision: 0.7811 - val_recall: 0.7704 - val_auc: 0.9354 - val_prc: 0.8411\n",
      "Epoch 153/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1269 - tp: 8595.0000 - fp: 336.0000 - tn: 26892.0000 - fn: 481.0000 - accuracy: 0.9550 - precision: 0.9624 - recall: 0.9470 - auc: 0.9966 - prc: 0.9905 - val_loss: 0.6896 - val_tp: 1743.0000 - val_fp: 502.0000 - val_tn: 6305.0000 - val_fn: 526.0000 - val_accuracy: 0.7708 - val_precision: 0.7764 - val_recall: 0.7682 - val_auc: 0.9344 - val_prc: 0.8397\n",
      "Epoch 154/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.1335 - tp: 8565.0000 - fp: 346.0000 - tn: 26882.0000 - fn: 511.0000 - accuracy: 0.9539 - precision: 0.9612 - recall: 0.9437 - auc: 0.9964 - prc: 0.9898 - val_loss: 0.6798 - val_tp: 1760.0000 - val_fp: 486.0000 - val_tn: 6321.0000 - val_fn: 509.0000 - val_accuracy: 0.7788 - val_precision: 0.7836 - val_recall: 0.7757 - val_auc: 0.9370 - val_prc: 0.8439\n",
      "Epoch 155/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1292 - tp: 8564.0000 - fp: 354.0000 - tn: 26874.0000 - fn: 512.0000 - accuracy: 0.9525 - precision: 0.9603 - recall: 0.9436 - auc: 0.9967 - prc: 0.9907 - val_loss: 0.6857 - val_tp: 1755.0000 - val_fp: 485.0000 - val_tn: 6322.0000 - val_fn: 514.0000 - val_accuracy: 0.7796 - val_precision: 0.7835 - val_recall: 0.7735 - val_auc: 0.9362 - val_prc: 0.8411\n",
      "Epoch 156/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.1285 - tp: 8603.0000 - fp: 325.0000 - tn: 26903.0000 - fn: 473.0000 - accuracy: 0.9566 - precision: 0.9636 - recall: 0.9479 - auc: 0.9965 - prc: 0.9901 - val_loss: 0.6771 - val_tp: 1765.0000 - val_fp: 483.0000 - val_tn: 6324.0000 - val_fn: 504.0000 - val_accuracy: 0.7823 - val_precision: 0.7851 - val_recall: 0.7779 - val_auc: 0.9378 - val_prc: 0.8450\n",
      "Epoch 157/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.1331 - tp: 8578.0000 - fp: 361.0000 - tn: 26867.0000 - fn: 498.0000 - accuracy: 0.9533 - precision: 0.9596 - recall: 0.9451 - auc: 0.9960 - prc: 0.9890 - val_loss: 0.6774 - val_tp: 1767.0000 - val_fp: 482.0000 - val_tn: 6325.0000 - val_fn: 502.0000 - val_accuracy: 0.7823 - val_precision: 0.7857 - val_recall: 0.7788 - val_auc: 0.9380 - val_prc: 0.8463\n",
      "Epoch 158/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1344 - tp: 8580.0000 - fp: 331.0000 - tn: 26897.0000 - fn: 496.0000 - accuracy: 0.9554 - precision: 0.9629 - recall: 0.9454 - auc: 0.9969 - prc: 0.9914 - val_loss: 0.6921 - val_tp: 1756.0000 - val_fp: 497.0000 - val_tn: 6310.0000 - val_fn: 513.0000 - val_accuracy: 0.7770 - val_precision: 0.7794 - val_recall: 0.7739 - val_auc: 0.9358 - val_prc: 0.8406\n",
      "Epoch 159/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.1279 - tp: 8642.0000 - fp: 320.0000 - tn: 26908.0000 - fn: 434.0000 - accuracy: 0.9598 - precision: 0.9643 - recall: 0.9522 - auc: 0.9970 - prc: 0.9916 - val_loss: 0.6918 - val_tp: 1755.0000 - val_fp: 496.0000 - val_tn: 6311.0000 - val_fn: 514.0000 - val_accuracy: 0.7766 - val_precision: 0.7797 - val_recall: 0.7735 - val_auc: 0.9363 - val_prc: 0.8415\n",
      "Epoch 160/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1263 - tp: 8597.0000 - fp: 333.0000 - tn: 26895.0000 - fn: 479.0000 - accuracy: 0.9561 - precision: 0.9627 - recall: 0.9472 - auc: 0.9963 - prc: 0.9900 - val_loss: 0.7173 - val_tp: 1731.0000 - val_fp: 519.0000 - val_tn: 6288.0000 - val_fn: 538.0000 - val_accuracy: 0.7655 - val_precision: 0.7693 - val_recall: 0.7629 - val_auc: 0.9323 - val_prc: 0.8303\n",
      "Epoch 161/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1259 - tp: 8637.0000 - fp: 299.0000 - tn: 26929.0000 - fn: 439.0000 - accuracy: 0.9586 - precision: 0.9665 - recall: 0.9516 - auc: 0.9972 - prc: 0.9924 - val_loss: 0.6803 - val_tp: 1765.0000 - val_fp: 481.0000 - val_tn: 6326.0000 - val_fn: 504.0000 - val_accuracy: 0.7810 - val_precision: 0.7858 - val_recall: 0.7779 - val_auc: 0.9382 - val_prc: 0.8472\n",
      "Epoch 162/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1173 - tp: 8643.0000 - fp: 293.0000 - tn: 26935.0000 - fn: 433.0000 - accuracy: 0.9613 - precision: 0.9672 - recall: 0.9523 - auc: 0.9969 - prc: 0.9913 - val_loss: 0.6588 - val_tp: 1776.0000 - val_fp: 456.0000 - val_tn: 6351.0000 - val_fn: 493.0000 - val_accuracy: 0.7937 - val_precision: 0.7957 - val_recall: 0.7827 - val_auc: 0.9413 - val_prc: 0.8587\n",
      "Epoch 163/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1162 - tp: 8641.0000 - fp: 309.0000 - tn: 26919.0000 - fn: 435.0000 - accuracy: 0.9595 - precision: 0.9655 - recall: 0.9521 - auc: 0.9971 - prc: 0.9921 - val_loss: 0.6684 - val_tp: 1775.0000 - val_fp: 472.0000 - val_tn: 6335.0000 - val_fn: 494.0000 - val_accuracy: 0.7854 - val_precision: 0.7899 - val_recall: 0.7823 - val_auc: 0.9400 - val_prc: 0.8531\n",
      "Epoch 164/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.1186 - tp: 8673.0000 - fp: 279.0000 - tn: 26949.0000 - fn: 403.0000 - accuracy: 0.9635 - precision: 0.9688 - recall: 0.9556 - auc: 0.9977 - prc: 0.9935 - val_loss: 0.6486 - val_tp: 1803.0000 - val_fp: 440.0000 - val_tn: 6367.0000 - val_fn: 466.0000 - val_accuracy: 0.8008 - val_precision: 0.8038 - val_recall: 0.7946 - val_auc: 0.9433 - val_prc: 0.8631\n",
      "Epoch 165/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1133 - tp: 8662.0000 - fp: 307.0000 - tn: 26921.0000 - fn: 414.0000 - accuracy: 0.9611 - precision: 0.9658 - recall: 0.9544 - auc: 0.9974 - prc: 0.9927 - val_loss: 0.6510 - val_tp: 1795.0000 - val_fp: 441.0000 - val_tn: 6366.0000 - val_fn: 474.0000 - val_accuracy: 0.7973 - val_precision: 0.8028 - val_recall: 0.7911 - val_auc: 0.9431 - val_prc: 0.8625\n",
      "Epoch 166/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1147 - tp: 8684.0000 - fp: 258.0000 - tn: 26970.0000 - fn: 392.0000 - accuracy: 0.9662 - precision: 0.9711 - recall: 0.9568 - auc: 0.9974 - prc: 0.9928 - val_loss: 0.6692 - val_tp: 1784.0000 - val_fp: 464.0000 - val_tn: 6343.0000 - val_fn: 485.0000 - val_accuracy: 0.7907 - val_precision: 0.7936 - val_recall: 0.7862 - val_auc: 0.9410 - val_prc: 0.8535\n",
      "Epoch 167/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.1120 - tp: 8667.0000 - fp: 279.0000 - tn: 26949.0000 - fn: 409.0000 - accuracy: 0.9642 - precision: 0.9688 - recall: 0.9549 - auc: 0.9974 - prc: 0.9929 - val_loss: 0.6753 - val_tp: 1776.0000 - val_fp: 478.0000 - val_tn: 6329.0000 - val_fn: 493.0000 - val_accuracy: 0.7849 - val_precision: 0.7879 - val_recall: 0.7827 - val_auc: 0.9396 - val_prc: 0.8532\n",
      "Epoch 168/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1135 - tp: 8687.0000 - fp: 270.0000 - tn: 26958.0000 - fn: 389.0000 - accuracy: 0.9643 - precision: 0.9699 - recall: 0.9571 - auc: 0.9975 - prc: 0.9929 - val_loss: 0.6543 - val_tp: 1809.0000 - val_fp: 440.0000 - val_tn: 6367.0000 - val_fn: 460.0000 - val_accuracy: 0.7999 - val_precision: 0.8044 - val_recall: 0.7973 - val_auc: 0.9435 - val_prc: 0.8610\n",
      "Epoch 169/200\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 0.1144 - tp: 8659.0000 - fp: 261.0000 - tn: 26967.0000 - fn: 417.0000 - accuracy: 0.9635 - precision: 0.9707 - recall: 0.9541 - auc: 0.9976 - prc: 0.9931 - val_loss: 0.6577 - val_tp: 1798.0000 - val_fp: 446.0000 - val_tn: 6361.0000 - val_fn: 471.0000 - val_accuracy: 0.7968 - val_precision: 0.8012 - val_recall: 0.7924 - val_auc: 0.9429 - val_prc: 0.8608\n",
      "Epoch 170/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1079 - tp: 8670.0000 - fp: 274.0000 - tn: 26954.0000 - fn: 406.0000 - accuracy: 0.9634 - precision: 0.9694 - recall: 0.9553 - auc: 0.9979 - prc: 0.9940 - val_loss: 0.6455 - val_tp: 1811.0000 - val_fp: 433.0000 - val_tn: 6374.0000 - val_fn: 458.0000 - val_accuracy: 0.8034 - val_precision: 0.8070 - val_recall: 0.7981 - val_auc: 0.9446 - val_prc: 0.8647\n",
      "Epoch 171/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1005 - tp: 8704.0000 - fp: 258.0000 - tn: 26970.0000 - fn: 372.0000 - accuracy: 0.9675 - precision: 0.9712 - recall: 0.9590 - auc: 0.9981 - prc: 0.9945 - val_loss: 0.6448 - val_tp: 1816.0000 - val_fp: 433.0000 - val_tn: 6374.0000 - val_fn: 453.0000 - val_accuracy: 0.8030 - val_precision: 0.8075 - val_recall: 0.8004 - val_auc: 0.9446 - val_prc: 0.8646\n",
      "Epoch 172/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.1053 - tp: 8691.0000 - fp: 254.0000 - tn: 26974.0000 - fn: 385.0000 - accuracy: 0.9656 - precision: 0.9716 - recall: 0.9576 - auc: 0.9980 - prc: 0.9946 - val_loss: 0.6636 - val_tp: 1795.0000 - val_fp: 457.0000 - val_tn: 6350.0000 - val_fn: 474.0000 - val_accuracy: 0.7951 - val_precision: 0.7971 - val_recall: 0.7911 - val_auc: 0.9420 - val_prc: 0.8573\n",
      "Epoch 173/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.1015 - tp: 8728.0000 - fp: 228.0000 - tn: 27000.0000 - fn: 348.0000 - accuracy: 0.9683 - precision: 0.9745 - recall: 0.9617 - auc: 0.9979 - prc: 0.9943 - val_loss: 0.6332 - val_tp: 1821.0000 - val_fp: 430.0000 - val_tn: 6377.0000 - val_fn: 448.0000 - val_accuracy: 0.8065 - val_precision: 0.8090 - val_recall: 0.8026 - val_auc: 0.9469 - val_prc: 0.8695\n",
      "Epoch 174/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0999 - tp: 8697.0000 - fp: 270.0000 - tn: 26958.0000 - fn: 379.0000 - accuracy: 0.9651 - precision: 0.9699 - recall: 0.9582 - auc: 0.9979 - prc: 0.9940 - val_loss: 0.6362 - val_tp: 1820.0000 - val_fp: 425.0000 - val_tn: 6382.0000 - val_fn: 449.0000 - val_accuracy: 0.8061 - val_precision: 0.8107 - val_recall: 0.8021 - val_auc: 0.9472 - val_prc: 0.8701\n",
      "Epoch 175/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0983 - tp: 8752.0000 - fp: 222.0000 - tn: 27006.0000 - fn: 324.0000 - accuracy: 0.9714 - precision: 0.9753 - recall: 0.9643 - auc: 0.9980 - prc: 0.9947 - val_loss: 0.6585 - val_tp: 1800.0000 - val_fp: 450.0000 - val_tn: 6357.0000 - val_fn: 469.0000 - val_accuracy: 0.7964 - val_precision: 0.8000 - val_recall: 0.7933 - val_auc: 0.9435 - val_prc: 0.8589\n",
      "Epoch 176/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0976 - tp: 8745.0000 - fp: 227.0000 - tn: 27001.0000 - fn: 331.0000 - accuracy: 0.9694 - precision: 0.9747 - recall: 0.9635 - auc: 0.9983 - prc: 0.9952 - val_loss: 0.6232 - val_tp: 1826.0000 - val_fp: 414.0000 - val_tn: 6393.0000 - val_fn: 443.0000 - val_accuracy: 0.8092 - val_precision: 0.8152 - val_recall: 0.8048 - val_auc: 0.9481 - val_prc: 0.8739\n",
      "Epoch 177/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0974 - tp: 8744.0000 - fp: 227.0000 - tn: 27001.0000 - fn: 332.0000 - accuracy: 0.9703 - precision: 0.9747 - recall: 0.9634 - auc: 0.9984 - prc: 0.9954 - val_loss: 0.6280 - val_tp: 1826.0000 - val_fp: 424.0000 - val_tn: 6383.0000 - val_fn: 443.0000 - val_accuracy: 0.8092 - val_precision: 0.8116 - val_recall: 0.8048 - val_auc: 0.9482 - val_prc: 0.8721\n",
      "Epoch 178/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0951 - tp: 8747.0000 - fp: 222.0000 - tn: 27006.0000 - fn: 329.0000 - accuracy: 0.9700 - precision: 0.9752 - recall: 0.9638 - auc: 0.9983 - prc: 0.9955 - val_loss: 0.6430 - val_tp: 1822.0000 - val_fp: 434.0000 - val_tn: 6373.0000 - val_fn: 447.0000 - val_accuracy: 0.8061 - val_precision: 0.8076 - val_recall: 0.8030 - val_auc: 0.9461 - val_prc: 0.8681\n",
      "Epoch 179/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0955 - tp: 8742.0000 - fp: 214.0000 - tn: 27014.0000 - fn: 334.0000 - accuracy: 0.9707 - precision: 0.9761 - recall: 0.9632 - auc: 0.9981 - prc: 0.9949 - val_loss: 0.6220 - val_tp: 1827.0000 - val_fp: 419.0000 - val_tn: 6388.0000 - val_fn: 442.0000 - val_accuracy: 0.8105 - val_precision: 0.8134 - val_recall: 0.8052 - val_auc: 0.9491 - val_prc: 0.8750\n",
      "Epoch 180/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0934 - tp: 8744.0000 - fp: 216.0000 - tn: 27012.0000 - fn: 332.0000 - accuracy: 0.9714 - precision: 0.9759 - recall: 0.9634 - auc: 0.9984 - prc: 0.9955 - val_loss: 0.6156 - val_tp: 1838.0000 - val_fp: 411.0000 - val_tn: 6396.0000 - val_fn: 431.0000 - val_accuracy: 0.8145 - val_precision: 0.8173 - val_recall: 0.8100 - val_auc: 0.9504 - val_prc: 0.8771\n",
      "Epoch 181/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0880 - tp: 8802.0000 - fp: 173.0000 - tn: 27055.0000 - fn: 274.0000 - accuracy: 0.9765 - precision: 0.9807 - recall: 0.9698 - auc: 0.9987 - prc: 0.9962 - val_loss: 0.6305 - val_tp: 1835.0000 - val_fp: 418.0000 - val_tn: 6389.0000 - val_fn: 434.0000 - val_accuracy: 0.8114 - val_precision: 0.8145 - val_recall: 0.8087 - val_auc: 0.9487 - val_prc: 0.8723\n",
      "Epoch 182/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0896 - tp: 8791.0000 - fp: 186.0000 - tn: 27042.0000 - fn: 285.0000 - accuracy: 0.9747 - precision: 0.9793 - recall: 0.9686 - auc: 0.9988 - prc: 0.9966 - val_loss: 0.6019 - val_tp: 1851.0000 - val_fp: 385.0000 - val_tn: 6422.0000 - val_fn: 418.0000 - val_accuracy: 0.8228 - val_precision: 0.8278 - val_recall: 0.8158 - val_auc: 0.9520 - val_prc: 0.8835\n",
      "Epoch 183/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0899 - tp: 8769.0000 - fp: 213.0000 - tn: 27015.0000 - fn: 307.0000 - accuracy: 0.9710 - precision: 0.9763 - recall: 0.9662 - auc: 0.9984 - prc: 0.9956 - val_loss: 0.6263 - val_tp: 1843.0000 - val_fp: 409.0000 - val_tn: 6398.0000 - val_fn: 426.0000 - val_accuracy: 0.8162 - val_precision: 0.8184 - val_recall: 0.8123 - val_auc: 0.9495 - val_prc: 0.8734\n",
      "Epoch 184/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0916 - tp: 8778.0000 - fp: 194.0000 - tn: 27034.0000 - fn: 298.0000 - accuracy: 0.9743 - precision: 0.9784 - recall: 0.9672 - auc: 0.9986 - prc: 0.9962 - val_loss: 0.6066 - val_tp: 1859.0000 - val_fp: 386.0000 - val_tn: 6421.0000 - val_fn: 410.0000 - val_accuracy: 0.8233 - val_precision: 0.8281 - val_recall: 0.8193 - val_auc: 0.9518 - val_prc: 0.8817\n",
      "Epoch 185/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0927 - tp: 8762.0000 - fp: 228.0000 - tn: 27000.0000 - fn: 314.0000 - accuracy: 0.9716 - precision: 0.9746 - recall: 0.9654 - auc: 0.9985 - prc: 0.9959 - val_loss: 0.6608 - val_tp: 1815.0000 - val_fp: 441.0000 - val_tn: 6366.0000 - val_fn: 454.0000 - val_accuracy: 0.8026 - val_precision: 0.8045 - val_recall: 0.7999 - val_auc: 0.9453 - val_prc: 0.8627\n",
      "Epoch 186/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0867 - tp: 8769.0000 - fp: 211.0000 - tn: 27017.0000 - fn: 307.0000 - accuracy: 0.9719 - precision: 0.9765 - recall: 0.9662 - auc: 0.9983 - prc: 0.9954 - val_loss: 0.6244 - val_tp: 1843.0000 - val_fp: 414.0000 - val_tn: 6393.0000 - val_fn: 426.0000 - val_accuracy: 0.8158 - val_precision: 0.8166 - val_recall: 0.8123 - val_auc: 0.9497 - val_prc: 0.8759\n",
      "Epoch 187/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0891 - tp: 8777.0000 - fp: 209.0000 - tn: 27019.0000 - fn: 299.0000 - accuracy: 0.9725 - precision: 0.9767 - recall: 0.9671 - auc: 0.9986 - prc: 0.9960 - val_loss: 0.6062 - val_tp: 1865.0000 - val_fp: 380.0000 - val_tn: 6427.0000 - val_fn: 404.0000 - val_accuracy: 0.8272 - val_precision: 0.8307 - val_recall: 0.8219 - val_auc: 0.9524 - val_prc: 0.8820\n",
      "Epoch 188/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0813 - tp: 8806.0000 - fp: 179.0000 - tn: 27049.0000 - fn: 270.0000 - accuracy: 0.9757 - precision: 0.9801 - recall: 0.9703 - auc: 0.9991 - prc: 0.9974 - val_loss: 0.6096 - val_tp: 1863.0000 - val_fp: 390.0000 - val_tn: 6417.0000 - val_fn: 406.0000 - val_accuracy: 0.8228 - val_precision: 0.8269 - val_recall: 0.8211 - val_auc: 0.9521 - val_prc: 0.8820\n",
      "Epoch 189/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0925 - tp: 8768.0000 - fp: 208.0000 - tn: 27020.0000 - fn: 308.0000 - accuracy: 0.9718 - precision: 0.9768 - recall: 0.9661 - auc: 0.9985 - prc: 0.9958 - val_loss: 0.6257 - val_tp: 1846.0000 - val_fp: 409.0000 - val_tn: 6398.0000 - val_fn: 423.0000 - val_accuracy: 0.8158 - val_precision: 0.8186 - val_recall: 0.8136 - val_auc: 0.9500 - val_prc: 0.8755\n",
      "Epoch 190/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0843 - tp: 8777.0000 - fp: 201.0000 - tn: 27027.0000 - fn: 299.0000 - accuracy: 0.9744 - precision: 0.9776 - recall: 0.9671 - auc: 0.9985 - prc: 0.9961 - val_loss: 0.5894 - val_tp: 1882.0000 - val_fp: 363.0000 - val_tn: 6444.0000 - val_fn: 387.0000 - val_accuracy: 0.8334 - val_precision: 0.8383 - val_recall: 0.8294 - val_auc: 0.9544 - val_prc: 0.8884\n",
      "Epoch 191/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0804 - tp: 8782.0000 - fp: 198.0000 - tn: 27030.0000 - fn: 294.0000 - accuracy: 0.9740 - precision: 0.9780 - recall: 0.9676 - auc: 0.9987 - prc: 0.9964 - val_loss: 0.5851 - val_tp: 1880.0000 - val_fp: 362.0000 - val_tn: 6445.0000 - val_fn: 389.0000 - val_accuracy: 0.8330 - val_precision: 0.8385 - val_recall: 0.8286 - val_auc: 0.9549 - val_prc: 0.8904\n",
      "Epoch 192/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.0828 - tp: 8795.0000 - fp: 181.0000 - tn: 27047.0000 - fn: 281.0000 - accuracy: 0.9754 - precision: 0.9798 - recall: 0.9690 - auc: 0.9988 - prc: 0.9965 - val_loss: 0.5891 - val_tp: 1877.0000 - val_fp: 373.0000 - val_tn: 6434.0000 - val_fn: 392.0000 - val_accuracy: 0.8308 - val_precision: 0.8342 - val_recall: 0.8272 - val_auc: 0.9544 - val_prc: 0.8883\n",
      "Epoch 193/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.0835 - tp: 8787.0000 - fp: 195.0000 - tn: 27033.0000 - fn: 289.0000 - accuracy: 0.9751 - precision: 0.9783 - recall: 0.9682 - auc: 0.9988 - prc: 0.9965 - val_loss: 0.5982 - val_tp: 1871.0000 - val_fp: 382.0000 - val_tn: 6425.0000 - val_fn: 398.0000 - val_accuracy: 0.8281 - val_precision: 0.8304 - val_recall: 0.8246 - val_auc: 0.9537 - val_prc: 0.8852\n",
      "Epoch 194/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.0760 - tp: 8820.0000 - fp: 173.0000 - tn: 27055.0000 - fn: 256.0000 - accuracy: 0.9772 - precision: 0.9808 - recall: 0.9718 - auc: 0.9990 - prc: 0.9974 - val_loss: 0.5898 - val_tp: 1881.0000 - val_fp: 371.0000 - val_tn: 6436.0000 - val_fn: 388.0000 - val_accuracy: 0.8325 - val_precision: 0.8353 - val_recall: 0.8290 - val_auc: 0.9552 - val_prc: 0.8894\n",
      "Epoch 195/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.0807 - tp: 8808.0000 - fp: 188.0000 - tn: 27040.0000 - fn: 268.0000 - accuracy: 0.9755 - precision: 0.9791 - recall: 0.9705 - auc: 0.9989 - prc: 0.9969 - val_loss: 0.5835 - val_tp: 1884.0000 - val_fp: 371.0000 - val_tn: 6436.0000 - val_fn: 385.0000 - val_accuracy: 0.8343 - val_precision: 0.8355 - val_recall: 0.8303 - val_auc: 0.9556 - val_prc: 0.8908\n",
      "Epoch 196/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0780 - tp: 8814.0000 - fp: 165.0000 - tn: 27063.0000 - fn: 262.0000 - accuracy: 0.9775 - precision: 0.9816 - recall: 0.9711 - auc: 0.9989 - prc: 0.9970 - val_loss: 0.5759 - val_tp: 1894.0000 - val_fp: 316.0000 - val_tn: 6491.0000 - val_fn: 375.0000 - val_accuracy: 0.8422 - val_precision: 0.8570 - val_recall: 0.8347 - val_auc: 0.9560 - val_prc: 0.8937\n",
      "Epoch 197/200\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.0795 - tp: 8834.0000 - fp: 160.0000 - tn: 27068.0000 - fn: 242.0000 - accuracy: 0.9785 - precision: 0.9822 - recall: 0.9733 - auc: 0.9989 - prc: 0.9970 - val_loss: 0.5852 - val_tp: 1887.0000 - val_fp: 362.0000 - val_tn: 6445.0000 - val_fn: 382.0000 - val_accuracy: 0.8347 - val_precision: 0.8390 - val_recall: 0.8316 - val_auc: 0.9557 - val_prc: 0.8922\n",
      "Epoch 198/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0748 - tp: 8826.0000 - fp: 159.0000 - tn: 27069.0000 - fn: 250.0000 - accuracy: 0.9780 - precision: 0.9823 - recall: 0.9725 - auc: 0.9992 - prc: 0.9977 - val_loss: 0.6200 - val_tp: 1858.0000 - val_fp: 399.0000 - val_tn: 6408.0000 - val_fn: 411.0000 - val_accuracy: 0.8219 - val_precision: 0.8232 - val_recall: 0.8189 - val_auc: 0.9519 - val_prc: 0.8792\n",
      "Epoch 199/200\n",
      "91/91 [==============================] - 4s 47ms/step - loss: 0.0741 - tp: 8834.0000 - fp: 168.0000 - tn: 27060.0000 - fn: 242.0000 - accuracy: 0.9772 - precision: 0.9813 - recall: 0.9733 - auc: 0.9990 - prc: 0.9975 - val_loss: 0.5761 - val_tp: 1900.0000 - val_fp: 351.0000 - val_tn: 6456.0000 - val_fn: 369.0000 - val_accuracy: 0.8400 - val_precision: 0.8441 - val_recall: 0.8374 - val_auc: 0.9576 - val_prc: 0.8983\n",
      "Epoch 200/200\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 0.0732 - tp: 8825.0000 - fp: 168.0000 - tn: 27060.0000 - fn: 251.0000 - accuracy: 0.9780 - precision: 0.9813 - recall: 0.9723 - auc: 0.9990 - prc: 0.9970 - val_loss: 0.5782 - val_tp: 1893.0000 - val_fp: 350.0000 - val_tn: 6457.0000 - val_fn: 376.0000 - val_accuracy: 0.8387 - val_precision: 0.8440 - val_recall: 0.8343 - val_auc: 0.9572 - val_prc: 0.8973\n"
     ]
    }
   ],
   "source": [
    "# model fitting\n",
    "hist_LSTM_embedding=LSTM_embedding.fit([x_train,x_train_meta],lbl_train,epochs=200,\n",
    "                               validation_data=([x_test,x_test_meta],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               shuffle=True,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c90603e-a3b2-441f-8d5f-7f29a4bb55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# LSTM_embedding.save('LSTM_embedding_model.h5') # first 200 epochs\n",
    "LSTM_embedding.save('LSTM_embedding_model2.h5') # additional 200 epochs (total of 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988151f5-6c16-4e1d-a303-ff4b2a0efd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# from keras.models import load_model\n",
    "# LSTM_embedding = load_model('LSTM_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eERKY868be_e",
   "metadata": {
    "id": "eERKY868be_e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 2s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test_LSTM=LSTM_embedding.predict([x_test,x_test_meta]) # predict\n",
    "lbl_pred_LSTM=np.argmax(pred_test_LSTM,axis=1).astype(int) # predicted value as its index\n",
    "lbl_real_LSTM=np.argmax(lbl_test,axis=1).astype(int) # real value as its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b464d10-cc3c-4ce9-98c7-9f54407e4bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.26.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.5.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.8.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->mlxtend) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m138.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "agxei54m_1wT",
   "metadata": {
    "id": "agxei54m_1wT"
   },
   "outputs": [],
   "source": [
    "# 결과 보고서를 위해 필요한 패키지 import\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "B3CgLTN4IyUH",
   "metadata": {
    "id": "B3CgLTN4IyUH"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lbl_pred_LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lbl_test), \u001b[43mlbl_pred_LSTM\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lbl_pred_LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(lbl_test), lbl_pred_LSTM.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "XQX3joaaEgOA",
   "metadata": {
    "id": "XQX3joaaEgOA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# BootstrapR2 함수를 통해 결과보고서 작성\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    # Initial Performance Metrics Calculation\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True, zero_division=0)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision']\n",
    "    recall= classfi_report['macro avg']['recall']\n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test, multi_class='ovr',average='weighted')\n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1))\n",
    "\n",
    "\n",
    "    # bootstrapping\n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True, zero_division=0)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision']\n",
    "        recall1[i]= classfi_report['macro avg']['recall']\n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        # print(f1_score1[i])\n",
    "        # print(lbl_test1.shape)\n",
    "        # print(pred_test1.shape)\n",
    "        # print(np.min(pred_test1), np.max(pred_test1))\n",
    "        # print(np.sum(pred_test1, axis=1))\n",
    "        # print(np.isnan(pred_test1).any())\n",
    "        # print(np.isinf(pred_test1).any())\n",
    "\n",
    "        # if the data is imbalance ( such as this artificial_data, ValueError occurs, so we pass for now )\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(lbl_test1, pred_test1, multi_class='ovr', average='weighted')\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "#----------------------\n",
    "\n",
    "    # Recalculate Baseline Metrics for Consistency Check\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True, zero_division=0)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision']\n",
    "    recall= classfi_report['macro avg']['recall']\n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted')\n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "\n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d9dffe0",
   "metadata": {
    "id": "2d9dffe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC\n",
      "0.965  -0.965\n",
      "0.965 + -0.965\n",
      "f1_score\n",
      "0.685  -0.018\n",
      "0.685 + 0.017\n",
      "accuracy\n",
      "0.839  -0.013\n",
      "0.839 + 0.012\n",
      "precision\n",
      "0.682  -0.015\n",
      "0.682 + 0.015\n",
      "recall\n",
      "0.701  -0.022\n",
      "0.701 + 0.022\n"
     ]
    }
   ],
   "source": [
    "roc_LSTM, f1_score_LSTM, accuracy_LSTM, precision_LSTM, recall_LSTM =BootstrapR2(pred_test_LSTM, lbl_test, numboot=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "610edc58",
   "metadata": {
    "id": "610edc58"
   },
   "outputs": [],
   "source": [
    "################################################################################gru model\n",
    "\n",
    "def Create_GRU(data, nb_words, maxlen, embedding_dim, data_meta):\n",
    "\n",
    "\n",
    "  # meta input\n",
    "  input_meta = keras.layers.Input(shape=data_meta.shape[1:], dtype='float64', name=\"input_meta\")\n",
    "  layer_meta = keras.layers.Conv1D(filters=16,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta.1\")(input_meta)\n",
    "  layer_meta = keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "  layer_meta = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(1),\n",
    "                                name=\"cov_meta.2\")(layer_meta)\n",
    "  layer_meta = keras.layers.GRU(128, return_sequences=True)(layer_meta)\n",
    "  layer_meta = keras.layers.GlobalAveragePooling1D(name=\"avg_meta\")(layer_meta)\n",
    "\n",
    "\n",
    "  # txt input\n",
    "  word_input_cnn = keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")\n",
    "  em_layer_cnn = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data = keras.layers.BatchNormalization()(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.1)(layer_data)\n",
    "\n",
    "  layer_data = keras.layers.Conv1D(filters=32,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data = keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool1\")(layer_data)\n",
    "  layer_data = keras.layers.Conv1D(filters=32,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_3\")(layer_data)\n",
    "  layer_data = keras.layers.BatchNormalization()(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 strides=(2),\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_4\")(layer_data)\n",
    "  layer_data = keras.layers.BatchNormalization()(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.1)(layer_data)\n",
    "  #layer_data= keras.layers.LSTM(32)(layer_data)\n",
    "  layer_data = keras.layers.GRU(128, return_sequences=True)(layer_data)\n",
    "  layer_data = keras.layers.GlobalAveragePooling1D(name=\"data_maxpool2\")(layer_data)\n",
    "\n",
    "\n",
    "\n",
    "  # hybrid\n",
    "  layer_last =keras.layers.Concatenate(axis=-1)([layer_meta,layer_data])\n",
    "  layer_last=keras.layers.Flatten()(layer_last)\n",
    "\n",
    "\n",
    "  layer_last =keras.layers.Dense(64,activation=\"relu\",\n",
    "                                 name=\"dens_2\")(layer_last)\n",
    "  layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "  output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.00001)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=METRICS)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a95567ee",
   "metadata": {
    "id": "a95567ee"
   },
   "outputs": [],
   "source": [
    "GRU_embedding=Create_GRU(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta=x_train_meta_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "PGbeuLzSoYRL",
   "metadata": {
    "id": "PGbeuLzSoYRL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "91/91 [==============================] - 19s 135ms/step - loss: 1.4109 - tp: 1893.0000 - fp: 350.0000 - tn: 33685.0000 - fn: 9452.0000 - accuracy: 0.3349 - precision: 0.8440 - recall: 0.1669 - auc: 0.6504 - prc: 0.4453 - val_loss: 1.3863 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4544 - val_prc: 0.2195\n",
      "Epoch 2/200\n",
      "91/91 [==============================] - 7s 78ms/step - loss: 1.4104 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2139 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4648 - prc: 0.2293 - val_loss: 1.3882 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4742 - val_prc: 0.2239\n",
      "Epoch 3/200\n",
      "91/91 [==============================] - 6s 69ms/step - loss: 1.4093 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2439 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4903 - prc: 0.2449 - val_loss: 1.3880 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5640 - val_prc: 0.2710\n",
      "Epoch 4/200\n",
      "91/91 [==============================] - 6s 63ms/step - loss: 1.4096 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2831 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5198 - prc: 0.2725 - val_loss: 1.3884 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5634 - val_prc: 0.2705\n",
      "Epoch 5/200\n",
      "91/91 [==============================] - 6s 61ms/step - loss: 1.4084 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3120 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5470 - prc: 0.2985 - val_loss: 1.3765 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.2591 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5565 - val_prc: 0.2658\n",
      "Epoch 6/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 1.4086 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3196 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5470 - prc: 0.3034 - val_loss: 1.3513 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.2613 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7203 - val_prc: 0.3970\n",
      "Epoch 7/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.4088 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2482 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5075 - prc: 0.2563 - val_loss: 1.3295 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1741 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6154 - val_prc: 0.2800\n",
      "Epoch 8/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 1.4084 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5217 - prc: 0.2684 - val_loss: 1.5112 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3719 - val_prc: 0.1977\n",
      "Epoch 9/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 1.4080 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2814 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5338 - prc: 0.2871 - val_loss: 1.5068 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.2591 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3579 - val_prc: 0.2224\n",
      "Epoch 10/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 1.4074 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.1992 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4927 - prc: 0.2391 - val_loss: 1.4096 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.0401 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3967 - val_prc: 0.1934\n",
      "Epoch 11/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 1.4073 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.1937 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4934 - prc: 0.2375 - val_loss: 1.3651 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.2591 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6759 - val_prc: 0.3256\n",
      "Epoch 12/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.4069 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.1873 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4930 - prc: 0.2360 - val_loss: 1.3011 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.5333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7829 - val_prc: 0.5332\n",
      "Epoch 13/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 1.4070 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2346 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5212 - prc: 0.2567 - val_loss: 1.3363 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.5333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7351 - val_prc: 0.5065\n",
      "Epoch 14/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 1.4069 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2324 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5225 - prc: 0.2580 - val_loss: 1.5733 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3302 - val_prc: 0.1906\n",
      "Epoch 15/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 1.4058 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2647 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5389 - prc: 0.2765 - val_loss: 1.6709 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.2888 - val_prc: 0.1891\n",
      "Epoch 16/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 1.4058 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2371 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5268 - prc: 0.2587 - val_loss: 1.5603 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.0401 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3477 - val_prc: 0.1819\n",
      "Epoch 17/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 1.4057 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2318 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5232 - prc: 0.2569 - val_loss: 1.4681 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3934 - val_prc: 0.2049\n",
      "Epoch 18/200\n",
      "91/91 [==============================] - 5s 58ms/step - loss: 1.4056 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2501 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5314 - prc: 0.2635 - val_loss: 1.7498 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3809 - val_prc: 0.2012\n",
      "Epoch 19/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 1.4052 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2457 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5385 - prc: 0.2676 - val_loss: 1.4178 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5757 - val_prc: 0.2785\n",
      "Epoch 20/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.4047 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2675 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5411 - prc: 0.2728 - val_loss: 1.3513 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1829 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6125 - val_prc: 0.3335\n",
      "Epoch 21/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 1.4038 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2554 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5398 - prc: 0.2677 - val_loss: 1.6591 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3578 - val_prc: 0.1974\n",
      "Epoch 22/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 1.4046 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2665 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5431 - prc: 0.2697 - val_loss: 1.3492 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5751 - val_prc: 0.2882\n",
      "Epoch 23/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 1.4037 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3020 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5664 - prc: 0.2976 - val_loss: 1.4917 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4127 - val_prc: 0.2330\n",
      "Epoch 24/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 1.4043 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2784 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5532 - prc: 0.2809 - val_loss: 1.3123 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.6095 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7087 - val_prc: 0.4125\n",
      "Epoch 25/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.4020 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.2829 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5555 - prc: 0.2827 - val_loss: 1.3720 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5665 - val_prc: 0.2739\n",
      "Epoch 26/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 1.4018 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3405 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5876 - prc: 0.3334 - val_loss: 1.3735 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5491 - val_prc: 0.2531\n",
      "Epoch 27/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.4016 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3396 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5878 - prc: 0.3323 - val_loss: 1.3746 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6559 - val_prc: 0.3369\n",
      "Epoch 28/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.4013 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3439 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5938 - prc: 0.3344 - val_loss: 1.3584 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5860 - val_prc: 0.2906\n",
      "Epoch 29/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 1.3997 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3214 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5858 - prc: 0.3159 - val_loss: 1.3128 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.2591 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7044 - val_prc: 0.3447\n",
      "Epoch 30/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 1.3984 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3517 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6036 - prc: 0.3459 - val_loss: 1.2811 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.5333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7983 - val_prc: 0.6796\n",
      "Epoch 31/200\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 1.3983 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3802 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6149 - prc: 0.3666 - val_loss: 1.5190 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5736 - val_prc: 0.2946\n",
      "Epoch 32/200\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 1.3982 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.3571 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6058 - prc: 0.3458 - val_loss: 1.3615 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6730 - val_prc: 0.3384\n",
      "Epoch 33/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 1.3967 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.4113 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6326 - prc: 0.4153 - val_loss: 1.5240 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4262 - val_prc: 0.2671\n",
      "Epoch 34/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.3951 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.4552 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6634 - prc: 0.4704 - val_loss: 1.3940 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5841 - val_prc: 0.2968\n",
      "Epoch 35/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 1.3933 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.4547 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6631 - prc: 0.4744 - val_loss: 1.3788 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5681 - val_prc: 0.2579\n",
      "Epoch 36/200\n",
      "91/91 [==============================] - 5s 59ms/step - loss: 1.3918 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.4301 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6520 - prc: 0.4458 - val_loss: 1.4513 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4128 - val_prc: 0.2319\n",
      "Epoch 37/200\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 1.3900 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.4805 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6800 - prc: 0.5110 - val_loss: 1.3083 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.2591 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7418 - val_prc: 0.4237\n",
      "Epoch 38/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 1.3870 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.5072 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7050 - prc: 0.5574 - val_loss: 1.2459 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.7091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8631 - val_prc: 0.7983\n",
      "Epoch 39/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 1.3833 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.5361 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7189 - prc: 0.5964 - val_loss: 1.2953 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.2591 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6293 - val_prc: 0.3766\n",
      "Epoch 40/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.3771 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.5725 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7502 - prc: 0.6501 - val_loss: 1.3073 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6991 - val_prc: 0.3593\n",
      "Epoch 41/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.3651 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.5960 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7915 - prc: 0.7021 - val_loss: 1.2903 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.1675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7069 - val_prc: 0.4007\n",
      "Epoch 42/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.3296 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9076.0000 - accuracy: 0.6578 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8501 - prc: 0.7590 - val_loss: 1.2479 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.6214 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7655 - val_prc: 0.7377\n",
      "Epoch 43/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 1.1827 - tp: 2641.0000 - fp: 31.0000 - tn: 27197.0000 - fn: 6435.0000 - accuracy: 0.6781 - precision: 0.9884 - recall: 0.2910 - auc: 0.9069 - prc: 0.8103 - val_loss: 0.9874 - val_tp: 1210.0000 - val_fp: 408.0000 - val_tn: 6399.0000 - val_fn: 1059.0000 - val_accuracy: 0.5333 - val_precision: 0.7478 - val_recall: 0.5333 - val_auc: 0.8358 - val_prc: 0.7181\n",
      "Epoch 44/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 1.0395 - tp: 4811.0000 - fp: 68.0000 - tn: 27160.0000 - fn: 4265.0000 - accuracy: 0.7472 - precision: 0.9861 - recall: 0.5301 - auc: 0.9390 - prc: 0.8580 - val_loss: 0.9293 - val_tp: 1210.0000 - val_fp: 395.0000 - val_tn: 6412.0000 - val_fn: 1059.0000 - val_accuracy: 0.5333 - val_precision: 0.7539 - val_recall: 0.5333 - val_auc: 0.8400 - val_prc: 0.7203\n",
      "Epoch 45/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.9829 - tp: 4858.0000 - fp: 64.0000 - tn: 27164.0000 - fn: 4218.0000 - accuracy: 0.7632 - precision: 0.9870 - recall: 0.5353 - auc: 0.9454 - prc: 0.8719 - val_loss: 0.7893 - val_tp: 1210.0000 - val_fp: 132.0000 - val_tn: 6675.0000 - val_fn: 1059.0000 - val_accuracy: 0.5597 - val_precision: 0.9016 - val_recall: 0.5333 - val_auc: 0.8453 - val_prc: 0.7254\n",
      "Epoch 46/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.9356 - tp: 4914.0000 - fp: 69.0000 - tn: 27159.0000 - fn: 4162.0000 - accuracy: 0.7729 - precision: 0.9862 - recall: 0.5414 - auc: 0.9499 - prc: 0.8833 - val_loss: 0.6763 - val_tp: 1210.0000 - val_fp: 26.0000 - val_tn: 6781.0000 - val_fn: 1059.0000 - val_accuracy: 0.5729 - val_precision: 0.9790 - val_recall: 0.5333 - val_auc: 0.9040 - val_prc: 0.7852\n",
      "Epoch 47/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.8748 - tp: 5202.0000 - fp: 92.0000 - tn: 27136.0000 - fn: 3874.0000 - accuracy: 0.7934 - precision: 0.9826 - recall: 0.5732 - auc: 0.9566 - prc: 0.8986 - val_loss: 0.6217 - val_tp: 1210.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 1059.0000 - val_accuracy: 0.7038 - val_precision: 0.9861 - val_recall: 0.5333 - val_auc: 0.9162 - val_prc: 0.8088\n",
      "Epoch 48/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.8310 - tp: 5709.0000 - fp: 151.0000 - tn: 27077.0000 - fn: 3367.0000 - accuracy: 0.8032 - precision: 0.9742 - recall: 0.6290 - auc: 0.9617 - prc: 0.9067 - val_loss: 0.5563 - val_tp: 1210.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 1059.0000 - val_accuracy: 0.7320 - val_precision: 0.9861 - val_recall: 0.5333 - val_auc: 0.9536 - val_prc: 0.8841\n",
      "Epoch 49/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.7676 - tp: 6186.0000 - fp: 389.0000 - tn: 26839.0000 - fn: 2890.0000 - accuracy: 0.8044 - precision: 0.9408 - recall: 0.6816 - auc: 0.9638 - prc: 0.9102 - val_loss: 0.5358 - val_tp: 1210.0000 - val_fp: 186.0000 - val_tn: 6621.0000 - val_fn: 1059.0000 - val_accuracy: 0.5853 - val_precision: 0.8668 - val_recall: 0.5333 - val_auc: 0.9438 - val_prc: 0.8469\n",
      "Epoch 50/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.7268 - tp: 6475.0000 - fp: 775.0000 - tn: 26453.0000 - fn: 2601.0000 - accuracy: 0.8034 - precision: 0.8931 - recall: 0.7134 - auc: 0.9637 - prc: 0.9078 - val_loss: 0.4784 - val_tp: 1562.0000 - val_fp: 16.0000 - val_tn: 6791.0000 - val_fn: 707.0000 - val_accuracy: 0.7342 - val_precision: 0.9899 - val_recall: 0.6884 - val_auc: 0.9682 - val_prc: 0.9086\n",
      "Epoch 51/200\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.6911 - tp: 6731.0000 - fp: 941.0000 - tn: 26287.0000 - fn: 2345.0000 - accuracy: 0.8124 - precision: 0.8773 - recall: 0.7416 - auc: 0.9663 - prc: 0.9131 - val_loss: 0.5444 - val_tp: 1217.0000 - val_fp: 355.0000 - val_tn: 6452.0000 - val_fn: 1052.0000 - val_accuracy: 0.5822 - val_precision: 0.7742 - val_recall: 0.5364 - val_auc: 0.9336 - val_prc: 0.8238\n",
      "Epoch 52/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.6835 - tp: 6895.0000 - fp: 1045.0000 - tn: 26183.0000 - fn: 2181.0000 - accuracy: 0.8173 - precision: 0.8684 - recall: 0.7597 - auc: 0.9676 - prc: 0.9153 - val_loss: 0.6370 - val_tp: 1210.0000 - val_fp: 366.0000 - val_tn: 6441.0000 - val_fn: 1059.0000 - val_accuracy: 0.5408 - val_precision: 0.7678 - val_recall: 0.5333 - val_auc: 0.9161 - val_prc: 0.7952\n",
      "Epoch 53/200\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.6625 - tp: 6975.0000 - fp: 1125.0000 - tn: 26103.0000 - fn: 2101.0000 - accuracy: 0.8226 - precision: 0.8611 - recall: 0.7685 - auc: 0.9685 - prc: 0.9150 - val_loss: 0.6922 - val_tp: 1210.0000 - val_fp: 365.0000 - val_tn: 6442.0000 - val_fn: 1059.0000 - val_accuracy: 0.5416 - val_precision: 0.7683 - val_recall: 0.5333 - val_auc: 0.9168 - val_prc: 0.7960\n",
      "Epoch 54/200\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.6631 - tp: 7011.0000 - fp: 1191.0000 - tn: 26037.0000 - fn: 2065.0000 - accuracy: 0.8191 - precision: 0.8548 - recall: 0.7725 - auc: 0.9683 - prc: 0.9138 - val_loss: 0.6268 - val_tp: 1210.0000 - val_fp: 366.0000 - val_tn: 6441.0000 - val_fn: 1059.0000 - val_accuracy: 0.7959 - val_precision: 0.7678 - val_recall: 0.5333 - val_auc: 0.9331 - val_prc: 0.8380\n",
      "Epoch 55/200\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.6522 - tp: 7086.0000 - fp: 1164.0000 - tn: 26064.0000 - fn: 1990.0000 - accuracy: 0.8207 - precision: 0.8589 - recall: 0.7807 - auc: 0.9697 - prc: 0.9190 - val_loss: 0.4862 - val_tp: 1225.0000 - val_fp: 345.0000 - val_tn: 6462.0000 - val_fn: 1044.0000 - val_accuracy: 0.5822 - val_precision: 0.7803 - val_recall: 0.5399 - val_auc: 0.9426 - val_prc: 0.8440\n",
      "Epoch 56/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.6546 - tp: 7097.0000 - fp: 1190.0000 - tn: 26038.0000 - fn: 1979.0000 - accuracy: 0.8237 - precision: 0.8564 - recall: 0.7820 - auc: 0.9701 - prc: 0.9194 - val_loss: 0.7024 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.5403 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.9068 - val_prc: 0.7857\n",
      "Epoch 57/200\n",
      "91/91 [==============================] - 5s 58ms/step - loss: 0.6469 - tp: 7154.0000 - fp: 1207.0000 - tn: 26021.0000 - fn: 1922.0000 - accuracy: 0.8254 - precision: 0.8556 - recall: 0.7882 - auc: 0.9709 - prc: 0.9214 - val_loss: 0.6826 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.5412 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.9133 - val_prc: 0.7929\n",
      "Epoch 58/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.6164 - tp: 7225.0000 - fp: 1114.0000 - tn: 26114.0000 - fn: 1851.0000 - accuracy: 0.8333 - precision: 0.8664 - recall: 0.7961 - auc: 0.9730 - prc: 0.9263 - val_loss: 0.7063 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.5756 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.9036 - val_prc: 0.7872\n",
      "Epoch 59/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.6407 - tp: 7182.0000 - fp: 1212.0000 - tn: 26016.0000 - fn: 1894.0000 - accuracy: 0.8250 - precision: 0.8556 - recall: 0.7913 - auc: 0.9717 - prc: 0.9237 - val_loss: 0.6360 - val_tp: 1210.0000 - val_fp: 366.0000 - val_tn: 6441.0000 - val_fn: 1059.0000 - val_accuracy: 0.7131 - val_precision: 0.7678 - val_recall: 0.5333 - val_auc: 0.9239 - val_prc: 0.8168\n",
      "Epoch 60/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.6116 - tp: 7249.0000 - fp: 1191.0000 - tn: 26037.0000 - fn: 1827.0000 - accuracy: 0.8312 - precision: 0.8589 - recall: 0.7987 - auc: 0.9735 - prc: 0.9289 - val_loss: 0.7137 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.5394 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.9040 - val_prc: 0.7819\n",
      "Epoch 61/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.6197 - tp: 7230.0000 - fp: 1210.0000 - tn: 26018.0000 - fn: 1846.0000 - accuracy: 0.8287 - precision: 0.8566 - recall: 0.7966 - auc: 0.9728 - prc: 0.9256 - val_loss: 0.6697 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.7942 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.9248 - val_prc: 0.8298\n",
      "Epoch 62/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.6125 - tp: 7210.0000 - fp: 1219.0000 - tn: 26009.0000 - fn: 1866.0000 - accuracy: 0.8288 - precision: 0.8554 - recall: 0.7944 - auc: 0.9724 - prc: 0.9248 - val_loss: 0.5680 - val_tp: 1212.0000 - val_fp: 366.0000 - val_tn: 6441.0000 - val_fn: 1057.0000 - val_accuracy: 0.7964 - val_precision: 0.7681 - val_recall: 0.5342 - val_auc: 0.9440 - val_prc: 0.8592\n",
      "Epoch 63/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.6036 - tp: 7283.0000 - fp: 1184.0000 - tn: 26044.0000 - fn: 1793.0000 - accuracy: 0.8316 - precision: 0.8602 - recall: 0.8024 - auc: 0.9736 - prc: 0.9275 - val_loss: 0.5963 - val_tp: 1210.0000 - val_fp: 357.0000 - val_tn: 6450.0000 - val_fn: 1059.0000 - val_accuracy: 0.5425 - val_precision: 0.7722 - val_recall: 0.5333 - val_auc: 0.9205 - val_prc: 0.8023\n",
      "Epoch 64/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.6056 - tp: 7264.0000 - fp: 1222.0000 - tn: 26006.0000 - fn: 1812.0000 - accuracy: 0.8301 - precision: 0.8560 - recall: 0.8004 - auc: 0.9732 - prc: 0.9271 - val_loss: 0.6266 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.7955 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.9330 - val_prc: 0.8398\n",
      "Epoch 65/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.5923 - tp: 7317.0000 - fp: 1201.0000 - tn: 26027.0000 - fn: 1759.0000 - accuracy: 0.8338 - precision: 0.8590 - recall: 0.8062 - auc: 0.9749 - prc: 0.9313 - val_loss: 0.6190 - val_tp: 1210.0000 - val_fp: 366.0000 - val_tn: 6441.0000 - val_fn: 1059.0000 - val_accuracy: 0.7951 - val_precision: 0.7678 - val_recall: 0.5333 - val_auc: 0.9340 - val_prc: 0.8400\n",
      "Epoch 66/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.5929 - tp: 7318.0000 - fp: 1212.0000 - tn: 26016.0000 - fn: 1758.0000 - accuracy: 0.8343 - precision: 0.8579 - recall: 0.8063 - auc: 0.9749 - prc: 0.9308 - val_loss: 0.6267 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.7973 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.9336 - val_prc: 0.8408\n",
      "Epoch 67/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.5722 - tp: 7321.0000 - fp: 1212.0000 - tn: 26016.0000 - fn: 1755.0000 - accuracy: 0.8340 - precision: 0.8580 - recall: 0.8066 - auc: 0.9752 - prc: 0.9326 - val_loss: 0.6221 - val_tp: 1215.0000 - val_fp: 365.0000 - val_tn: 6442.0000 - val_fn: 1054.0000 - val_accuracy: 0.5685 - val_precision: 0.7690 - val_recall: 0.5355 - val_auc: 0.8916 - val_prc: 0.7669\n",
      "Epoch 68/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5813 - tp: 7333.0000 - fp: 1197.0000 - tn: 26031.0000 - fn: 1743.0000 - accuracy: 0.8375 - precision: 0.8597 - recall: 0.8080 - auc: 0.9759 - prc: 0.9356 - val_loss: 0.6391 - val_tp: 1210.0000 - val_fp: 364.0000 - val_tn: 6443.0000 - val_fn: 1059.0000 - val_accuracy: 0.5685 - val_precision: 0.7687 - val_recall: 0.5333 - val_auc: 0.8923 - val_prc: 0.7708\n",
      "Epoch 69/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.5764 - tp: 7330.0000 - fp: 1206.0000 - tn: 26022.0000 - fn: 1746.0000 - accuracy: 0.8332 - precision: 0.8587 - recall: 0.8076 - auc: 0.9753 - prc: 0.9322 - val_loss: 0.6062 - val_tp: 1214.0000 - val_fp: 362.0000 - val_tn: 6445.0000 - val_fn: 1055.0000 - val_accuracy: 0.5685 - val_precision: 0.7703 - val_recall: 0.5350 - val_auc: 0.9045 - val_prc: 0.7864\n",
      "Epoch 70/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5747 - tp: 7376.0000 - fp: 1179.0000 - tn: 26049.0000 - fn: 1700.0000 - accuracy: 0.8390 - precision: 0.8622 - recall: 0.8127 - auc: 0.9763 - prc: 0.9352 - val_loss: 0.6535 - val_tp: 1210.0000 - val_fp: 368.0000 - val_tn: 6439.0000 - val_fn: 1059.0000 - val_accuracy: 0.7946 - val_precision: 0.7668 - val_recall: 0.5333 - val_auc: 0.9344 - val_prc: 0.8412\n",
      "Epoch 71/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.5823 - tp: 7332.0000 - fp: 1174.0000 - tn: 26054.0000 - fn: 1744.0000 - accuracy: 0.8364 - precision: 0.8620 - recall: 0.8078 - auc: 0.9755 - prc: 0.9313 - val_loss: 0.6789 - val_tp: 1214.0000 - val_fp: 365.0000 - val_tn: 6442.0000 - val_fn: 1055.0000 - val_accuracy: 0.5681 - val_precision: 0.7688 - val_recall: 0.5350 - val_auc: 0.8867 - val_prc: 0.7607\n",
      "Epoch 72/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.5824 - tp: 7343.0000 - fp: 1167.0000 - tn: 26061.0000 - fn: 1733.0000 - accuracy: 0.8380 - precision: 0.8629 - recall: 0.8091 - auc: 0.9764 - prc: 0.9351 - val_loss: 0.5593 - val_tp: 1229.0000 - val_fp: 360.0000 - val_tn: 6447.0000 - val_fn: 1040.0000 - val_accuracy: 0.8096 - val_precision: 0.7734 - val_recall: 0.5416 - val_auc: 0.9574 - val_prc: 0.8804\n",
      "Epoch 73/200\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.5717 - tp: 7272.0000 - fp: 1249.0000 - tn: 25979.0000 - fn: 1804.0000 - accuracy: 0.8305 - precision: 0.8534 - recall: 0.8012 - auc: 0.9751 - prc: 0.9321 - val_loss: 0.5706 - val_tp: 1213.0000 - val_fp: 366.0000 - val_tn: 6441.0000 - val_fn: 1056.0000 - val_accuracy: 0.7964 - val_precision: 0.7682 - val_recall: 0.5346 - val_auc: 0.9501 - val_prc: 0.8632\n",
      "Epoch 74/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.5633 - tp: 7360.0000 - fp: 1173.0000 - tn: 26055.0000 - fn: 1716.0000 - accuracy: 0.8392 - precision: 0.8625 - recall: 0.8109 - auc: 0.9772 - prc: 0.9379 - val_loss: 0.5961 - val_tp: 1236.0000 - val_fp: 260.0000 - val_tn: 6547.0000 - val_fn: 1033.0000 - val_accuracy: 0.5910 - val_precision: 0.8262 - val_recall: 0.5447 - val_auc: 0.9186 - val_prc: 0.8163\n",
      "Epoch 75/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.5562 - tp: 7392.0000 - fp: 1163.0000 - tn: 26065.0000 - fn: 1684.0000 - accuracy: 0.8427 - precision: 0.8641 - recall: 0.8145 - auc: 0.9771 - prc: 0.9361 - val_loss: 0.5810 - val_tp: 1215.0000 - val_fp: 365.0000 - val_tn: 6442.0000 - val_fn: 1054.0000 - val_accuracy: 0.5818 - val_precision: 0.7690 - val_recall: 0.5355 - val_auc: 0.9287 - val_prc: 0.8163\n",
      "Epoch 76/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.5543 - tp: 7393.0000 - fp: 1121.0000 - tn: 26107.0000 - fn: 1683.0000 - accuracy: 0.8456 - precision: 0.8683 - recall: 0.8146 - auc: 0.9776 - prc: 0.9386 - val_loss: 0.6128 - val_tp: 1219.0000 - val_fp: 350.0000 - val_tn: 6457.0000 - val_fn: 1050.0000 - val_accuracy: 0.5813 - val_precision: 0.7769 - val_recall: 0.5372 - val_auc: 0.9093 - val_prc: 0.7967\n",
      "Epoch 77/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5514 - tp: 7439.0000 - fp: 1115.0000 - tn: 26113.0000 - fn: 1637.0000 - accuracy: 0.8478 - precision: 0.8697 - recall: 0.8196 - auc: 0.9785 - prc: 0.9401 - val_loss: 0.6447 - val_tp: 1210.0000 - val_fp: 367.0000 - val_tn: 6440.0000 - val_fn: 1059.0000 - val_accuracy: 0.5677 - val_precision: 0.7673 - val_recall: 0.5333 - val_auc: 0.8907 - val_prc: 0.7685\n",
      "Epoch 78/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.5423 - tp: 7394.0000 - fp: 1136.0000 - tn: 26092.0000 - fn: 1682.0000 - accuracy: 0.8424 - precision: 0.8668 - recall: 0.8147 - auc: 0.9782 - prc: 0.9389 - val_loss: 0.4676 - val_tp: 1252.0000 - val_fp: 84.0000 - val_tn: 6723.0000 - val_fn: 1017.0000 - val_accuracy: 0.8841 - val_precision: 0.9371 - val_recall: 0.5518 - val_auc: 0.9718 - val_prc: 0.9080\n",
      "Epoch 79/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5393 - tp: 7401.0000 - fp: 1117.0000 - tn: 26111.0000 - fn: 1675.0000 - accuracy: 0.8456 - precision: 0.8689 - recall: 0.8154 - auc: 0.9784 - prc: 0.9403 - val_loss: 0.5519 - val_tp: 1229.0000 - val_fp: 357.0000 - val_tn: 6450.0000 - val_fn: 1040.0000 - val_accuracy: 0.6148 - val_precision: 0.7749 - val_recall: 0.5416 - val_auc: 0.9468 - val_prc: 0.8534\n",
      "Epoch 80/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5365 - tp: 7405.0000 - fp: 1125.0000 - tn: 26103.0000 - fn: 1671.0000 - accuracy: 0.8449 - precision: 0.8681 - recall: 0.8159 - auc: 0.9783 - prc: 0.9405 - val_loss: 0.5292 - val_tp: 1230.0000 - val_fp: 350.0000 - val_tn: 6457.0000 - val_fn: 1039.0000 - val_accuracy: 0.6078 - val_precision: 0.7785 - val_recall: 0.5421 - val_auc: 0.9499 - val_prc: 0.8586\n",
      "Epoch 81/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5213 - tp: 7498.0000 - fp: 1060.0000 - tn: 26168.0000 - fn: 1578.0000 - accuracy: 0.8525 - precision: 0.8761 - recall: 0.8261 - auc: 0.9803 - prc: 0.9459 - val_loss: 0.4528 - val_tp: 1224.0000 - val_fp: 271.0000 - val_tn: 6536.0000 - val_fn: 1045.0000 - val_accuracy: 0.8083 - val_precision: 0.8187 - val_recall: 0.5394 - val_auc: 0.9678 - val_prc: 0.8942\n",
      "Epoch 82/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.5293 - tp: 7482.0000 - fp: 1079.0000 - tn: 26149.0000 - fn: 1594.0000 - accuracy: 0.8511 - precision: 0.8740 - recall: 0.8244 - auc: 0.9791 - prc: 0.9417 - val_loss: 0.4179 - val_tp: 1358.0000 - val_fp: 84.0000 - val_tn: 6723.0000 - val_fn: 911.0000 - val_accuracy: 0.6871 - val_precision: 0.9417 - val_recall: 0.5985 - val_auc: 0.9531 - val_prc: 0.8762\n",
      "Epoch 83/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5293 - tp: 7470.0000 - fp: 1116.0000 - tn: 26112.0000 - fn: 1606.0000 - accuracy: 0.8485 - precision: 0.8700 - recall: 0.8230 - auc: 0.9792 - prc: 0.9412 - val_loss: 0.5024 - val_tp: 1235.0000 - val_fp: 324.0000 - val_tn: 6483.0000 - val_fn: 1034.0000 - val_accuracy: 0.5870 - val_precision: 0.7922 - val_recall: 0.5443 - val_auc: 0.9468 - val_prc: 0.8526\n",
      "Epoch 84/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.5158 - tp: 7482.0000 - fp: 1109.0000 - tn: 26119.0000 - fn: 1594.0000 - accuracy: 0.8499 - precision: 0.8709 - recall: 0.8244 - auc: 0.9798 - prc: 0.9437 - val_loss: 0.5009 - val_tp: 1380.0000 - val_fp: 55.0000 - val_tn: 6752.0000 - val_fn: 889.0000 - val_accuracy: 0.7056 - val_precision: 0.9617 - val_recall: 0.6082 - val_auc: 0.9514 - val_prc: 0.8745\n",
      "Epoch 85/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.5183 - tp: 7467.0000 - fp: 1098.0000 - tn: 26130.0000 - fn: 1609.0000 - accuracy: 0.8476 - precision: 0.8718 - recall: 0.8227 - auc: 0.9801 - prc: 0.9451 - val_loss: 0.5419 - val_tp: 1581.0000 - val_fp: 29.0000 - val_tn: 6778.0000 - val_fn: 688.0000 - val_accuracy: 0.7307 - val_precision: 0.9820 - val_recall: 0.6968 - val_auc: 0.9327 - val_prc: 0.8592\n",
      "Epoch 86/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.5068 - tp: 7558.0000 - fp: 1033.0000 - tn: 26195.0000 - fn: 1518.0000 - accuracy: 0.8580 - precision: 0.8798 - recall: 0.8327 - auc: 0.9815 - prc: 0.9497 - val_loss: 0.5768 - val_tp: 1591.0000 - val_fp: 39.0000 - val_tn: 6768.0000 - val_fn: 678.0000 - val_accuracy: 0.7219 - val_precision: 0.9761 - val_recall: 0.7012 - val_auc: 0.9303 - val_prc: 0.8537\n",
      "Epoch 87/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5108 - tp: 7570.0000 - fp: 1024.0000 - tn: 26204.0000 - fn: 1506.0000 - accuracy: 0.8592 - precision: 0.8808 - recall: 0.8341 - auc: 0.9811 - prc: 0.9467 - val_loss: 0.4286 - val_tp: 1627.0000 - val_fp: 25.0000 - val_tn: 6782.0000 - val_fn: 642.0000 - val_accuracy: 0.7338 - val_precision: 0.9849 - val_recall: 0.7171 - val_auc: 0.9657 - val_prc: 0.9054\n",
      "Epoch 88/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5022 - tp: 7550.0000 - fp: 1035.0000 - tn: 26193.0000 - fn: 1526.0000 - accuracy: 0.8591 - precision: 0.8794 - recall: 0.8319 - auc: 0.9816 - prc: 0.9486 - val_loss: 0.3844 - val_tp: 1561.0000 - val_fp: 25.0000 - val_tn: 6782.0000 - val_fn: 708.0000 - val_accuracy: 0.9718 - val_precision: 0.9842 - val_recall: 0.6880 - val_auc: 0.9924 - val_prc: 0.9702\n",
      "Epoch 89/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.5048 - tp: 7545.0000 - fp: 1020.0000 - tn: 26208.0000 - fn: 1531.0000 - accuracy: 0.8584 - precision: 0.8809 - recall: 0.8313 - auc: 0.9810 - prc: 0.9477 - val_loss: 0.6092 - val_tp: 1225.0000 - val_fp: 366.0000 - val_tn: 6441.0000 - val_fn: 1044.0000 - val_accuracy: 0.5677 - val_precision: 0.7700 - val_recall: 0.5399 - val_auc: 0.9208 - val_prc: 0.8253\n",
      "Epoch 90/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.4840 - tp: 7654.0000 - fp: 880.0000 - tn: 26348.0000 - fn: 1422.0000 - accuracy: 0.8735 - precision: 0.8969 - recall: 0.8433 - auc: 0.9840 - prc: 0.9557 - val_loss: 0.5311 - val_tp: 1232.0000 - val_fp: 202.0000 - val_tn: 6605.0000 - val_fn: 1037.0000 - val_accuracy: 0.6214 - val_precision: 0.8591 - val_recall: 0.5430 - val_auc: 0.9510 - val_prc: 0.8721\n",
      "Epoch 91/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.4734 - tp: 7715.0000 - fp: 819.0000 - tn: 26409.0000 - fn: 1361.0000 - accuracy: 0.8802 - precision: 0.9040 - recall: 0.8500 - auc: 0.9843 - prc: 0.9560 - val_loss: 0.4643 - val_tp: 1553.0000 - val_fp: 41.0000 - val_tn: 6766.0000 - val_fn: 716.0000 - val_accuracy: 0.9233 - val_precision: 0.9743 - val_recall: 0.6844 - val_auc: 0.9738 - val_prc: 0.9317\n",
      "Epoch 92/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.4526 - tp: 7828.0000 - fp: 720.0000 - tn: 26508.0000 - fn: 1248.0000 - accuracy: 0.8942 - precision: 0.9158 - recall: 0.8625 - auc: 0.9861 - prc: 0.9599 - val_loss: 0.3520 - val_tp: 1640.0000 - val_fp: 19.0000 - val_tn: 6788.0000 - val_fn: 629.0000 - val_accuracy: 0.9868 - val_precision: 0.9885 - val_recall: 0.7228 - val_auc: 0.9938 - val_prc: 0.9754\n",
      "Epoch 93/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.4231 - tp: 7993.0000 - fp: 554.0000 - tn: 26674.0000 - fn: 1083.0000 - accuracy: 0.9125 - precision: 0.9352 - recall: 0.8807 - auc: 0.9886 - prc: 0.9678 - val_loss: 0.4048 - val_tp: 1541.0000 - val_fp: 37.0000 - val_tn: 6770.0000 - val_fn: 728.0000 - val_accuracy: 0.9714 - val_precision: 0.9766 - val_recall: 0.6792 - val_auc: 0.9908 - val_prc: 0.9745\n",
      "Epoch 94/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.4325 - tp: 8036.0000 - fp: 523.0000 - tn: 26705.0000 - fn: 1040.0000 - accuracy: 0.9164 - precision: 0.9389 - recall: 0.8854 - auc: 0.9887 - prc: 0.9686 - val_loss: 0.3786 - val_tp: 1551.0000 - val_fp: 33.0000 - val_tn: 6774.0000 - val_fn: 718.0000 - val_accuracy: 0.9731 - val_precision: 0.9792 - val_recall: 0.6836 - val_auc: 0.9929 - val_prc: 0.9704\n",
      "Epoch 95/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.4027 - tp: 8095.0000 - fp: 484.0000 - tn: 26744.0000 - fn: 981.0000 - accuracy: 0.9233 - precision: 0.9436 - recall: 0.8919 - auc: 0.9898 - prc: 0.9714 - val_loss: 0.3930 - val_tp: 1600.0000 - val_fp: 45.0000 - val_tn: 6762.0000 - val_fn: 669.0000 - val_accuracy: 0.9714 - val_precision: 0.9726 - val_recall: 0.7052 - val_auc: 0.9892 - val_prc: 0.9591\n",
      "Epoch 96/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.3803 - tp: 8182.0000 - fp: 399.0000 - tn: 26829.0000 - fn: 894.0000 - accuracy: 0.9331 - precision: 0.9535 - recall: 0.9015 - auc: 0.9911 - prc: 0.9756 - val_loss: 0.4219 - val_tp: 1634.0000 - val_fp: 29.0000 - val_tn: 6778.0000 - val_fn: 635.0000 - val_accuracy: 0.7514 - val_precision: 0.9826 - val_recall: 0.7201 - val_auc: 0.9716 - val_prc: 0.9174\n",
      "Epoch 97/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.3553 - tp: 8326.0000 - fp: 351.0000 - tn: 26877.0000 - fn: 750.0000 - accuracy: 0.9409 - precision: 0.9595 - recall: 0.9174 - auc: 0.9917 - prc: 0.9766 - val_loss: 0.3559 - val_tp: 1672.0000 - val_fp: 37.0000 - val_tn: 6770.0000 - val_fn: 597.0000 - val_accuracy: 0.8651 - val_precision: 0.9783 - val_recall: 0.7369 - val_auc: 0.9815 - val_prc: 0.9419\n",
      "Epoch 98/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.3488 - tp: 8339.0000 - fp: 346.0000 - tn: 26882.0000 - fn: 737.0000 - accuracy: 0.9456 - precision: 0.9602 - recall: 0.9188 - auc: 0.9920 - prc: 0.9766 - val_loss: 0.3416 - val_tp: 1636.0000 - val_fp: 34.0000 - val_tn: 6773.0000 - val_fn: 633.0000 - val_accuracy: 0.9793 - val_precision: 0.9796 - val_recall: 0.7210 - val_auc: 0.9936 - val_prc: 0.9728\n",
      "Epoch 99/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.3348 - tp: 8448.0000 - fp: 318.0000 - tn: 26910.0000 - fn: 628.0000 - accuracy: 0.9503 - precision: 0.9637 - recall: 0.9308 - auc: 0.9925 - prc: 0.9791 - val_loss: 0.2235 - val_tp: 2209.0000 - val_fp: 36.0000 - val_tn: 6771.0000 - val_fn: 60.0000 - val_accuracy: 0.9810 - val_precision: 0.9840 - val_recall: 0.9736 - val_auc: 0.9931 - val_prc: 0.9751\n",
      "Epoch 100/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.3089 - tp: 8518.0000 - fp: 285.0000 - tn: 26943.0000 - fn: 558.0000 - accuracy: 0.9576 - precision: 0.9676 - recall: 0.9385 - auc: 0.9932 - prc: 0.9803 - val_loss: 0.2122 - val_tp: 2214.0000 - val_fp: 37.0000 - val_tn: 6770.0000 - val_fn: 55.0000 - val_accuracy: 0.9810 - val_precision: 0.9836 - val_recall: 0.9758 - val_auc: 0.9932 - val_prc: 0.9754\n",
      "Epoch 101/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.2963 - tp: 8536.0000 - fp: 291.0000 - tn: 26937.0000 - fn: 540.0000 - accuracy: 0.9578 - precision: 0.9670 - recall: 0.9405 - auc: 0.9936 - prc: 0.9820 - val_loss: 0.2886 - val_tp: 1798.0000 - val_fp: 65.0000 - val_tn: 6742.0000 - val_fn: 471.0000 - val_accuracy: 0.9586 - val_precision: 0.9651 - val_recall: 0.7924 - val_auc: 0.9901 - val_prc: 0.9659\n",
      "Epoch 102/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.2845 - tp: 8574.0000 - fp: 270.0000 - tn: 26958.0000 - fn: 502.0000 - accuracy: 0.9607 - precision: 0.9695 - recall: 0.9447 - auc: 0.9939 - prc: 0.9840 - val_loss: 0.1923 - val_tp: 2233.0000 - val_fp: 32.0000 - val_tn: 6775.0000 - val_fn: 36.0000 - val_accuracy: 0.9846 - val_precision: 0.9859 - val_recall: 0.9841 - val_auc: 0.9942 - val_prc: 0.9774\n",
      "Epoch 103/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 0.2720 - tp: 8605.0000 - fp: 284.0000 - tn: 26944.0000 - fn: 471.0000 - accuracy: 0.9614 - precision: 0.9681 - recall: 0.9481 - auc: 0.9938 - prc: 0.9835 - val_loss: 0.1392 - val_tp: 2241.0000 - val_fp: 27.0000 - val_tn: 6780.0000 - val_fn: 28.0000 - val_accuracy: 0.9877 - val_precision: 0.9881 - val_recall: 0.9877 - val_auc: 0.9963 - val_prc: 0.9805\n",
      "Epoch 104/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.2648 - tp: 8665.0000 - fp: 240.0000 - tn: 26988.0000 - fn: 411.0000 - accuracy: 0.9658 - precision: 0.9730 - recall: 0.9547 - auc: 0.9940 - prc: 0.9836 - val_loss: 0.1846 - val_tp: 2226.0000 - val_fp: 37.0000 - val_tn: 6770.0000 - val_fn: 43.0000 - val_accuracy: 0.9824 - val_precision: 0.9837 - val_recall: 0.9810 - val_auc: 0.9943 - val_prc: 0.9771\n",
      "Epoch 105/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.2492 - tp: 8689.0000 - fp: 234.0000 - tn: 26994.0000 - fn: 387.0000 - accuracy: 0.9679 - precision: 0.9738 - recall: 0.9574 - auc: 0.9941 - prc: 0.9835 - val_loss: 0.3191 - val_tp: 1667.0000 - val_fp: 71.0000 - val_tn: 6736.0000 - val_fn: 602.0000 - val_accuracy: 0.9524 - val_precision: 0.9591 - val_recall: 0.7347 - val_auc: 0.9897 - val_prc: 0.9738\n",
      "Epoch 106/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.2520 - tp: 8703.0000 - fp: 232.0000 - tn: 26996.0000 - fn: 373.0000 - accuracy: 0.9688 - precision: 0.9740 - recall: 0.9589 - auc: 0.9940 - prc: 0.9833 - val_loss: 0.1749 - val_tp: 2217.0000 - val_fp: 43.0000 - val_tn: 6764.0000 - val_fn: 52.0000 - val_accuracy: 0.9797 - val_precision: 0.9810 - val_recall: 0.9771 - val_auc: 0.9954 - val_prc: 0.9780\n",
      "Epoch 107/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.2356 - tp: 8711.0000 - fp: 230.0000 - tn: 26998.0000 - fn: 365.0000 - accuracy: 0.9678 - precision: 0.9743 - recall: 0.9598 - auc: 0.9945 - prc: 0.9845 - val_loss: 0.1919 - val_tp: 2225.0000 - val_fp: 39.0000 - val_tn: 6768.0000 - val_fn: 44.0000 - val_accuracy: 0.9815 - val_precision: 0.9828 - val_recall: 0.9806 - val_auc: 0.9949 - val_prc: 0.9779\n",
      "Epoch 108/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.2390 - tp: 8737.0000 - fp: 231.0000 - tn: 26997.0000 - fn: 339.0000 - accuracy: 0.9689 - precision: 0.9742 - recall: 0.9626 - auc: 0.9942 - prc: 0.9837 - val_loss: 0.1137 - val_tp: 2244.0000 - val_fp: 22.0000 - val_tn: 6785.0000 - val_fn: 25.0000 - val_accuracy: 0.9903 - val_precision: 0.9903 - val_recall: 0.9890 - val_auc: 0.9963 - val_prc: 0.9807\n",
      "Epoch 109/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.2235 - tp: 8782.0000 - fp: 202.0000 - tn: 27026.0000 - fn: 294.0000 - accuracy: 0.9748 - precision: 0.9775 - recall: 0.9676 - auc: 0.9953 - prc: 0.9876 - val_loss: 0.2322 - val_tp: 2179.0000 - val_fp: 72.0000 - val_tn: 6735.0000 - val_fn: 90.0000 - val_accuracy: 0.9652 - val_precision: 0.9680 - val_recall: 0.9603 - val_auc: 0.9918 - val_prc: 0.9709\n",
      "Epoch 110/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.2171 - tp: 8783.0000 - fp: 206.0000 - tn: 27022.0000 - fn: 293.0000 - accuracy: 0.9743 - precision: 0.9771 - recall: 0.9677 - auc: 0.9949 - prc: 0.9856 - val_loss: 0.2197 - val_tp: 2193.0000 - val_fp: 59.0000 - val_tn: 6748.0000 - val_fn: 76.0000 - val_accuracy: 0.9696 - val_precision: 0.9738 - val_recall: 0.9665 - val_auc: 0.9929 - val_prc: 0.9737\n",
      "Epoch 111/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.2127 - tp: 8795.0000 - fp: 200.0000 - tn: 27028.0000 - fn: 281.0000 - accuracy: 0.9745 - precision: 0.9778 - recall: 0.9690 - auc: 0.9951 - prc: 0.9859 - val_loss: 0.3052 - val_tp: 1711.0000 - val_fp: 409.0000 - val_tn: 6398.0000 - val_fn: 558.0000 - val_accuracy: 0.7823 - val_precision: 0.8071 - val_recall: 0.7541 - val_auc: 0.9749 - val_prc: 0.9258\n",
      "Epoch 112/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.1975 - tp: 8821.0000 - fp: 184.0000 - tn: 27044.0000 - fn: 255.0000 - accuracy: 0.9769 - precision: 0.9796 - recall: 0.9719 - auc: 0.9952 - prc: 0.9864 - val_loss: 0.0962 - val_tp: 2249.0000 - val_fp: 19.0000 - val_tn: 6788.0000 - val_fn: 20.0000 - val_accuracy: 0.9916 - val_precision: 0.9916 - val_recall: 0.9912 - val_auc: 0.9964 - val_prc: 0.9809\n",
      "Epoch 113/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.1947 - tp: 8823.0000 - fp: 183.0000 - tn: 27045.0000 - fn: 253.0000 - accuracy: 0.9770 - precision: 0.9797 - recall: 0.9721 - auc: 0.9953 - prc: 0.9866 - val_loss: 0.0962 - val_tp: 2249.0000 - val_fp: 19.0000 - val_tn: 6788.0000 - val_fn: 20.0000 - val_accuracy: 0.9916 - val_precision: 0.9916 - val_recall: 0.9912 - val_auc: 0.9957 - val_prc: 0.9806\n",
      "Epoch 114/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1851 - tp: 8865.0000 - fp: 150.0000 - tn: 27078.0000 - fn: 211.0000 - accuracy: 0.9810 - precision: 0.9834 - recall: 0.9768 - auc: 0.9959 - prc: 0.9884 - val_loss: 0.0764 - val_tp: 2254.0000 - val_fp: 14.0000 - val_tn: 6793.0000 - val_fn: 15.0000 - val_accuracy: 0.9934 - val_precision: 0.9938 - val_recall: 0.9934 - val_auc: 0.9967 - val_prc: 0.9813\n",
      "Epoch 115/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1843 - tp: 8860.0000 - fp: 162.0000 - tn: 27066.0000 - fn: 216.0000 - accuracy: 0.9795 - precision: 0.9820 - recall: 0.9762 - auc: 0.9956 - prc: 0.9885 - val_loss: 0.1244 - val_tp: 2248.0000 - val_fp: 20.0000 - val_tn: 6787.0000 - val_fn: 21.0000 - val_accuracy: 0.9907 - val_precision: 0.9912 - val_recall: 0.9907 - val_auc: 0.9968 - val_prc: 0.9811\n",
      "Epoch 116/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1835 - tp: 8883.0000 - fp: 147.0000 - tn: 27081.0000 - fn: 193.0000 - accuracy: 0.9816 - precision: 0.9837 - recall: 0.9787 - auc: 0.9956 - prc: 0.9884 - val_loss: 0.0877 - val_tp: 2251.0000 - val_fp: 18.0000 - val_tn: 6789.0000 - val_fn: 18.0000 - val_accuracy: 0.9921 - val_precision: 0.9921 - val_recall: 0.9921 - val_auc: 0.9965 - val_prc: 0.9810\n",
      "Epoch 117/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1763 - tp: 8886.0000 - fp: 143.0000 - tn: 27085.0000 - fn: 190.0000 - accuracy: 0.9826 - precision: 0.9842 - recall: 0.9791 - auc: 0.9953 - prc: 0.9867 - val_loss: 0.1179 - val_tp: 2244.0000 - val_fp: 25.0000 - val_tn: 6782.0000 - val_fn: 25.0000 - val_accuracy: 0.9890 - val_precision: 0.9890 - val_recall: 0.9890 - val_auc: 0.9956 - val_prc: 0.9804\n",
      "Epoch 118/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1685 - tp: 8898.0000 - fp: 145.0000 - tn: 27083.0000 - fn: 178.0000 - accuracy: 0.9819 - precision: 0.9840 - recall: 0.9804 - auc: 0.9959 - prc: 0.9885 - val_loss: 0.0940 - val_tp: 2251.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 18.0000 - val_accuracy: 0.9921 - val_precision: 0.9925 - val_recall: 0.9921 - val_auc: 0.9954 - val_prc: 0.9805\n",
      "Epoch 119/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1613 - tp: 8931.0000 - fp: 120.0000 - tn: 27108.0000 - fn: 145.0000 - accuracy: 0.9853 - precision: 0.9867 - recall: 0.9840 - auc: 0.9958 - prc: 0.9892 - val_loss: 0.0902 - val_tp: 2253.0000 - val_fp: 16.0000 - val_tn: 6791.0000 - val_fn: 16.0000 - val_accuracy: 0.9929 - val_precision: 0.9929 - val_recall: 0.9929 - val_auc: 0.9960 - val_prc: 0.9808\n",
      "Epoch 120/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1643 - tp: 8910.0000 - fp: 128.0000 - tn: 27100.0000 - fn: 166.0000 - accuracy: 0.9841 - precision: 0.9858 - recall: 0.9817 - auc: 0.9957 - prc: 0.9893 - val_loss: 0.0643 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9966 - val_prc: 0.9812\n",
      "Epoch 121/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1628 - tp: 8911.0000 - fp: 124.0000 - tn: 27104.0000 - fn: 165.0000 - accuracy: 0.9849 - precision: 0.9863 - recall: 0.9818 - auc: 0.9949 - prc: 0.9859 - val_loss: 0.0771 - val_tp: 2255.0000 - val_fp: 14.0000 - val_tn: 6793.0000 - val_fn: 14.0000 - val_accuracy: 0.9938 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9960 - val_prc: 0.9809\n",
      "Epoch 122/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1542 - tp: 8907.0000 - fp: 134.0000 - tn: 27094.0000 - fn: 169.0000 - accuracy: 0.9840 - precision: 0.9852 - recall: 0.9814 - auc: 0.9962 - prc: 0.9899 - val_loss: 0.0719 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9959 - val_prc: 0.9808\n",
      "Epoch 123/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1479 - tp: 8916.0000 - fp: 122.0000 - tn: 27106.0000 - fn: 160.0000 - accuracy: 0.9848 - precision: 0.9865 - recall: 0.9824 - auc: 0.9963 - prc: 0.9905 - val_loss: 0.0906 - val_tp: 2251.0000 - val_fp: 18.0000 - val_tn: 6789.0000 - val_fn: 18.0000 - val_accuracy: 0.9921 - val_precision: 0.9921 - val_recall: 0.9921 - val_auc: 0.9953 - val_prc: 0.9805\n",
      "Epoch 124/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1554 - tp: 8911.0000 - fp: 136.0000 - tn: 27092.0000 - fn: 165.0000 - accuracy: 0.9839 - precision: 0.9850 - recall: 0.9818 - auc: 0.9955 - prc: 0.9884 - val_loss: 0.0720 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9962 - val_prc: 0.9810\n",
      "Epoch 125/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1433 - tp: 8944.0000 - fp: 110.0000 - tn: 27118.0000 - fn: 132.0000 - accuracy: 0.9870 - precision: 0.9879 - recall: 0.9855 - auc: 0.9956 - prc: 0.9872 - val_loss: 0.0558 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9970 - val_prc: 0.9815\n",
      "Epoch 126/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1454 - tp: 8929.0000 - fp: 125.0000 - tn: 27103.0000 - fn: 147.0000 - accuracy: 0.9853 - precision: 0.9862 - recall: 0.9838 - auc: 0.9959 - prc: 0.9896 - val_loss: 0.0536 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9969 - val_prc: 0.9814\n",
      "Epoch 127/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.1475 - tp: 8943.0000 - fp: 105.0000 - tn: 27123.0000 - fn: 133.0000 - accuracy: 0.9874 - precision: 0.9884 - recall: 0.9853 - auc: 0.9963 - prc: 0.9904 - val_loss: 0.0856 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9953 - val_prc: 0.9805\n",
      "Epoch 128/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1390 - tp: 8944.0000 - fp: 108.0000 - tn: 27120.0000 - fn: 132.0000 - accuracy: 0.9869 - precision: 0.9881 - recall: 0.9855 - auc: 0.9962 - prc: 0.9908 - val_loss: 0.1477 - val_tp: 2249.0000 - val_fp: 20.0000 - val_tn: 6787.0000 - val_fn: 20.0000 - val_accuracy: 0.9912 - val_precision: 0.9912 - val_recall: 0.9912 - val_auc: 0.9948 - val_prc: 0.9793\n",
      "Epoch 129/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1322 - tp: 8937.0000 - fp: 111.0000 - tn: 27117.0000 - fn: 139.0000 - accuracy: 0.9862 - precision: 0.9877 - recall: 0.9847 - auc: 0.9966 - prc: 0.9923 - val_loss: 0.1949 - val_tp: 2229.0000 - val_fp: 35.0000 - val_tn: 6772.0000 - val_fn: 40.0000 - val_accuracy: 0.9837 - val_precision: 0.9845 - val_recall: 0.9824 - val_auc: 0.9942 - val_prc: 0.9778\n",
      "Epoch 130/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1440 - tp: 8945.0000 - fp: 113.0000 - tn: 27115.0000 - fn: 131.0000 - accuracy: 0.9868 - precision: 0.9875 - recall: 0.9856 - auc: 0.9955 - prc: 0.9893 - val_loss: 0.0797 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9960 - val_prc: 0.9809\n",
      "Epoch 131/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1356 - tp: 8951.0000 - fp: 98.0000 - tn: 27130.0000 - fn: 125.0000 - accuracy: 0.9883 - precision: 0.9892 - recall: 0.9862 - auc: 0.9963 - prc: 0.9913 - val_loss: 0.0719 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9953 - val_prc: 0.9806\n",
      "Epoch 132/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1312 - tp: 8957.0000 - fp: 99.0000 - tn: 27129.0000 - fn: 119.0000 - accuracy: 0.9880 - precision: 0.9891 - recall: 0.9869 - auc: 0.9960 - prc: 0.9897 - val_loss: 0.1057 - val_tp: 2254.0000 - val_fp: 15.0000 - val_tn: 6792.0000 - val_fn: 15.0000 - val_accuracy: 0.9934 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9957 - val_prc: 0.9805\n",
      "Epoch 133/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.1343 - tp: 8950.0000 - fp: 113.0000 - tn: 27115.0000 - fn: 126.0000 - accuracy: 0.9871 - precision: 0.9875 - recall: 0.9861 - auc: 0.9960 - prc: 0.9910 - val_loss: 0.2013 - val_tp: 2230.0000 - val_fp: 37.0000 - val_tn: 6770.0000 - val_fn: 39.0000 - val_accuracy: 0.9833 - val_precision: 0.9837 - val_recall: 0.9828 - val_auc: 0.9950 - val_prc: 0.9894\n",
      "Epoch 134/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1353 - tp: 8951.0000 - fp: 111.0000 - tn: 27117.0000 - fn: 125.0000 - accuracy: 0.9872 - precision: 0.9878 - recall: 0.9862 - auc: 0.9960 - prc: 0.9908 - val_loss: 0.0854 - val_tp: 2255.0000 - val_fp: 14.0000 - val_tn: 6793.0000 - val_fn: 14.0000 - val_accuracy: 0.9938 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9953 - val_prc: 0.9805\n",
      "Epoch 135/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.1264 - tp: 8963.0000 - fp: 98.0000 - tn: 27130.0000 - fn: 113.0000 - accuracy: 0.9888 - precision: 0.9892 - recall: 0.9875 - auc: 0.9962 - prc: 0.9909 - val_loss: 0.1096 - val_tp: 2250.0000 - val_fp: 19.0000 - val_tn: 6788.0000 - val_fn: 19.0000 - val_accuracy: 0.9916 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9952 - val_prc: 0.9804\n",
      "Epoch 136/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1255 - tp: 8953.0000 - fp: 107.0000 - tn: 27121.0000 - fn: 123.0000 - accuracy: 0.9878 - precision: 0.9882 - recall: 0.9864 - auc: 0.9964 - prc: 0.9917 - val_loss: 0.0833 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9953 - val_prc: 0.9805\n",
      "Epoch 137/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1282 - tp: 8955.0000 - fp: 108.0000 - tn: 27120.0000 - fn: 121.0000 - accuracy: 0.9877 - precision: 0.9881 - recall: 0.9867 - auc: 0.9963 - prc: 0.9914 - val_loss: 0.1210 - val_tp: 2250.0000 - val_fp: 19.0000 - val_tn: 6788.0000 - val_fn: 19.0000 - val_accuracy: 0.9916 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9950 - val_prc: 0.9800\n",
      "Epoch 138/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1259 - tp: 8960.0000 - fp: 100.0000 - tn: 27128.0000 - fn: 116.0000 - accuracy: 0.9885 - precision: 0.9890 - recall: 0.9872 - auc: 0.9964 - prc: 0.9919 - val_loss: 0.0852 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9959 - val_prc: 0.9839\n",
      "Epoch 139/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.1254 - tp: 8963.0000 - fp: 104.0000 - tn: 27124.0000 - fn: 113.0000 - accuracy: 0.9880 - precision: 0.9885 - recall: 0.9875 - auc: 0.9966 - prc: 0.9923 - val_loss: 0.1475 - val_tp: 2245.0000 - val_fp: 24.0000 - val_tn: 6783.0000 - val_fn: 24.0000 - val_accuracy: 0.9894 - val_precision: 0.9894 - val_recall: 0.9894 - val_auc: 0.9950 - val_prc: 0.9793\n",
      "Epoch 140/200\n",
      "91/91 [==============================] - 5s 51ms/step - loss: 0.1225 - tp: 8967.0000 - fp: 96.0000 - tn: 27132.0000 - fn: 109.0000 - accuracy: 0.9889 - precision: 0.9894 - recall: 0.9880 - auc: 0.9964 - prc: 0.9921 - val_loss: 0.0592 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9964 - val_prc: 0.9812\n",
      "Epoch 141/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1196 - tp: 8968.0000 - fp: 90.0000 - tn: 27138.0000 - fn: 108.0000 - accuracy: 0.9895 - precision: 0.9901 - recall: 0.9881 - auc: 0.9965 - prc: 0.9921 - val_loss: 0.0612 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9954 - val_prc: 0.9807\n",
      "Epoch 142/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1189 - tp: 8974.0000 - fp: 89.0000 - tn: 27139.0000 - fn: 102.0000 - accuracy: 0.9896 - precision: 0.9902 - recall: 0.9888 - auc: 0.9961 - prc: 0.9916 - val_loss: 0.0777 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9958 - val_prc: 0.9845\n",
      "Epoch 143/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1193 - tp: 8967.0000 - fp: 94.0000 - tn: 27134.0000 - fn: 109.0000 - accuracy: 0.9891 - precision: 0.9896 - recall: 0.9880 - auc: 0.9965 - prc: 0.9916 - val_loss: 0.0608 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9955 - val_prc: 0.9838\n",
      "Epoch 144/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1156 - tp: 8975.0000 - fp: 91.0000 - tn: 27137.0000 - fn: 101.0000 - accuracy: 0.9894 - precision: 0.9900 - recall: 0.9889 - auc: 0.9965 - prc: 0.9923 - val_loss: 0.0953 - val_tp: 2252.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 17.0000 - val_accuracy: 0.9925 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9959 - val_prc: 0.9806\n",
      "Epoch 145/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1153 - tp: 8976.0000 - fp: 89.0000 - tn: 27139.0000 - fn: 100.0000 - accuracy: 0.9899 - precision: 0.9902 - recall: 0.9890 - auc: 0.9963 - prc: 0.9920 - val_loss: 0.1348 - val_tp: 2248.0000 - val_fp: 20.0000 - val_tn: 6787.0000 - val_fn: 21.0000 - val_accuracy: 0.9912 - val_precision: 0.9912 - val_recall: 0.9907 - val_auc: 0.9957 - val_prc: 0.9887\n",
      "Epoch 146/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1099 - tp: 8961.0000 - fp: 97.0000 - tn: 27131.0000 - fn: 115.0000 - accuracy: 0.9882 - precision: 0.9893 - recall: 0.9873 - auc: 0.9966 - prc: 0.9921 - val_loss: 0.1413 - val_tp: 2241.0000 - val_fp: 27.0000 - val_tn: 6780.0000 - val_fn: 28.0000 - val_accuracy: 0.9881 - val_precision: 0.9881 - val_recall: 0.9877 - val_auc: 0.9959 - val_prc: 0.9911\n",
      "Epoch 147/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1100 - tp: 8975.0000 - fp: 90.0000 - tn: 27138.0000 - fn: 101.0000 - accuracy: 0.9895 - precision: 0.9901 - recall: 0.9889 - auc: 0.9971 - prc: 0.9936 - val_loss: 0.1127 - val_tp: 2249.0000 - val_fp: 20.0000 - val_tn: 6787.0000 - val_fn: 20.0000 - val_accuracy: 0.9912 - val_precision: 0.9912 - val_recall: 0.9912 - val_auc: 0.9959 - val_prc: 0.9912\n",
      "Epoch 148/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.1162 - tp: 8969.0000 - fp: 97.0000 - tn: 27131.0000 - fn: 107.0000 - accuracy: 0.9889 - precision: 0.9893 - recall: 0.9882 - auc: 0.9960 - prc: 0.9916 - val_loss: 0.0465 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9968 - val_prc: 0.9894\n",
      "Epoch 149/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1109 - tp: 8979.0000 - fp: 83.0000 - tn: 27145.0000 - fn: 97.0000 - accuracy: 0.9902 - precision: 0.9908 - recall: 0.9893 - auc: 0.9966 - prc: 0.9925 - val_loss: 0.1504 - val_tp: 2242.0000 - val_fp: 26.0000 - val_tn: 6781.0000 - val_fn: 27.0000 - val_accuracy: 0.9881 - val_precision: 0.9885 - val_recall: 0.9881 - val_auc: 0.9956 - val_prc: 0.9906\n",
      "Epoch 150/200\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 0.1136 - tp: 8979.0000 - fp: 89.0000 - tn: 27139.0000 - fn: 97.0000 - accuracy: 0.9899 - precision: 0.9902 - recall: 0.9893 - auc: 0.9962 - prc: 0.9913 - val_loss: 0.0453 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9971 - val_prc: 0.9888\n",
      "Epoch 151/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.1111 - tp: 8980.0000 - fp: 83.0000 - tn: 27145.0000 - fn: 96.0000 - accuracy: 0.9904 - precision: 0.9908 - recall: 0.9894 - auc: 0.9965 - prc: 0.9926 - val_loss: 0.0574 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9959 - val_prc: 0.9809\n",
      "Epoch 152/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1075 - tp: 8970.0000 - fp: 93.0000 - tn: 27135.0000 - fn: 106.0000 - accuracy: 0.9891 - precision: 0.9897 - recall: 0.9883 - auc: 0.9969 - prc: 0.9928 - val_loss: 0.0970 - val_tp: 2255.0000 - val_fp: 14.0000 - val_tn: 6793.0000 - val_fn: 14.0000 - val_accuracy: 0.9938 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9957 - val_prc: 0.9909\n",
      "Epoch 153/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.1035 - tp: 8976.0000 - fp: 92.0000 - tn: 27136.0000 - fn: 100.0000 - accuracy: 0.9896 - precision: 0.9899 - recall: 0.9890 - auc: 0.9967 - prc: 0.9925 - val_loss: 0.0501 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9957 - val_prc: 0.9840\n",
      "Epoch 154/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1068 - tp: 8976.0000 - fp: 87.0000 - tn: 27141.0000 - fn: 100.0000 - accuracy: 0.9895 - precision: 0.9904 - recall: 0.9890 - auc: 0.9966 - prc: 0.9925 - val_loss: 0.0859 - val_tp: 2252.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 17.0000 - val_accuracy: 0.9925 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9959 - val_prc: 0.9829\n",
      "Epoch 155/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.1048 - tp: 8985.0000 - fp: 81.0000 - tn: 27147.0000 - fn: 91.0000 - accuracy: 0.9905 - precision: 0.9911 - recall: 0.9900 - auc: 0.9966 - prc: 0.9925 - val_loss: 0.0799 - val_tp: 2255.0000 - val_fp: 14.0000 - val_tn: 6793.0000 - val_fn: 14.0000 - val_accuracy: 0.9938 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9961 - val_prc: 0.9918\n",
      "Epoch 156/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1006 - tp: 8987.0000 - fp: 77.0000 - tn: 27151.0000 - fn: 89.0000 - accuracy: 0.9907 - precision: 0.9915 - recall: 0.9902 - auc: 0.9964 - prc: 0.9920 - val_loss: 0.0660 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9960 - val_prc: 0.9809\n",
      "Epoch 157/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1043 - tp: 8983.0000 - fp: 87.0000 - tn: 27141.0000 - fn: 93.0000 - accuracy: 0.9901 - precision: 0.9904 - recall: 0.9898 - auc: 0.9972 - prc: 0.9939 - val_loss: 0.0637 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9960 - val_prc: 0.9810\n",
      "Epoch 158/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1018 - tp: 8986.0000 - fp: 81.0000 - tn: 27147.0000 - fn: 90.0000 - accuracy: 0.9904 - precision: 0.9911 - recall: 0.9901 - auc: 0.9971 - prc: 0.9933 - val_loss: 0.0627 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9958 - val_prc: 0.9808\n",
      "Epoch 159/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.1046 - tp: 8982.0000 - fp: 85.0000 - tn: 27143.0000 - fn: 94.0000 - accuracy: 0.9902 - precision: 0.9906 - recall: 0.9896 - auc: 0.9968 - prc: 0.9922 - val_loss: 0.0626 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9960 - val_prc: 0.9809\n",
      "Epoch 160/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.0997 - tp: 8987.0000 - fp: 78.0000 - tn: 27150.0000 - fn: 89.0000 - accuracy: 0.9907 - precision: 0.9914 - recall: 0.9902 - auc: 0.9973 - prc: 0.9940 - val_loss: 0.0930 - val_tp: 2250.0000 - val_fp: 19.0000 - val_tn: 6788.0000 - val_fn: 19.0000 - val_accuracy: 0.9916 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9959 - val_prc: 0.9804\n",
      "Epoch 161/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.0988 - tp: 8980.0000 - fp: 90.0000 - tn: 27138.0000 - fn: 96.0000 - accuracy: 0.9899 - precision: 0.9901 - recall: 0.9894 - auc: 0.9972 - prc: 0.9936 - val_loss: 0.0679 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9956 - val_prc: 0.9807\n",
      "Epoch 162/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1019 - tp: 8990.0000 - fp: 77.0000 - tn: 27151.0000 - fn: 86.0000 - accuracy: 0.9909 - precision: 0.9915 - recall: 0.9905 - auc: 0.9964 - prc: 0.9921 - val_loss: 0.1014 - val_tp: 2250.0000 - val_fp: 19.0000 - val_tn: 6788.0000 - val_fn: 19.0000 - val_accuracy: 0.9916 - val_precision: 0.9916 - val_recall: 0.9916 - val_auc: 0.9960 - val_prc: 0.9916\n",
      "Epoch 163/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.1034 - tp: 8984.0000 - fp: 85.0000 - tn: 27143.0000 - fn: 92.0000 - accuracy: 0.9903 - precision: 0.9906 - recall: 0.9899 - auc: 0.9970 - prc: 0.9936 - val_loss: 0.1119 - val_tp: 2251.0000 - val_fp: 18.0000 - val_tn: 6789.0000 - val_fn: 18.0000 - val_accuracy: 0.9921 - val_precision: 0.9921 - val_recall: 0.9921 - val_auc: 0.9959 - val_prc: 0.9909\n",
      "Epoch 164/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1000 - tp: 8986.0000 - fp: 82.0000 - tn: 27146.0000 - fn: 90.0000 - accuracy: 0.9905 - precision: 0.9910 - recall: 0.9901 - auc: 0.9970 - prc: 0.9938 - val_loss: 0.0603 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9956 - val_prc: 0.9810\n",
      "Epoch 165/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.1014 - tp: 8986.0000 - fp: 82.0000 - tn: 27146.0000 - fn: 90.0000 - accuracy: 0.9907 - precision: 0.9910 - recall: 0.9901 - auc: 0.9964 - prc: 0.9926 - val_loss: 0.0482 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9959 - val_prc: 0.9923\n",
      "Epoch 166/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.0967 - tp: 8991.0000 - fp: 76.0000 - tn: 27152.0000 - fn: 85.0000 - accuracy: 0.9913 - precision: 0.9916 - recall: 0.9906 - auc: 0.9969 - prc: 0.9934 - val_loss: 0.0416 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9961 - val_prc: 0.9939\n",
      "Epoch 167/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.0960 - tp: 8988.0000 - fp: 80.0000 - tn: 27148.0000 - fn: 88.0000 - accuracy: 0.9907 - precision: 0.9912 - recall: 0.9903 - auc: 0.9972 - prc: 0.9940 - val_loss: 0.0420 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9962 - val_prc: 0.9945\n",
      "Epoch 168/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.0995 - tp: 8979.0000 - fp: 77.0000 - tn: 27151.0000 - fn: 97.0000 - accuracy: 0.9903 - precision: 0.9915 - recall: 0.9893 - auc: 0.9967 - prc: 0.9931 - val_loss: 0.0589 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9960 - val_prc: 0.9809\n",
      "Epoch 169/200\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 0.0962 - tp: 8990.0000 - fp: 73.0000 - tn: 27155.0000 - fn: 86.0000 - accuracy: 0.9914 - precision: 0.9919 - recall: 0.9905 - auc: 0.9966 - prc: 0.9933 - val_loss: 0.0592 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9960 - val_prc: 0.9809\n",
      "Epoch 170/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.1002 - tp: 8986.0000 - fp: 79.0000 - tn: 27149.0000 - fn: 90.0000 - accuracy: 0.9910 - precision: 0.9913 - recall: 0.9901 - auc: 0.9964 - prc: 0.9924 - val_loss: 0.0399 - val_tp: 2257.0000 - val_fp: 11.0000 - val_tn: 6796.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9951 - val_recall: 0.9947 - val_auc: 0.9974 - val_prc: 0.9952\n",
      "Epoch 171/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.0962 - tp: 8991.0000 - fp: 71.0000 - tn: 27157.0000 - fn: 85.0000 - accuracy: 0.9916 - precision: 0.9922 - recall: 0.9906 - auc: 0.9972 - prc: 0.9945 - val_loss: 0.0816 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9958 - val_prc: 0.9803\n",
      "Epoch 172/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0972 - tp: 8995.0000 - fp: 72.0000 - tn: 27156.0000 - fn: 81.0000 - accuracy: 0.9915 - precision: 0.9921 - recall: 0.9911 - auc: 0.9967 - prc: 0.9936 - val_loss: 0.0867 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9958 - val_prc: 0.9909\n",
      "Epoch 173/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0941 - tp: 8990.0000 - fp: 77.0000 - tn: 27151.0000 - fn: 86.0000 - accuracy: 0.9911 - precision: 0.9915 - recall: 0.9905 - auc: 0.9970 - prc: 0.9942 - val_loss: 0.1181 - val_tp: 2252.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 17.0000 - val_accuracy: 0.9925 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9958 - val_prc: 0.9909\n",
      "Epoch 174/200\n",
      "91/91 [==============================] - 5s 56ms/step - loss: 0.0963 - tp: 8985.0000 - fp: 79.0000 - tn: 27149.0000 - fn: 91.0000 - accuracy: 0.9909 - precision: 0.9913 - recall: 0.9900 - auc: 0.9969 - prc: 0.9938 - val_loss: 0.0957 - val_tp: 2255.0000 - val_fp: 14.0000 - val_tn: 6793.0000 - val_fn: 14.0000 - val_accuracy: 0.9938 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9957 - val_prc: 0.9908\n",
      "Epoch 175/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0884 - tp: 8998.0000 - fp: 67.0000 - tn: 27161.0000 - fn: 78.0000 - accuracy: 0.9920 - precision: 0.9926 - recall: 0.9914 - auc: 0.9972 - prc: 0.9938 - val_loss: 0.0768 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9960 - val_prc: 0.9916\n",
      "Epoch 176/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.0961 - tp: 8997.0000 - fp: 75.0000 - tn: 27153.0000 - fn: 79.0000 - accuracy: 0.9913 - precision: 0.9917 - recall: 0.9913 - auc: 0.9966 - prc: 0.9930 - val_loss: 0.0611 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9962 - val_prc: 0.9922\n",
      "Epoch 177/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.0930 - tp: 8987.0000 - fp: 78.0000 - tn: 27150.0000 - fn: 89.0000 - accuracy: 0.9910 - precision: 0.9914 - recall: 0.9902 - auc: 0.9969 - prc: 0.9935 - val_loss: 0.0750 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9959 - val_prc: 0.9911\n",
      "Epoch 178/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0940 - tp: 8992.0000 - fp: 76.0000 - tn: 27152.0000 - fn: 84.0000 - accuracy: 0.9912 - precision: 0.9916 - recall: 0.9907 - auc: 0.9969 - prc: 0.9937 - val_loss: 0.0735 - val_tp: 2253.0000 - val_fp: 16.0000 - val_tn: 6791.0000 - val_fn: 16.0000 - val_accuracy: 0.9929 - val_precision: 0.9929 - val_recall: 0.9929 - val_auc: 0.9959 - val_prc: 0.9805\n",
      "Epoch 179/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.0992 - tp: 8976.0000 - fp: 87.0000 - tn: 27141.0000 - fn: 100.0000 - accuracy: 0.9900 - precision: 0.9904 - recall: 0.9890 - auc: 0.9969 - prc: 0.9937 - val_loss: 0.0427 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9963 - val_prc: 0.9940\n",
      "Epoch 180/200\n",
      "91/91 [==============================] - 5s 53ms/step - loss: 0.0918 - tp: 8992.0000 - fp: 76.0000 - tn: 27152.0000 - fn: 84.0000 - accuracy: 0.9910 - precision: 0.9916 - recall: 0.9907 - auc: 0.9968 - prc: 0.9929 - val_loss: 0.0547 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9958 - val_prc: 0.9807\n",
      "Epoch 181/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.0894 - tp: 8996.0000 - fp: 72.0000 - tn: 27156.0000 - fn: 80.0000 - accuracy: 0.9917 - precision: 0.9921 - recall: 0.9912 - auc: 0.9968 - prc: 0.9937 - val_loss: 0.0527 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9958 - val_prc: 0.9809\n",
      "Epoch 182/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0879 - tp: 8990.0000 - fp: 77.0000 - tn: 27151.0000 - fn: 86.0000 - accuracy: 0.9912 - precision: 0.9915 - recall: 0.9905 - auc: 0.9972 - prc: 0.9941 - val_loss: 0.0676 - val_tp: 2254.0000 - val_fp: 15.0000 - val_tn: 6792.0000 - val_fn: 15.0000 - val_accuracy: 0.9934 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9956 - val_prc: 0.9806\n",
      "Epoch 183/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0944 - tp: 8989.0000 - fp: 74.0000 - tn: 27154.0000 - fn: 87.0000 - accuracy: 0.9912 - precision: 0.9918 - recall: 0.9904 - auc: 0.9966 - prc: 0.9927 - val_loss: 0.0486 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9960 - val_prc: 0.9811\n",
      "Epoch 184/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0908 - tp: 9001.0000 - fp: 67.0000 - tn: 27161.0000 - fn: 75.0000 - accuracy: 0.9922 - precision: 0.9926 - recall: 0.9917 - auc: 0.9967 - prc: 0.9934 - val_loss: 0.0630 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9961 - val_prc: 0.9919\n",
      "Epoch 185/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.0926 - tp: 8995.0000 - fp: 71.0000 - tn: 27157.0000 - fn: 81.0000 - accuracy: 0.9916 - precision: 0.9922 - recall: 0.9911 - auc: 0.9968 - prc: 0.9928 - val_loss: 0.0549 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9962 - val_prc: 0.9868\n",
      "Epoch 186/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0928 - tp: 8994.0000 - fp: 76.0000 - tn: 27152.0000 - fn: 82.0000 - accuracy: 0.9914 - precision: 0.9916 - recall: 0.9910 - auc: 0.9971 - prc: 0.9939 - val_loss: 0.0426 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9964 - val_prc: 0.9946\n",
      "Epoch 187/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0916 - tp: 8996.0000 - fp: 77.0000 - tn: 27151.0000 - fn: 80.0000 - accuracy: 0.9913 - precision: 0.9915 - recall: 0.9912 - auc: 0.9968 - prc: 0.9933 - val_loss: 0.0532 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9961 - val_prc: 0.9819\n",
      "Epoch 188/200\n",
      "91/91 [==============================] - 5s 57ms/step - loss: 0.0894 - tp: 8999.0000 - fp: 70.0000 - tn: 27158.0000 - fn: 77.0000 - accuracy: 0.9918 - precision: 0.9923 - recall: 0.9915 - auc: 0.9968 - prc: 0.9936 - val_loss: 0.0732 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9958 - val_prc: 0.9804\n",
      "Epoch 189/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0861 - tp: 8989.0000 - fp: 78.0000 - tn: 27150.0000 - fn: 87.0000 - accuracy: 0.9907 - precision: 0.9914 - recall: 0.9904 - auc: 0.9977 - prc: 0.9949 - val_loss: 0.0636 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9957 - val_prc: 0.9872\n",
      "Epoch 190/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0859 - tp: 9001.0000 - fp: 68.0000 - tn: 27160.0000 - fn: 75.0000 - accuracy: 0.9920 - precision: 0.9925 - recall: 0.9917 - auc: 0.9969 - prc: 0.9939 - val_loss: 0.0442 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9964 - val_prc: 0.9946\n",
      "Epoch 191/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0853 - tp: 8998.0000 - fp: 65.0000 - tn: 27163.0000 - fn: 78.0000 - accuracy: 0.9917 - precision: 0.9928 - recall: 0.9914 - auc: 0.9971 - prc: 0.9937 - val_loss: 0.0520 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9963 - val_prc: 0.9898\n",
      "Epoch 192/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0894 - tp: 8993.0000 - fp: 72.0000 - tn: 27156.0000 - fn: 83.0000 - accuracy: 0.9913 - precision: 0.9921 - recall: 0.9909 - auc: 0.9972 - prc: 0.9938 - val_loss: 0.0524 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9961 - val_prc: 0.9810\n",
      "Epoch 193/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.0880 - tp: 9000.0000 - fp: 71.0000 - tn: 27157.0000 - fn: 76.0000 - accuracy: 0.9920 - precision: 0.9922 - recall: 0.9916 - auc: 0.9971 - prc: 0.9940 - val_loss: 0.0588 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9961 - val_prc: 0.9860\n",
      "Epoch 194/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0884 - tp: 8999.0000 - fp: 67.0000 - tn: 27161.0000 - fn: 77.0000 - accuracy: 0.9918 - precision: 0.9926 - recall: 0.9915 - auc: 0.9970 - prc: 0.9940 - val_loss: 0.0455 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9964 - val_prc: 0.9920\n",
      "Epoch 195/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0833 - tp: 8994.0000 - fp: 69.0000 - tn: 27159.0000 - fn: 82.0000 - accuracy: 0.9920 - precision: 0.9924 - recall: 0.9910 - auc: 0.9972 - prc: 0.9943 - val_loss: 0.0553 - val_tp: 2256.0000 - val_fp: 13.0000 - val_tn: 6794.0000 - val_fn: 13.0000 - val_accuracy: 0.9943 - val_precision: 0.9943 - val_recall: 0.9943 - val_auc: 0.9963 - val_prc: 0.9920\n",
      "Epoch 196/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0891 - tp: 8994.0000 - fp: 76.0000 - tn: 27152.0000 - fn: 82.0000 - accuracy: 0.9914 - precision: 0.9916 - recall: 0.9910 - auc: 0.9971 - prc: 0.9936 - val_loss: 0.0834 - val_tp: 2252.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 17.0000 - val_accuracy: 0.9925 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9956 - val_prc: 0.9916\n",
      "Epoch 197/200\n",
      "91/91 [==============================] - 5s 52ms/step - loss: 0.0846 - tp: 8995.0000 - fp: 67.0000 - tn: 27161.0000 - fn: 81.0000 - accuracy: 0.9916 - precision: 0.9926 - recall: 0.9911 - auc: 0.9972 - prc: 0.9944 - val_loss: 0.0498 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9964 - val_prc: 0.9930\n",
      "Epoch 198/200\n",
      "91/91 [==============================] - 5s 54ms/step - loss: 0.0864 - tp: 8997.0000 - fp: 69.0000 - tn: 27159.0000 - fn: 79.0000 - accuracy: 0.9921 - precision: 0.9924 - recall: 0.9913 - auc: 0.9967 - prc: 0.9934 - val_loss: 0.0734 - val_tp: 2252.0000 - val_fp: 17.0000 - val_tn: 6790.0000 - val_fn: 17.0000 - val_accuracy: 0.9925 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9961 - val_prc: 0.9918\n",
      "Epoch 199/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0855 - tp: 9000.0000 - fp: 68.0000 - tn: 27160.0000 - fn: 76.0000 - accuracy: 0.9921 - precision: 0.9925 - recall: 0.9916 - auc: 0.9969 - prc: 0.9939 - val_loss: 0.0491 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9964 - val_prc: 0.9926\n",
      "Epoch 200/200\n",
      "91/91 [==============================] - 5s 55ms/step - loss: 0.0809 - tp: 8997.0000 - fp: 68.0000 - tn: 27160.0000 - fn: 79.0000 - accuracy: 0.9918 - precision: 0.9925 - recall: 0.9913 - auc: 0.9973 - prc: 0.9944 - val_loss: 0.0446 - val_tp: 2257.0000 - val_fp: 12.0000 - val_tn: 6795.0000 - val_fn: 12.0000 - val_accuracy: 0.9947 - val_precision: 0.9947 - val_recall: 0.9947 - val_auc: 0.9962 - val_prc: 0.9900\n"
     ]
    }
   ],
   "source": [
    "hist_GRU_embedding=GRU_embedding.fit([x_train,x_train_meta],lbl_train,epochs=200,\n",
    "                               validation_data=([x_test,x_test_meta],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c6368a5-178e-45f9-919b-89b0915154e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "GRU_embedding.save('GRU_embedding_model.h5') # first 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f96ed-d759-4f89-9ece-3cd222ffa976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# GRU_embedding = load_model('GRU_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28c0a26c",
   "metadata": {
    "id": "28c0a26c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 2s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test_GRU = GRU_embedding.predict([x_test,x_test_meta])\n",
    "lbl_pred_GRU = np.argmax(pred_test_GRU,axis=1).astype(int)\n",
    "lbl_real_GRU = np.argmax(lbl_test,axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7e46d5a",
   "metadata": {
    "id": "b7e46d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC\n",
      "0.994  -0.994\n",
      "0.994 + -0.994\n",
      "f1_score\n",
      "0.994  -0.003\n",
      "0.994 + 0.003\n",
      "accuracy\n",
      "0.995  -0.003\n",
      "0.995 + 0.002\n",
      "precision\n",
      "0.998  -0.001\n",
      "0.998 + 0.001\n",
      "recall\n",
      "0.991  -0.006\n",
      "0.991 + 0.005\n"
     ]
    }
   ],
   "source": [
    "roc_GRU, f1_score_GRU, accuracy_GRU, precision_GRU, recall_GRU = BootstrapR2(pred_test_GRU,lbl_test, numboot=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ca28de3",
   "metadata": {
    "id": "6ca28de3"
   },
   "outputs": [],
   "source": [
    "################################################## CNN\n",
    "\n",
    "def Create_CNN(data,nb_words,maxlen,embedding_dim,data_meta):\n",
    "\n",
    "\n",
    "  # meta input\n",
    "  input_meta = keras.layers.Input(shape=data_meta.shape[1:],dtype='float64',name=\"input_meta\")\n",
    "  layer_meta = keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta.1\")(input_meta)\n",
    "  layer_meta = keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "  layer_meta = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(1),\n",
    "                                name=\"cov_meta.2\")(layer_meta)\n",
    "  layer_meta = keras.layers.GlobalAveragePooling1D(name=\"avg_meta\")(layer_meta)\n",
    "\n",
    "\n",
    "  # txt input\n",
    "  word_input_cnn = keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")\n",
    "  em_layer_cnn = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data = keras.layers.BatchNormalization()(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.1)(layer_data)\n",
    "\n",
    "  layer_data = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data = keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool1\")(layer_data)\n",
    "\n",
    "  layer_data = keras.layers.BatchNormalization()(layer_data)\n",
    "  layer_data = keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data = keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 strides=(2),\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_4\")(layer_data)\n",
    "  layer_data = keras.layers.GlobalAveragePooling1D(name=\"data_maxpool2\")(layer_data)\n",
    "\n",
    "\n",
    "\n",
    "  # hybrid\n",
    "  layer_last = keras.layers.Concatenate(axis=-1)([layer_meta,layer_data])\n",
    "  layer_last = keras.layers.Flatten()(layer_last)\n",
    "\n",
    "\n",
    "  layer_last = keras.layers.Dense(64,activation=\"relu\",\n",
    "                                 name=\"dens_2\")(layer_last)\n",
    "  layer_last = keras.layers.Dropout(0.5)(layer_last)\n",
    "  output = keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta],\n",
    "                       outputs=output)\n",
    "  opti = keras.optimizers.Adam(learning_rate=0.00001)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=METRICS)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ed92d3b",
   "metadata": {
    "id": "3ed92d3b"
   },
   "outputs": [],
   "source": [
    "CNN_embedding = Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta=x_train_meta_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c614d164",
   "metadata": {
    "id": "c614d164",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/91 [==============================] - 4s 46ms/step - loss: 1.2049 - tp: 38.0000 - fp: 2.0000 - tn: 27226.0000 - fn: 9038.0000 - accuracy: 0.5322 - precision: 0.9500 - recall: 0.0042 - auc: 0.8058 - prc: 0.5315 - val_loss: 5.7985 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4648 - val_prc: 0.2159\n",
      "Epoch 2/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1976 - tp: 51.0000 - fp: 3.0000 - tn: 27225.0000 - fn: 9025.0000 - accuracy: 0.5391 - precision: 0.9444 - recall: 0.0056 - auc: 0.8094 - prc: 0.5457 - val_loss: 1.7532 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5734 - val_prc: 0.2865\n",
      "Epoch 3/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1933 - tp: 55.0000 - fp: 0.0000e+00 - tn: 27228.0000 - fn: 9021.0000 - accuracy: 0.5382 - precision: 1.0000 - recall: 0.0061 - auc: 0.8105 - prc: 0.5481 - val_loss: 3.0613 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4877 - val_prc: 0.2222\n",
      "Epoch 4/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1824 - tp: 69.0000 - fp: 1.0000 - tn: 27227.0000 - fn: 9007.0000 - accuracy: 0.5422 - precision: 0.9857 - recall: 0.0076 - auc: 0.8104 - prc: 0.5433 - val_loss: 1.2435 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2269.0000 - val_accuracy: 0.5584 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7928 - val_prc: 0.6011\n",
      "Epoch 5/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1738 - tp: 62.0000 - fp: 9.0000 - tn: 27219.0000 - fn: 9014.0000 - accuracy: 0.5448 - precision: 0.8732 - recall: 0.0068 - auc: 0.8145 - prc: 0.5520 - val_loss: 6.0764 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4648 - val_prc: 0.2159\n",
      "Epoch 6/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1683 - tp: 81.0000 - fp: 3.0000 - tn: 27225.0000 - fn: 8995.0000 - accuracy: 0.5506 - precision: 0.9643 - recall: 0.0089 - auc: 0.8171 - prc: 0.5580 - val_loss: 1.2925 - val_tp: 0.0000e+00 - val_fp: 2.0000 - val_tn: 6805.0000 - val_fn: 2269.0000 - val_accuracy: 0.2591 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5913 - val_prc: 0.3026\n",
      "Epoch 7/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 1.1593 - tp: 106.0000 - fp: 8.0000 - tn: 27220.0000 - fn: 8970.0000 - accuracy: 0.5443 - precision: 0.9298 - recall: 0.0117 - auc: 0.8157 - prc: 0.5539 - val_loss: 1.5271 - val_tp: 0.0000e+00 - val_fp: 1.0000 - val_tn: 6806.0000 - val_fn: 2269.0000 - val_accuracy: 0.1966 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5614 - val_prc: 0.2647\n",
      "Epoch 8/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1450 - tp: 116.0000 - fp: 11.0000 - tn: 27217.0000 - fn: 8960.0000 - accuracy: 0.5554 - precision: 0.9134 - recall: 0.0128 - auc: 0.8211 - prc: 0.5660 - val_loss: 1.5345 - val_tp: 22.0000 - val_fp: 294.0000 - val_tn: 6513.0000 - val_fn: 2247.0000 - val_accuracy: 0.2076 - val_precision: 0.0696 - val_recall: 0.0097 - val_auc: 0.5537 - val_prc: 0.2517\n",
      "Epoch 9/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 1.1403 - tp: 119.0000 - fp: 14.0000 - tn: 27214.0000 - fn: 8957.0000 - accuracy: 0.5583 - precision: 0.8947 - recall: 0.0131 - auc: 0.8227 - prc: 0.5681 - val_loss: 1.7586 - val_tp: 86.0000 - val_fp: 1778.0000 - val_tn: 5029.0000 - val_fn: 2183.0000 - val_accuracy: 0.0405 - val_precision: 0.0461 - val_recall: 0.0379 - val_auc: 0.4829 - val_prc: 0.2217\n",
      "Epoch 10/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 1.1291 - tp: 133.0000 - fp: 13.0000 - tn: 27215.0000 - fn: 8943.0000 - accuracy: 0.5589 - precision: 0.9110 - recall: 0.0147 - auc: 0.8240 - prc: 0.5699 - val_loss: 1.3785 - val_tp: 4.0000 - val_fp: 31.0000 - val_tn: 6776.0000 - val_fn: 2265.0000 - val_accuracy: 0.2323 - val_precision: 0.1143 - val_recall: 0.0018 - val_auc: 0.5875 - val_prc: 0.2743\n",
      "Epoch 11/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1252 - tp: 136.0000 - fp: 17.0000 - tn: 27211.0000 - fn: 8940.0000 - accuracy: 0.5717 - precision: 0.8889 - recall: 0.0150 - auc: 0.8299 - prc: 0.5766 - val_loss: 1.6598 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5750 - val_prc: 0.2913\n",
      "Epoch 12/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1197 - tp: 148.0000 - fp: 18.0000 - tn: 27210.0000 - fn: 8928.0000 - accuracy: 0.5562 - precision: 0.8916 - recall: 0.0163 - auc: 0.8242 - prc: 0.5650 - val_loss: 4.3815 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4653 - val_prc: 0.2162\n",
      "Epoch 13/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1063 - tp: 171.0000 - fp: 21.0000 - tn: 27207.0000 - fn: 8905.0000 - accuracy: 0.5745 - precision: 0.8906 - recall: 0.0188 - auc: 0.8304 - prc: 0.5833 - val_loss: 1.7748 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5757 - val_prc: 0.2846\n",
      "Epoch 14/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.1017 - tp: 174.0000 - fp: 25.0000 - tn: 27203.0000 - fn: 8902.0000 - accuracy: 0.5776 - precision: 0.8744 - recall: 0.0192 - auc: 0.8323 - prc: 0.5830 - val_loss: 6.5878 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4640 - val_prc: 0.2153\n",
      "Epoch 15/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0913 - tp: 207.0000 - fp: 30.0000 - tn: 27198.0000 - fn: 8869.0000 - accuracy: 0.5700 - precision: 0.8734 - recall: 0.0228 - auc: 0.8325 - prc: 0.5863 - val_loss: 2.0772 - val_tp: 91.0000 - val_fp: 2154.0000 - val_tn: 4653.0000 - val_fn: 2178.0000 - val_accuracy: 0.0405 - val_precision: 0.0405 - val_recall: 0.0401 - val_auc: 0.4743 - val_prc: 0.2190\n",
      "Epoch 16/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0800 - tp: 234.0000 - fp: 27.0000 - tn: 27201.0000 - fn: 8842.0000 - accuracy: 0.5780 - precision: 0.8966 - recall: 0.0258 - auc: 0.8375 - prc: 0.5958 - val_loss: 2.7682 - val_tp: 92.0000 - val_fp: 2176.0000 - val_tn: 4631.0000 - val_fn: 2177.0000 - val_accuracy: 0.0405 - val_precision: 0.0406 - val_recall: 0.0405 - val_auc: 0.4838 - val_prc: 0.2216\n",
      "Epoch 17/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0746 - tp: 244.0000 - fp: 37.0000 - tn: 27191.0000 - fn: 8832.0000 - accuracy: 0.5756 - precision: 0.8683 - recall: 0.0269 - auc: 0.8347 - prc: 0.5867 - val_loss: 1.1957 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 6807.0000 - val_fn: 2268.0000 - val_accuracy: 0.5333 - val_precision: 1.0000 - val_recall: 4.4072e-04 - val_auc: 0.7736 - val_prc: 0.6026\n",
      "Epoch 18/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0615 - tp: 259.0000 - fp: 37.0000 - tn: 27191.0000 - fn: 8817.0000 - accuracy: 0.5853 - precision: 0.8750 - recall: 0.0285 - auc: 0.8409 - prc: 0.6002 - val_loss: 2.2232 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5721 - val_prc: 0.2880\n",
      "Epoch 19/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0561 - tp: 275.0000 - fp: 35.0000 - tn: 27193.0000 - fn: 8801.0000 - accuracy: 0.5922 - precision: 0.8871 - recall: 0.0303 - auc: 0.8414 - prc: 0.6008 - val_loss: 2.1379 - val_tp: 91.0000 - val_fp: 2164.0000 - val_tn: 4643.0000 - val_fn: 2178.0000 - val_accuracy: 0.0405 - val_precision: 0.0404 - val_recall: 0.0401 - val_auc: 0.4956 - val_prc: 0.2251\n",
      "Epoch 20/100\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 1.0446 - tp: 319.0000 - fp: 41.0000 - tn: 27187.0000 - fn: 8757.0000 - accuracy: 0.5956 - precision: 0.8861 - recall: 0.0351 - auc: 0.8474 - prc: 0.6165 - val_loss: 1.4462 - val_tp: 12.0000 - val_fp: 153.0000 - val_tn: 6654.0000 - val_fn: 2257.0000 - val_accuracy: 0.1604 - val_precision: 0.0727 - val_recall: 0.0053 - val_auc: 0.5321 - val_prc: 0.2451\n",
      "Epoch 21/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0397 - tp: 301.0000 - fp: 63.0000 - tn: 27165.0000 - fn: 8775.0000 - accuracy: 0.6037 - precision: 0.8269 - recall: 0.0332 - auc: 0.8486 - prc: 0.6110 - val_loss: 5.4841 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4649 - val_prc: 0.2159\n",
      "Epoch 22/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0237 - tp: 354.0000 - fp: 43.0000 - tn: 27185.0000 - fn: 8722.0000 - accuracy: 0.6081 - precision: 0.8917 - recall: 0.0390 - auc: 0.8522 - prc: 0.6253 - val_loss: 9.1228 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4140 - val_prc: 0.2000\n",
      "Epoch 23/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 1.0107 - tp: 381.0000 - fp: 56.0000 - tn: 27172.0000 - fn: 8695.0000 - accuracy: 0.6052 - precision: 0.8719 - recall: 0.0420 - auc: 0.8534 - prc: 0.6266 - val_loss: 1.5095 - val_tp: 358.0000 - val_fp: 1690.0000 - val_tn: 5117.0000 - val_fn: 1911.0000 - val_accuracy: 0.1679 - val_precision: 0.1748 - val_recall: 0.1578 - val_auc: 0.5895 - val_prc: 0.2734\n",
      "Epoch 24/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 1.0093 - tp: 399.0000 - fp: 67.0000 - tn: 27161.0000 - fn: 8677.0000 - accuracy: 0.6167 - precision: 0.8562 - recall: 0.0440 - auc: 0.8578 - prc: 0.6340 - val_loss: 2.2008 - val_tp: 92.0000 - val_fp: 2117.0000 - val_tn: 4690.0000 - val_fn: 2177.0000 - val_accuracy: 0.0428 - val_precision: 0.0416 - val_recall: 0.0405 - val_auc: 0.4970 - val_prc: 0.2253\n",
      "Epoch 25/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.9979 - tp: 415.0000 - fp: 63.0000 - tn: 27165.0000 - fn: 8661.0000 - accuracy: 0.6162 - precision: 0.8682 - recall: 0.0457 - auc: 0.8594 - prc: 0.6349 - val_loss: 2.1197 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5702 - val_prc: 0.2871\n",
      "Epoch 26/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.9888 - tp: 461.0000 - fp: 72.0000 - tn: 27156.0000 - fn: 8615.0000 - accuracy: 0.6256 - precision: 0.8649 - recall: 0.0508 - auc: 0.8625 - prc: 0.6449 - val_loss: 3.4143 - val_tp: 92.0000 - val_fp: 2177.0000 - val_tn: 4630.0000 - val_fn: 2177.0000 - val_accuracy: 0.0405 - val_precision: 0.0405 - val_recall: 0.0405 - val_auc: 0.4845 - val_prc: 0.2219\n",
      "Epoch 27/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.9778 - tp: 489.0000 - fp: 64.0000 - tn: 27164.0000 - fn: 8587.0000 - accuracy: 0.6365 - precision: 0.8843 - recall: 0.0539 - auc: 0.8660 - prc: 0.6537 - val_loss: 1.7684 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5767 - val_prc: 0.2891\n",
      "Epoch 28/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.9627 - tp: 529.0000 - fp: 72.0000 - tn: 27156.0000 - fn: 8547.0000 - accuracy: 0.6363 - precision: 0.8802 - recall: 0.0583 - auc: 0.8678 - prc: 0.6593 - val_loss: 3.1739 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4589 - val_prc: 0.2137\n",
      "Epoch 29/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.9565 - tp: 577.0000 - fp: 108.0000 - tn: 27120.0000 - fn: 8499.0000 - accuracy: 0.6373 - precision: 0.8423 - recall: 0.0636 - auc: 0.8688 - prc: 0.6607 - val_loss: 2.5397 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5673 - val_prc: 0.2826\n",
      "Epoch 30/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.9515 - tp: 575.0000 - fp: 100.0000 - tn: 27128.0000 - fn: 8501.0000 - accuracy: 0.6426 - precision: 0.8519 - recall: 0.0634 - auc: 0.8731 - prc: 0.6649 - val_loss: 1.3215 - val_tp: 587.0000 - val_fp: 1491.0000 - val_tn: 5316.0000 - val_fn: 1682.0000 - val_accuracy: 0.2874 - val_precision: 0.2825 - val_recall: 0.2587 - val_auc: 0.7044 - val_prc: 0.3673\n",
      "Epoch 31/100\n",
      "91/91 [==============================] - 4s 45ms/step - loss: 0.9382 - tp: 662.0000 - fp: 91.0000 - tn: 27137.0000 - fn: 8414.0000 - accuracy: 0.6503 - precision: 0.8792 - recall: 0.0729 - auc: 0.8763 - prc: 0.6749 - val_loss: 2.4243 - val_tp: 350.0000 - val_fp: 1268.0000 - val_tn: 5539.0000 - val_fn: 1919.0000 - val_accuracy: 0.2115 - val_precision: 0.2163 - val_recall: 0.1543 - val_auc: 0.5538 - val_prc: 0.2569\n",
      "Epoch 32/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.9330 - tp: 648.0000 - fp: 98.0000 - tn: 27130.0000 - fn: 8428.0000 - accuracy: 0.6555 - precision: 0.8686 - recall: 0.0714 - auc: 0.8775 - prc: 0.6769 - val_loss: 1.4563 - val_tp: 58.0000 - val_fp: 146.0000 - val_tn: 6661.0000 - val_fn: 2211.0000 - val_accuracy: 0.1855 - val_precision: 0.2843 - val_recall: 0.0256 - val_auc: 0.5581 - val_prc: 0.2596\n",
      "Epoch 33/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.9237 - tp: 728.0000 - fp: 131.0000 - tn: 27097.0000 - fn: 8348.0000 - accuracy: 0.6591 - precision: 0.8475 - recall: 0.0802 - auc: 0.8795 - prc: 0.6793 - val_loss: 1.9579 - val_tp: 88.0000 - val_fp: 1867.0000 - val_tn: 4940.0000 - val_fn: 2181.0000 - val_accuracy: 0.0494 - val_precision: 0.0450 - val_recall: 0.0388 - val_auc: 0.4884 - val_prc: 0.2227\n",
      "Epoch 34/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.9165 - tp: 719.0000 - fp: 118.0000 - tn: 27110.0000 - fn: 8357.0000 - accuracy: 0.6625 - precision: 0.8590 - recall: 0.0792 - auc: 0.8831 - prc: 0.6893 - val_loss: 1.4585 - val_tp: 53.0000 - val_fp: 154.0000 - val_tn: 6653.0000 - val_fn: 2216.0000 - val_accuracy: 0.1644 - val_precision: 0.2560 - val_recall: 0.0234 - val_auc: 0.5410 - val_prc: 0.2524\n",
      "Epoch 35/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.9056 - tp: 822.0000 - fp: 139.0000 - tn: 27089.0000 - fn: 8254.0000 - accuracy: 0.6686 - precision: 0.8554 - recall: 0.0906 - auc: 0.8856 - prc: 0.6949 - val_loss: 1.9502 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5722 - val_prc: 0.2877\n",
      "Epoch 36/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8961 - tp: 830.0000 - fp: 124.0000 - tn: 27104.0000 - fn: 8246.0000 - accuracy: 0.6671 - precision: 0.8700 - recall: 0.0914 - auc: 0.8888 - prc: 0.7035 - val_loss: 1.4759 - val_tp: 81.0000 - val_fp: 286.0000 - val_tn: 6521.0000 - val_fn: 2188.0000 - val_accuracy: 0.1657 - val_precision: 0.2207 - val_recall: 0.0357 - val_auc: 0.5371 - val_prc: 0.2519\n",
      "Epoch 37/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.8828 - tp: 875.0000 - fp: 124.0000 - tn: 27104.0000 - fn: 8201.0000 - accuracy: 0.6850 - precision: 0.8759 - recall: 0.0964 - auc: 0.8930 - prc: 0.7119 - val_loss: 1.8623 - val_tp: 246.0000 - val_fp: 865.0000 - val_tn: 5942.0000 - val_fn: 2023.0000 - val_accuracy: 0.2151 - val_precision: 0.2214 - val_recall: 0.1084 - val_auc: 0.5594 - val_prc: 0.2600\n",
      "Epoch 38/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.8765 - tp: 936.0000 - fp: 141.0000 - tn: 27087.0000 - fn: 8140.0000 - accuracy: 0.6857 - precision: 0.8691 - recall: 0.1031 - auc: 0.8945 - prc: 0.7156 - val_loss: 5.3609 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4633 - val_prc: 0.2155\n",
      "Epoch 39/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8739 - tp: 975.0000 - fp: 154.0000 - tn: 27074.0000 - fn: 8101.0000 - accuracy: 0.6800 - precision: 0.8636 - recall: 0.1074 - auc: 0.8905 - prc: 0.7080 - val_loss: 1.8601 - val_tp: 375.0000 - val_fp: 1875.0000 - val_tn: 4932.0000 - val_fn: 1894.0000 - val_accuracy: 0.1684 - val_precision: 0.1667 - val_recall: 0.1653 - val_auc: 0.5499 - val_prc: 0.2515\n",
      "Epoch 40/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8549 - tp: 1087.0000 - fp: 156.0000 - tn: 27072.0000 - fn: 7989.0000 - accuracy: 0.7016 - precision: 0.8745 - recall: 0.1198 - auc: 0.9027 - prc: 0.7335 - val_loss: 7.0246 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4617 - val_prc: 0.2148\n",
      "Epoch 41/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8513 - tp: 1126.0000 - fp: 161.0000 - tn: 27067.0000 - fn: 7950.0000 - accuracy: 0.7015 - precision: 0.8749 - recall: 0.1241 - auc: 0.9042 - prc: 0.7381 - val_loss: 4.6839 - val_tp: 92.0000 - val_fp: 2176.0000 - val_tn: 4631.0000 - val_fn: 2177.0000 - val_accuracy: 0.0405 - val_precision: 0.0406 - val_recall: 0.0405 - val_auc: 0.4630 - val_prc: 0.2154\n",
      "Epoch 42/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.8459 - tp: 1168.0000 - fp: 193.0000 - tn: 27035.0000 - fn: 7908.0000 - accuracy: 0.6995 - precision: 0.8582 - recall: 0.1287 - auc: 0.9003 - prc: 0.7312 - val_loss: 2.8413 - val_tp: 92.0000 - val_fp: 2161.0000 - val_tn: 4646.0000 - val_fn: 2177.0000 - val_accuracy: 0.0405 - val_precision: 0.0408 - val_recall: 0.0405 - val_auc: 0.4740 - val_prc: 0.2186\n",
      "Epoch 43/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8321 - tp: 1296.0000 - fp: 183.0000 - tn: 27045.0000 - fn: 7780.0000 - accuracy: 0.7161 - precision: 0.8763 - recall: 0.1428 - auc: 0.9081 - prc: 0.7502 - val_loss: 1.5335 - val_tp: 1209.0000 - val_fp: 1059.0000 - val_tn: 5748.0000 - val_fn: 1060.0000 - val_accuracy: 0.5328 - val_precision: 0.5331 - val_recall: 0.5328 - val_auc: 0.7536 - val_prc: 0.5699\n",
      "Epoch 44/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8188 - tp: 1381.0000 - fp: 176.0000 - tn: 27052.0000 - fn: 7695.0000 - accuracy: 0.7201 - precision: 0.8870 - recall: 0.1522 - auc: 0.9112 - prc: 0.7588 - val_loss: 1.1925 - val_tp: 32.0000 - val_fp: 105.0000 - val_tn: 6702.0000 - val_fn: 2237.0000 - val_accuracy: 0.4976 - val_precision: 0.2336 - val_recall: 0.0141 - val_auc: 0.7504 - val_prc: 0.4750\n",
      "Epoch 45/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8148 - tp: 1448.0000 - fp: 189.0000 - tn: 27039.0000 - fn: 7628.0000 - accuracy: 0.7184 - precision: 0.8845 - recall: 0.1595 - auc: 0.9117 - prc: 0.7596 - val_loss: 1.3793 - val_tp: 37.0000 - val_fp: 373.0000 - val_tn: 6434.0000 - val_fn: 2232.0000 - val_accuracy: 0.3495 - val_precision: 0.0902 - val_recall: 0.0163 - val_auc: 0.6339 - val_prc: 0.3191\n",
      "Epoch 46/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.8100 - tp: 1549.0000 - fp: 232.0000 - tn: 26996.0000 - fn: 7527.0000 - accuracy: 0.7255 - precision: 0.8697 - recall: 0.1707 - auc: 0.9127 - prc: 0.7578 - val_loss: 2.4213 - val_tp: 370.0000 - val_fp: 1818.0000 - val_tn: 4989.0000 - val_fn: 1899.0000 - val_accuracy: 0.1732 - val_precision: 0.1691 - val_recall: 0.1631 - val_auc: 0.5445 - val_prc: 0.2458\n",
      "Epoch 47/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7995 - tp: 1650.0000 - fp: 226.0000 - tn: 27002.0000 - fn: 7426.0000 - accuracy: 0.7298 - precision: 0.8795 - recall: 0.1818 - auc: 0.9149 - prc: 0.7666 - val_loss: 14.5355 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.3601 - val_prc: 0.1846\n",
      "Epoch 48/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7850 - tp: 1742.0000 - fp: 206.0000 - tn: 27022.0000 - fn: 7334.0000 - accuracy: 0.7369 - precision: 0.8943 - recall: 0.1919 - auc: 0.9204 - prc: 0.7832 - val_loss: 1.2467 - val_tp: 23.0000 - val_fp: 50.0000 - val_tn: 6757.0000 - val_fn: 2246.0000 - val_accuracy: 0.3253 - val_precision: 0.3151 - val_recall: 0.0101 - val_auc: 0.6629 - val_prc: 0.3288\n",
      "Epoch 49/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7830 - tp: 1790.0000 - fp: 231.0000 - tn: 26997.0000 - fn: 7286.0000 - accuracy: 0.7337 - precision: 0.8857 - recall: 0.1972 - auc: 0.9193 - prc: 0.7783 - val_loss: 1.9239 - val_tp: 358.0000 - val_fp: 1722.0000 - val_tn: 5085.0000 - val_fn: 1911.0000 - val_accuracy: 0.1772 - val_precision: 0.1721 - val_recall: 0.1578 - val_auc: 0.5470 - val_prc: 0.2494\n",
      "Epoch 50/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.7737 - tp: 1909.0000 - fp: 220.0000 - tn: 27008.0000 - fn: 7167.0000 - accuracy: 0.7482 - precision: 0.8967 - recall: 0.2103 - auc: 0.9232 - prc: 0.7873 - val_loss: 1.2084 - val_tp: 27.0000 - val_fp: 78.0000 - val_tn: 6729.0000 - val_fn: 2242.0000 - val_accuracy: 0.4795 - val_precision: 0.2571 - val_recall: 0.0119 - val_auc: 0.7407 - val_prc: 0.4647\n",
      "Epoch 51/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.7713 - tp: 1979.0000 - fp: 223.0000 - tn: 27005.0000 - fn: 7097.0000 - accuracy: 0.7521 - precision: 0.8987 - recall: 0.2180 - auc: 0.9258 - prc: 0.7953 - val_loss: 3.2590 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5638 - val_prc: 0.2802\n",
      "Epoch 52/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7581 - tp: 2060.0000 - fp: 244.0000 - tn: 26984.0000 - fn: 7016.0000 - accuracy: 0.7531 - precision: 0.8941 - recall: 0.2270 - auc: 0.9266 - prc: 0.7955 - val_loss: 1.2865 - val_tp: 60.0000 - val_fp: 138.0000 - val_tn: 6669.0000 - val_fn: 2209.0000 - val_accuracy: 0.2653 - val_precision: 0.3030 - val_recall: 0.0264 - val_auc: 0.6372 - val_prc: 0.3027\n",
      "Epoch 53/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.7474 - tp: 2240.0000 - fp: 262.0000 - tn: 26966.0000 - fn: 6836.0000 - accuracy: 0.7642 - precision: 0.8953 - recall: 0.2468 - auc: 0.9298 - prc: 0.8054 - val_loss: 3.7641 - val_tp: 92.0000 - val_fp: 2176.0000 - val_tn: 4631.0000 - val_fn: 2177.0000 - val_accuracy: 0.0405 - val_precision: 0.0406 - val_recall: 0.0405 - val_auc: 0.4815 - val_prc: 0.2209\n",
      "Epoch 54/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7425 - tp: 2330.0000 - fp: 237.0000 - tn: 26991.0000 - fn: 6746.0000 - accuracy: 0.7643 - precision: 0.9077 - recall: 0.2567 - auc: 0.9309 - prc: 0.8094 - val_loss: 2.1696 - val_tp: 1058.0000 - val_fp: 1183.0000 - val_tn: 5624.0000 - val_fn: 1211.0000 - val_accuracy: 0.4689 - val_precision: 0.4721 - val_recall: 0.4663 - val_auc: 0.6467 - val_prc: 0.3897\n",
      "Epoch 55/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7267 - tp: 2514.0000 - fp: 246.0000 - tn: 26982.0000 - fn: 6562.0000 - accuracy: 0.7670 - precision: 0.9109 - recall: 0.2770 - auc: 0.9350 - prc: 0.8198 - val_loss: 8.8245 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.3667 - val_prc: 0.1863\n",
      "Epoch 56/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.7197 - tp: 2639.0000 - fp: 264.0000 - tn: 26964.0000 - fn: 6437.0000 - accuracy: 0.7684 - precision: 0.9091 - recall: 0.2908 - auc: 0.9353 - prc: 0.8212 - val_loss: 4.3411 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.4702 - val_prc: 0.2505\n",
      "Epoch 57/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7125 - tp: 2745.0000 - fp: 267.0000 - tn: 26961.0000 - fn: 6331.0000 - accuracy: 0.7724 - precision: 0.9114 - recall: 0.3024 - auc: 0.9369 - prc: 0.8246 - val_loss: 1.4178 - val_tp: 262.0000 - val_fp: 1095.0000 - val_tn: 5712.0000 - val_fn: 2007.0000 - val_accuracy: 0.1944 - val_precision: 0.1931 - val_recall: 0.1155 - val_auc: 0.5981 - val_prc: 0.2765\n",
      "Epoch 58/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7107 - tp: 2875.0000 - fp: 299.0000 - tn: 26929.0000 - fn: 6201.0000 - accuracy: 0.7743 - precision: 0.9058 - recall: 0.3168 - auc: 0.9388 - prc: 0.8296 - val_loss: 1.6056 - val_tp: 170.0000 - val_fp: 822.0000 - val_tn: 5985.0000 - val_fn: 2099.0000 - val_accuracy: 0.1503 - val_precision: 0.1714 - val_recall: 0.0749 - val_auc: 0.5077 - val_prc: 0.2336\n",
      "Epoch 59/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.7009 - tp: 3048.0000 - fp: 302.0000 - tn: 26926.0000 - fn: 6028.0000 - accuracy: 0.7812 - precision: 0.9099 - recall: 0.3358 - auc: 0.9405 - prc: 0.8329 - val_loss: 2.2189 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5869 - val_prc: 0.2953\n",
      "Epoch 60/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6918 - tp: 3107.0000 - fp: 303.0000 - tn: 26925.0000 - fn: 5969.0000 - accuracy: 0.7868 - precision: 0.9111 - recall: 0.3423 - auc: 0.9422 - prc: 0.8394 - val_loss: 1.7176 - val_tp: 360.0000 - val_fp: 1678.0000 - val_tn: 5129.0000 - val_fn: 1909.0000 - val_accuracy: 0.1833 - val_precision: 0.1766 - val_recall: 0.1587 - val_auc: 0.5546 - val_prc: 0.2563\n",
      "Epoch 61/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.6860 - tp: 3181.0000 - fp: 295.0000 - tn: 26933.0000 - fn: 5895.0000 - accuracy: 0.7891 - precision: 0.9151 - recall: 0.3505 - auc: 0.9429 - prc: 0.8414 - val_loss: 6.4418 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.4394 - val_prc: 0.2078\n",
      "Epoch 62/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.6746 - tp: 3356.0000 - fp: 312.0000 - tn: 26916.0000 - fn: 5720.0000 - accuracy: 0.7913 - precision: 0.9149 - recall: 0.3698 - auc: 0.9445 - prc: 0.8470 - val_loss: 1.5388 - val_tp: 285.0000 - val_fp: 997.0000 - val_tn: 5810.0000 - val_fn: 1984.0000 - val_accuracy: 0.2138 - val_precision: 0.2223 - val_recall: 0.1256 - val_auc: 0.5680 - val_prc: 0.2717\n",
      "Epoch 63/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6663 - tp: 3486.0000 - fp: 295.0000 - tn: 26933.0000 - fn: 5590.0000 - accuracy: 0.8013 - precision: 0.9220 - recall: 0.3841 - auc: 0.9488 - prc: 0.8570 - val_loss: 1.3564 - val_tp: 288.0000 - val_fp: 469.0000 - val_tn: 6338.0000 - val_fn: 1981.0000 - val_accuracy: 0.3023 - val_precision: 0.3804 - val_recall: 0.1269 - val_auc: 0.6196 - val_prc: 0.3204\n",
      "Epoch 64/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6658 - tp: 3537.0000 - fp: 323.0000 - tn: 26905.0000 - fn: 5539.0000 - accuracy: 0.7966 - precision: 0.9163 - recall: 0.3897 - auc: 0.9482 - prc: 0.8563 - val_loss: 1.2098 - val_tp: 1119.0000 - val_fp: 948.0000 - val_tn: 5859.0000 - val_fn: 1150.0000 - val_accuracy: 0.5245 - val_precision: 0.5414 - val_recall: 0.4932 - val_auc: 0.7501 - val_prc: 0.5472\n",
      "Epoch 65/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6530 - tp: 3741.0000 - fp: 320.0000 - tn: 26908.0000 - fn: 5335.0000 - accuracy: 0.8037 - precision: 0.9212 - recall: 0.4122 - auc: 0.9494 - prc: 0.8572 - val_loss: 1.9741 - val_tp: 588.0000 - val_fp: 1673.0000 - val_tn: 5134.0000 - val_fn: 1681.0000 - val_accuracy: 0.2596 - val_precision: 0.2601 - val_recall: 0.2591 - val_auc: 0.6693 - val_prc: 0.3442\n",
      "Epoch 66/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6423 - tp: 3937.0000 - fp: 350.0000 - tn: 26878.0000 - fn: 5139.0000 - accuracy: 0.8100 - precision: 0.9184 - recall: 0.4338 - auc: 0.9517 - prc: 0.8665 - val_loss: 2.6792 - val_tp: 381.0000 - val_fp: 1887.0000 - val_tn: 4920.0000 - val_fn: 1888.0000 - val_accuracy: 0.1679 - val_precision: 0.1680 - val_recall: 0.1679 - val_auc: 0.5431 - val_prc: 0.2475\n",
      "Epoch 67/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.6381 - tp: 4032.0000 - fp: 315.0000 - tn: 26913.0000 - fn: 5044.0000 - accuracy: 0.8097 - precision: 0.9275 - recall: 0.4442 - auc: 0.9531 - prc: 0.8693 - val_loss: 5.3581 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5111 - val_prc: 0.2573\n",
      "Epoch 68/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6318 - tp: 4068.0000 - fp: 363.0000 - tn: 26865.0000 - fn: 5008.0000 - accuracy: 0.8075 - precision: 0.9181 - recall: 0.4482 - auc: 0.9517 - prc: 0.8648 - val_loss: 2.0377 - val_tp: 587.0000 - val_fp: 1673.0000 - val_tn: 5134.0000 - val_fn: 1682.0000 - val_accuracy: 0.2596 - val_precision: 0.2597 - val_recall: 0.2587 - val_auc: 0.5927 - val_prc: 0.2985\n",
      "Epoch 69/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6194 - tp: 4333.0000 - fp: 366.0000 - tn: 26862.0000 - fn: 4743.0000 - accuracy: 0.8206 - precision: 0.9221 - recall: 0.4774 - auc: 0.9570 - prc: 0.8780 - val_loss: 4.2266 - val_tp: 380.0000 - val_fp: 1889.0000 - val_tn: 4918.0000 - val_fn: 1889.0000 - val_accuracy: 0.1675 - val_precision: 0.1675 - val_recall: 0.1675 - val_auc: 0.4394 - val_prc: 0.2162\n",
      "Epoch 70/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.6165 - tp: 4342.0000 - fp: 354.0000 - tn: 26874.0000 - fn: 4734.0000 - accuracy: 0.8127 - precision: 0.9246 - recall: 0.4784 - auc: 0.9564 - prc: 0.8775 - val_loss: 26.2607 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.3601 - val_prc: 0.1846\n",
      "Epoch 71/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.6075 - tp: 4524.0000 - fp: 372.0000 - tn: 26856.0000 - fn: 4552.0000 - accuracy: 0.8280 - precision: 0.9240 - recall: 0.4985 - auc: 0.9590 - prc: 0.8850 - val_loss: 1.2606 - val_tp: 199.0000 - val_fp: 455.0000 - val_tn: 6352.0000 - val_fn: 2070.0000 - val_accuracy: 0.3455 - val_precision: 0.3043 - val_recall: 0.0877 - val_auc: 0.6746 - val_prc: 0.3372\n",
      "Epoch 72/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5938 - tp: 4724.0000 - fp: 343.0000 - tn: 26885.0000 - fn: 4352.0000 - accuracy: 0.8320 - precision: 0.9323 - recall: 0.5205 - auc: 0.9608 - prc: 0.8909 - val_loss: 2.2139 - val_tp: 376.0000 - val_fp: 1865.0000 - val_tn: 4942.0000 - val_fn: 1893.0000 - val_accuracy: 0.1692 - val_precision: 0.1678 - val_recall: 0.1657 - val_auc: 0.5496 - val_prc: 0.2521\n",
      "Epoch 73/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5840 - tp: 4812.0000 - fp: 346.0000 - tn: 26882.0000 - fn: 4264.0000 - accuracy: 0.8372 - precision: 0.9329 - recall: 0.5302 - auc: 0.9631 - prc: 0.8963 - val_loss: 1.9029 - val_tp: 443.0000 - val_fp: 1226.0000 - val_tn: 5581.0000 - val_fn: 1826.0000 - val_accuracy: 0.2389 - val_precision: 0.2654 - val_recall: 0.1952 - val_auc: 0.5693 - val_prc: 0.2844\n",
      "Epoch 74/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5873 - tp: 4812.0000 - fp: 383.0000 - tn: 26845.0000 - fn: 4264.0000 - accuracy: 0.8270 - precision: 0.9263 - recall: 0.5302 - auc: 0.9596 - prc: 0.8878 - val_loss: 1.7705 - val_tp: 369.0000 - val_fp: 1706.0000 - val_tn: 5101.0000 - val_fn: 1900.0000 - val_accuracy: 0.1807 - val_precision: 0.1778 - val_recall: 0.1626 - val_auc: 0.5616 - val_prc: 0.2586\n",
      "Epoch 75/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.5698 - tp: 5065.0000 - fp: 361.0000 - tn: 26867.0000 - fn: 4011.0000 - accuracy: 0.8400 - precision: 0.9335 - recall: 0.5581 - auc: 0.9651 - prc: 0.9005 - val_loss: 1.6963 - val_tp: 340.0000 - val_fp: 1524.0000 - val_tn: 5283.0000 - val_fn: 1929.0000 - val_accuracy: 0.1758 - val_precision: 0.1824 - val_recall: 0.1498 - val_auc: 0.5790 - val_prc: 0.2684\n",
      "Epoch 76/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5670 - tp: 5156.0000 - fp: 386.0000 - tn: 26842.0000 - fn: 3920.0000 - accuracy: 0.8421 - precision: 0.9304 - recall: 0.5681 - auc: 0.9645 - prc: 0.9009 - val_loss: 4.3987 - val_tp: 382.0000 - val_fp: 1884.0000 - val_tn: 4923.0000 - val_fn: 1887.0000 - val_accuracy: 0.1688 - val_precision: 0.1686 - val_recall: 0.1684 - val_auc: 0.4389 - val_prc: 0.2162\n",
      "Epoch 77/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5519 - tp: 5314.0000 - fp: 343.0000 - tn: 26885.0000 - fn: 3762.0000 - accuracy: 0.8475 - precision: 0.9394 - recall: 0.5855 - auc: 0.9673 - prc: 0.9081 - val_loss: 22.5737 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.3601 - val_prc: 0.1846\n",
      "Epoch 78/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.5485 - tp: 5430.0000 - fp: 381.0000 - tn: 26847.0000 - fn: 3646.0000 - accuracy: 0.8484 - precision: 0.9344 - recall: 0.5983 - auc: 0.9666 - prc: 0.9070 - val_loss: 2.8484 - val_tp: 588.0000 - val_fp: 1679.0000 - val_tn: 5128.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2594 - val_recall: 0.2591 - val_auc: 0.5733 - val_prc: 0.2851\n",
      "Epoch 79/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5433 - tp: 5506.0000 - fp: 416.0000 - tn: 26812.0000 - fn: 3570.0000 - accuracy: 0.8502 - precision: 0.9298 - recall: 0.6067 - auc: 0.9668 - prc: 0.9058 - val_loss: 2.6880 - val_tp: 587.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1682.0000 - val_accuracy: 0.2591 - val_precision: 0.2588 - val_recall: 0.2587 - val_auc: 0.6948 - val_prc: 0.3621\n",
      "Epoch 80/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5357 - tp: 5648.0000 - fp: 413.0000 - tn: 26815.0000 - fn: 3428.0000 - accuracy: 0.8508 - precision: 0.9319 - recall: 0.6223 - auc: 0.9681 - prc: 0.9108 - val_loss: 5.5302 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.4786 - val_prc: 0.2506\n",
      "Epoch 81/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5274 - tp: 5779.0000 - fp: 359.0000 - tn: 26869.0000 - fn: 3297.0000 - accuracy: 0.8625 - precision: 0.9415 - recall: 0.6367 - auc: 0.9711 - prc: 0.9185 - val_loss: 1.1284 - val_tp: 688.0000 - val_fp: 357.0000 - val_tn: 6450.0000 - val_fn: 1581.0000 - val_accuracy: 0.5214 - val_precision: 0.6584 - val_recall: 0.3032 - val_auc: 0.7702 - val_prc: 0.5205\n",
      "Epoch 82/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5161 - tp: 5846.0000 - fp: 399.0000 - tn: 26829.0000 - fn: 3230.0000 - accuracy: 0.8596 - precision: 0.9361 - recall: 0.6441 - auc: 0.9711 - prc: 0.9188 - val_loss: 2.2509 - val_tp: 463.0000 - val_fp: 1537.0000 - val_tn: 5270.0000 - val_fn: 1806.0000 - val_accuracy: 0.2217 - val_precision: 0.2315 - val_recall: 0.2041 - val_auc: 0.5605 - val_prc: 0.2731\n",
      "Epoch 83/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5107 - tp: 5873.0000 - fp: 397.0000 - tn: 26831.0000 - fn: 3203.0000 - accuracy: 0.8610 - precision: 0.9367 - recall: 0.6471 - auc: 0.9713 - prc: 0.9206 - val_loss: 5.4022 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.5084 - val_prc: 0.2574\n",
      "Epoch 84/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.5064 - tp: 6004.0000 - fp: 419.0000 - tn: 26809.0000 - fn: 3072.0000 - accuracy: 0.8688 - precision: 0.9348 - recall: 0.6615 - auc: 0.9734 - prc: 0.9238 - val_loss: 1.5960 - val_tp: 363.0000 - val_fp: 1511.0000 - val_tn: 5296.0000 - val_fn: 1906.0000 - val_accuracy: 0.2124 - val_precision: 0.1937 - val_recall: 0.1600 - val_auc: 0.6159 - val_prc: 0.2856\n",
      "Epoch 85/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4965 - tp: 6122.0000 - fp: 402.0000 - tn: 26826.0000 - fn: 2954.0000 - accuracy: 0.8682 - precision: 0.9384 - recall: 0.6745 - auc: 0.9746 - prc: 0.9272 - val_loss: 2.3257 - val_tp: 738.0000 - val_fp: 1475.0000 - val_tn: 5332.0000 - val_fn: 1531.0000 - val_accuracy: 0.3323 - val_precision: 0.3335 - val_recall: 0.3253 - val_auc: 0.5962 - val_prc: 0.3063\n",
      "Epoch 86/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.4909 - tp: 6166.0000 - fp: 413.0000 - tn: 26815.0000 - fn: 2910.0000 - accuracy: 0.8700 - precision: 0.9372 - recall: 0.6794 - auc: 0.9739 - prc: 0.9271 - val_loss: 3.4362 - val_tp: 1210.0000 - val_fp: 1059.0000 - val_tn: 5748.0000 - val_fn: 1059.0000 - val_accuracy: 0.5333 - val_precision: 0.5333 - val_recall: 0.5333 - val_auc: 0.7681 - val_prc: 0.5601\n",
      "Epoch 87/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4846 - tp: 6265.0000 - fp: 398.0000 - tn: 26830.0000 - fn: 2811.0000 - accuracy: 0.8673 - precision: 0.9403 - recall: 0.6903 - auc: 0.9742 - prc: 0.9277 - val_loss: 1.9232 - val_tp: 420.0000 - val_fp: 1634.0000 - val_tn: 5173.0000 - val_fn: 1849.0000 - val_accuracy: 0.2076 - val_precision: 0.2045 - val_recall: 0.1851 - val_auc: 0.5609 - val_prc: 0.2616\n",
      "Epoch 88/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4776 - tp: 6361.0000 - fp: 418.0000 - tn: 26810.0000 - fn: 2715.0000 - accuracy: 0.8736 - precision: 0.9383 - recall: 0.7009 - auc: 0.9750 - prc: 0.9307 - val_loss: 3.3186 - val_tp: 101.0000 - val_fp: 1975.0000 - val_tn: 4832.0000 - val_fn: 2168.0000 - val_accuracy: 0.0621 - val_precision: 0.0487 - val_recall: 0.0445 - val_auc: 0.4406 - val_prc: 0.2067\n",
      "Epoch 89/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4691 - tp: 6423.0000 - fp: 393.0000 - tn: 26835.0000 - fn: 2653.0000 - accuracy: 0.8760 - precision: 0.9423 - recall: 0.7077 - auc: 0.9772 - prc: 0.9366 - val_loss: 2.4981 - val_tp: 220.0000 - val_fp: 1836.0000 - val_tn: 4971.0000 - val_fn: 2049.0000 - val_accuracy: 0.1150 - val_precision: 0.1070 - val_recall: 0.0970 - val_auc: 0.5088 - val_prc: 0.2296\n",
      "Epoch 90/100\n",
      "91/91 [==============================] - 4s 44ms/step - loss: 0.4652 - tp: 6476.0000 - fp: 422.0000 - tn: 26806.0000 - fn: 2600.0000 - accuracy: 0.8775 - precision: 0.9388 - recall: 0.7135 - auc: 0.9766 - prc: 0.9348 - val_loss: 5.6272 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.4985 - val_prc: 0.2542\n",
      "Epoch 91/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4591 - tp: 6614.0000 - fp: 404.0000 - tn: 26824.0000 - fn: 2462.0000 - accuracy: 0.8811 - precision: 0.9424 - recall: 0.7287 - auc: 0.9785 - prc: 0.9393 - val_loss: 1.8357 - val_tp: 412.0000 - val_fp: 1455.0000 - val_tn: 5352.0000 - val_fn: 1857.0000 - val_accuracy: 0.2168 - val_precision: 0.2207 - val_recall: 0.1816 - val_auc: 0.5707 - val_prc: 0.2750\n",
      "Epoch 92/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4474 - tp: 6722.0000 - fp: 372.0000 - tn: 26856.0000 - fn: 2354.0000 - accuracy: 0.8898 - precision: 0.9476 - recall: 0.7406 - auc: 0.9805 - prc: 0.9445 - val_loss: 1.1606 - val_tp: 639.0000 - val_fp: 465.0000 - val_tn: 6342.0000 - val_fn: 1630.0000 - val_accuracy: 0.4879 - val_precision: 0.5788 - val_recall: 0.2816 - val_auc: 0.7531 - val_prc: 0.4730\n",
      "Epoch 93/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4380 - tp: 6732.0000 - fp: 378.0000 - tn: 26850.0000 - fn: 2344.0000 - accuracy: 0.8913 - precision: 0.9468 - recall: 0.7417 - auc: 0.9797 - prc: 0.9427 - val_loss: 1.4574 - val_tp: 429.0000 - val_fp: 846.0000 - val_tn: 5961.0000 - val_fn: 1840.0000 - val_accuracy: 0.3107 - val_precision: 0.3365 - val_recall: 0.1891 - val_auc: 0.6318 - val_prc: 0.3251\n",
      "Epoch 94/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4345 - tp: 6812.0000 - fp: 398.0000 - tn: 26830.0000 - fn: 2264.0000 - accuracy: 0.8897 - precision: 0.9448 - recall: 0.7506 - auc: 0.9803 - prc: 0.9456 - val_loss: 1.1561 - val_tp: 745.0000 - val_fp: 485.0000 - val_tn: 6322.0000 - val_fn: 1524.0000 - val_accuracy: 0.5011 - val_precision: 0.6057 - val_recall: 0.3283 - val_auc: 0.7591 - val_prc: 0.5016\n",
      "Epoch 95/100\n",
      "91/91 [==============================] - 4s 43ms/step - loss: 0.4292 - tp: 6847.0000 - fp: 417.0000 - tn: 26811.0000 - fn: 2229.0000 - accuracy: 0.8881 - precision: 0.9426 - recall: 0.7544 - auc: 0.9804 - prc: 0.9454 - val_loss: 5.3290 - val_tp: 588.0000 - val_fp: 1681.0000 - val_tn: 5126.0000 - val_fn: 1681.0000 - val_accuracy: 0.2591 - val_precision: 0.2591 - val_recall: 0.2591 - val_auc: 0.4676 - val_prc: 0.2473\n",
      "Epoch 96/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4193 - tp: 6927.0000 - fp: 372.0000 - tn: 26856.0000 - fn: 2149.0000 - accuracy: 0.8979 - precision: 0.9490 - recall: 0.7632 - auc: 0.9824 - prc: 0.9505 - val_loss: 9.8874 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.3599 - val_prc: 0.1846\n",
      "Epoch 97/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4154 - tp: 6998.0000 - fp: 402.0000 - tn: 26826.0000 - fn: 2078.0000 - accuracy: 0.8916 - precision: 0.9457 - recall: 0.7710 - auc: 0.9822 - prc: 0.9499 - val_loss: 17.1871 - val_tp: 91.0000 - val_fp: 2178.0000 - val_tn: 4629.0000 - val_fn: 2178.0000 - val_accuracy: 0.0401 - val_precision: 0.0401 - val_recall: 0.0401 - val_auc: 0.3601 - val_prc: 0.1846\n",
      "Epoch 98/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.4053 - tp: 7065.0000 - fp: 421.0000 - tn: 26807.0000 - fn: 2011.0000 - accuracy: 0.8932 - precision: 0.9438 - recall: 0.7784 - auc: 0.9821 - prc: 0.9501 - val_loss: 1.3856 - val_tp: 476.0000 - val_fp: 1002.0000 - val_tn: 5805.0000 - val_fn: 1793.0000 - val_accuracy: 0.3407 - val_precision: 0.3221 - val_recall: 0.2098 - val_auc: 0.6607 - val_prc: 0.3319\n",
      "Epoch 99/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.3988 - tp: 7179.0000 - fp: 372.0000 - tn: 26856.0000 - fn: 1897.0000 - accuracy: 0.9017 - precision: 0.9507 - recall: 0.7910 - auc: 0.9839 - prc: 0.9556 - val_loss: 1.3418 - val_tp: 1222.0000 - val_fp: 1009.0000 - val_tn: 5798.0000 - val_fn: 1047.0000 - val_accuracy: 0.5434 - val_precision: 0.5477 - val_recall: 0.5386 - val_auc: 0.7669 - val_prc: 0.5793\n",
      "Epoch 100/100\n",
      "91/91 [==============================] - 4s 42ms/step - loss: 0.3978 - tp: 7163.0000 - fp: 392.0000 - tn: 26836.0000 - fn: 1913.0000 - accuracy: 0.8983 - precision: 0.9481 - recall: 0.7892 - auc: 0.9825 - prc: 0.9510 - val_loss: 1.1385 - val_tp: 824.0000 - val_fp: 555.0000 - val_tn: 6252.0000 - val_fn: 1445.0000 - val_accuracy: 0.5196 - val_precision: 0.5975 - val_recall: 0.3632 - val_auc: 0.7703 - val_prc: 0.5083\n"
     ]
    }
   ],
   "source": [
    "hist_CNN_embedding = CNN_embedding.fit([x_train,x_train_meta], lbl_train, epochs=100,\n",
    "                               validation_data=([x_test,x_test_meta], lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "405ccf59-a31b-4149-8b79-b31f3d3dd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# CNN_embedding.save('CNN_embedding_model.h5') # first 100 epochs\n",
    "CNN_embedding.save('CNN_embedding_model2.h5') # Next 100 epochs (total of 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58aa09-f692-47a7-8ba1-118656848a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# CNN_embedding = load_model('CNN_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8071c0b4",
   "metadata": {
    "id": "8071c0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test = CNN_embedding.predict([x_test,x_test_meta])\n",
    "lbl_pred_CNN = np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real_CNN = np.argmax(lbl_test,axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "246698c5",
   "metadata": {
    "id": "246698c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC\n",
      "0.672  -0.672\n",
      "0.672 + -0.672\n",
      "f1_score\n",
      "0.337  -0.016\n",
      "0.337 + 0.017\n",
      "accuracy\n",
      "0.52  -0.017\n",
      "0.52 + 0.018\n",
      "precision\n",
      "0.341  -0.02\n",
      "0.341 + 0.023\n",
      "recall\n",
      "0.341  -0.014\n",
      "0.341 + 0.015\n"
     ]
    }
   ],
   "source": [
    "roc_CNN, f1_score_CNN, accuracy_CNN, precision_CNN, recall_CNN = BootstrapR2(pred_test, lbl_test, numboot=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c733a751",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26420,
     "status": "ok",
     "timestamp": 1720587409995,
     "user": {
      "displayName": "핑도도도",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "c733a751",
    "outputId": "7aacb893-0360-4059-f6f0-40739c5039c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py-params>=0.9.6 (from bert-for-tf2)\n",
      "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting params-flow>=0.8.0 (from bert-for-tf2)\n",
      "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.26.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.4)\n",
      "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30509 sha256=fffde012a5ac56a52b38af7267474bf3c7c79d2aeee6dffdcc1cb74e58249e20\n",
      "  Stored in directory: /root/.cache/pip/wheels/69/ce/32/63d802240b53e413325c5f7bc239a66d0e889d73a875ab5921\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19455 sha256=b85c588633af44e077fb15d43c90b75489d6d3ea15f6b0e230f9fadc151855ed\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/93/50/109403ddb74f3c35e76118cc9494c02986a198551629c97da6\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7892 sha256=70eca83783ab27ded1b38ebd04b44ed94e47e952d80c58c2db661553bafb9b19\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/fd/59/6697ae173bbaca0c074ff4a23f42391ceb0b7881413240435b\n",
      "Successfully built bert-for-tf2 params-flow py-params\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80f543c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12501,
     "status": "ok",
     "timestamp": 1720587422468,
     "user": {
      "displayName": "핑도도도",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "80f543c3",
    "outputId": "ff9ef258-75a4-4942-de31-1f3d704f3bac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m148.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m160.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.5 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68920d03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13678,
     "status": "ok",
     "timestamp": 1720587436129,
     "user": {
      "displayName": "핑도도도",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "68920d03",
    "outputId": "5dc27fbd-96fe-4346-98e6-0e2b40775dbe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_hub) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow_hub) (4.24.3)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow_hub)\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.18,>=2.17 (from tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
      "Collecting h5py>=3.10.0 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (16.0.6)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (23.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.58.0)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.34.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.41.2)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading optree-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading h5py-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m141.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.1/349.1 kB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, flatbuffers, optree, ml-dtypes, mdurl, h5py, tensorboard, markdown-it-py, rich, keras, tensorflow, tf-keras, tensorflow_hub\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.5.26\n",
      "    Uninstalling flatbuffers-23.5.26:\n",
      "      Successfully uninstalled flatbuffers-23.5.26\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.14.0\n",
      "    Uninstalling tensorboard-2.14.0:\n",
      "      Successfully uninstalled tensorboard-2.14.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.14.0\n",
      "    Uninstalling keras-2.14.0:\n",
      "      Successfully uninstalled keras-2.14.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.14.0\n",
      "    Uninstalling tensorflow-2.14.0:\n",
      "      Successfully uninstalled tensorflow-2.14.0\n",
      "Successfully installed flatbuffers-24.3.25 h5py-3.11.0 keras-3.4.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.0 namex-0.0.8 optree-0.12.1 rich-13.7.1 tensorboard-2.17.0 tensorflow-2.17.0 tensorflow_hub-0.16.1 tf-keras-2.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fbedd14",
   "metadata": {
    "id": "4fbedd14"
   },
   "outputs": [],
   "source": [
    "################################################################### BERT\n",
    "\n",
    "import keras\n",
    "from bert import tokenization\n",
    "from bert import bert_tokenization\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "with strategy.scope():\n",
    "    BertTokenizer = bert_tokenization.FullTokenizer\n",
    "    # BertTokenizer = tokenization.FullTokenizer\n",
    "#bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
    "\n",
    "with strategy.scope():\n",
    "    # bert_layer = hub.KerasLayer(\"EXT/DDD/ce53fe6769d2ac3a260e92555120c54e1aecbea6\",trainable=False)\n",
    "    bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True, signature=\"serving_default\", signature_outputs_as_dict=True)\n",
    "\n",
    "    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca9ba333",
   "metadata": {
    "id": "ca9ba333"
   },
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(str(text))\n",
    "\n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a49cad8",
   "metadata": {
    "id": "7a49cad8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from absl import flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36d67eaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1720587505554,
     "user": {
      "displayName": "핑도도도",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "36d67eaf",
    "outputId": "99e620a0-07a8-4ccb-a2c1-03a00e2b5bed"
   },
   "outputs": [],
   "source": [
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)\n",
    "\n",
    "with strategy.scope():\n",
    "    train_input_bert = bert_encode(x_train_txt, tokenizer)\n",
    "    test_input_bert = bert_encode(x_test_txt, tokenizer)\n",
    "\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5438ccc1",
   "metadata": {
    "id": "5438ccc1"
   },
   "outputs": [],
   "source": [
    "def create_beart_model(bert_layer, max_len=512, data_meta=None):\n",
    "    if data_meta is None:\n",
    "        data_meta = np.zeros((1, 1, 1))  # 또는 적절한 기본값\n",
    "\n",
    "    # meta input\n",
    "    input_meta=keras.layers.Input(shape=data_meta.shape[1:],dtype='float64',name=\"input_meta\")\n",
    "    layer_meta=keras.layers.Conv1D(filters=16,\n",
    "                                  kernel_size=2,\n",
    "                                  activation='relu',\n",
    "                                  padding=\"same\",\n",
    "                                  strides=(1),\n",
    "                                name=\"cov_meta.1\")(input_meta)\n",
    "    layer_meta=keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "    layer_meta = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_meta,layer_meta)\n",
    "    layer_meta=keras.layers.GlobalAveragePooling1D(name=\"avg_meta\")(layer_meta)\n",
    "\n",
    "    # txt input\n",
    "\n",
    "    input_word_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    # BERT layer\n",
    "    with strategy.scope():\n",
    "        BertTokenizer = bert_tokenization.FullTokenizer\n",
    "        # BertTokenizer = tokenization.FullTokenizer\n",
    "        bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
    "        vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "        to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "        tokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "    layer_bert = sequence_output[:, :, :]\n",
    "\n",
    "    layer_bert=keras.layers.GlobalAveragePooling1D(name=\"avg1\")(layer_bert)\n",
    "\n",
    "\n",
    "\n",
    "    # layer_last = keras.layers.Dense(200, activation='relu')(clf_output)\n",
    "    # # layer_last =keras.layers.BatchNormalization()(layer_last)\n",
    "    # layer_last =keras.layers.Dropout(0.2)(layer_last)\n",
    "\n",
    "    # hybrid\n",
    "    layer_last =keras.layers.Concatenate(axis=-1)([layer_meta,layer_bert])\n",
    "    layer_last=keras.layers.Flatten()(layer_last)\n",
    "    layer_last =keras.layers.Dense(100,activation=\"relu\",name=\"dens_1\")(layer_last)\n",
    "    layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "    output = keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "    bert_model = keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids,input_meta],\n",
    "                        outputs=output)\n",
    "    bert_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "    return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe18dfa4",
   "metadata": {
    "id": "fe18dfa4"
   },
   "outputs": [],
   "source": [
    "model_bert = create_beart_model(bert_layer, data_meta=x_train_meta_re)\n",
    "train_bert_new = train_input_bert + (x_train_meta_re,)\n",
    "test_bert_new = test_input_bert + (x_test_meta_re,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7LgzwXdkApYp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1720587626767,
     "user": {
      "displayName": "핑도도도",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "7LgzwXdkApYp",
    "outputId": "cdaad678-db73-469a-af64-b9d8f0386714"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9076, 512)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bert_new[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "jVYUk5_vBGvY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1720587731099,
     "user": {
      "displayName": "핑도도도",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "jVYUk5_vBGvY",
    "outputId": "99e190cd-719a-46ae-df3b-6f7ab9a1e13e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "p_QhdQ7GB9Wq",
   "metadata": {
    "id": "p_QhdQ7GB9Wq"
   },
   "outputs": [],
   "source": [
    "def convert_booleans(arr):\n",
    "    \"\"\"Recursively converts boolean values to integers in a nested array.\"\"\"\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        if arr.dtype == bool:\n",
    "            return arr.astype(int)\n",
    "        else:\n",
    "            return np.array([convert_booleans(item) for item in arr])\n",
    "    else:\n",
    "        return arr\n",
    "\n",
    "# Apply the recursive conversion to the fourth array\n",
    "train_bert_new[3][:,:8,0] = convert_booleans(train_bert_new[3][:,:8,0])\n",
    "test_bert_new[3][:,:8,0] = convert_booleans(test_bert_new[3][:,:8,0])\n",
    "# Convert the entire array to float32\n",
    "train_bert_new[3][:,:8,0] = train_bert_new[3][:,:8,0].astype(np.float32)\n",
    "test_bert_new[3][:,:8,0] = test_bert_new[3][:,:8,0].astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "NI6Pf6UiB85v",
   "metadata": {
    "id": "NI6Pf6UiB85v"
   },
   "outputs": [],
   "source": [
    "def convert_to_float32(arr):\n",
    "    \"\"\"Recursively converts elements of a nested array to float32.\"\"\"\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        return arr.astype(np.float32)\n",
    "    elif isinstance(arr, (list, tuple)):\n",
    "        return type(arr)(convert_to_float32(item) for item in arr)\n",
    "    # Handle the case where a single float value is encountered\n",
    "    elif isinstance(arr, float):\n",
    "        return np.float32(arr)\n",
    "    else:\n",
    "        return arr\n",
    "\n",
    "# Apply the conversion to all arrays in train_bert_new\n",
    "train_bert_new = convert_to_float32(train_bert_new)\n",
    "test_bert_new = convert_to_float32(test_bert_new)\n",
    "\n",
    "train_bert_new = tuple(tf.convert_to_tensor(arr) for arr in train_bert_new)\n",
    "test_bert_new = tuple(tf.convert_to_tensor(arr) for arr in test_bert_new)\n",
    "\n",
    "# ... rest of your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6559a84c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "executionInfo": {
     "elapsed": 884573,
     "status": "error",
     "timestamp": 1720589161577,
     "user": {
      "displayName": "핑도도도",
      "userId": "04659225726467828001"
     },
     "user_tz": -540
    },
    "id": "6559a84c",
    "outputId": "2338e892-8fbe-4749-ac8e-b2441398e6b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.2473 - accuracy: 0.3667WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7d4adaa51120> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x7d4adaa51120>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7d4adaa51120> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x7d4adaa51120>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7d4adaa51120> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x7d4adaa51120>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.2473 - accuracy: 0.3667 - val_loss: 1.3485 - val_accuracy: 0.3028\n",
      "Epoch 2/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2554 - accuracy: 0.3691 - val_loss: 1.3180 - val_accuracy: 0.3596\n",
      "Epoch 3/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2447 - accuracy: 0.3712 - val_loss: 1.3340 - val_accuracy: 0.3257\n",
      "Epoch 4/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2472 - accuracy: 0.3712 - val_loss: 1.3680 - val_accuracy: 0.2662\n",
      "Epoch 5/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2461 - accuracy: 0.3660 - val_loss: 1.3323 - val_accuracy: 0.3402\n",
      "Epoch 6/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2442 - accuracy: 0.3651 - val_loss: 1.3611 - val_accuracy: 0.2785\n",
      "Epoch 7/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.2462 - accuracy: 0.3547 - val_loss: 1.3208 - val_accuracy: 0.3508\n",
      "Epoch 8/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2376 - accuracy: 0.3747 - val_loss: 1.2978 - val_accuracy: 0.3808\n",
      "Epoch 9/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2436 - accuracy: 0.3736 - val_loss: 1.3099 - val_accuracy: 0.3680\n",
      "Epoch 10/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.2349 - accuracy: 0.3760 - val_loss: 1.3438 - val_accuracy: 0.3169\n",
      "Epoch 11/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2384 - accuracy: 0.3678 - val_loss: 1.2957 - val_accuracy: 0.3856\n",
      "Epoch 12/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2363 - accuracy: 0.3754 - val_loss: 1.3310 - val_accuracy: 0.3270\n",
      "Epoch 13/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.2247 - accuracy: 0.3667 - val_loss: 1.3197 - val_accuracy: 0.3433\n",
      "Epoch 14/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2233 - accuracy: 0.3679 - val_loss: 1.2950 - val_accuracy: 0.3799\n",
      "Epoch 15/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2226 - accuracy: 0.3691 - val_loss: 1.3197 - val_accuracy: 0.3376\n",
      "Epoch 16/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2219 - accuracy: 0.3727 - val_loss: 1.3215 - val_accuracy: 0.3416\n",
      "Epoch 17/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2201 - accuracy: 0.3796 - val_loss: 1.3110 - val_accuracy: 0.3574\n",
      "Epoch 18/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2173 - accuracy: 0.3802 - val_loss: 1.3001 - val_accuracy: 0.3724\n",
      "Epoch 19/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2135 - accuracy: 0.3769 - val_loss: 1.2880 - val_accuracy: 0.3856\n",
      "Epoch 20/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2126 - accuracy: 0.3733 - val_loss: 1.3365 - val_accuracy: 0.3178\n",
      "Epoch 21/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2138 - accuracy: 0.3753 - val_loss: 1.2861 - val_accuracy: 0.3887\n",
      "Epoch 22/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2050 - accuracy: 0.3862 - val_loss: 1.2764 - val_accuracy: 0.4006\n",
      "Epoch 23/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2009 - accuracy: 0.3783 - val_loss: 1.2899 - val_accuracy: 0.3817\n",
      "Epoch 24/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2061 - accuracy: 0.3785 - val_loss: 1.3485 - val_accuracy: 0.2931\n",
      "Epoch 25/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.2010 - accuracy: 0.3689 - val_loss: 1.3300 - val_accuracy: 0.3151\n",
      "Epoch 26/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1979 - accuracy: 0.3791 - val_loss: 1.2817 - val_accuracy: 0.3975\n",
      "Epoch 27/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1929 - accuracy: 0.3931 - val_loss: 1.2754 - val_accuracy: 0.3989\n",
      "Epoch 28/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1880 - accuracy: 0.3831 - val_loss: 1.2691 - val_accuracy: 0.3975\n",
      "Epoch 29/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.1913 - accuracy: 0.3903 - val_loss: 1.3156 - val_accuracy: 0.3244\n",
      "Epoch 30/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1871 - accuracy: 0.3841 - val_loss: 1.2937 - val_accuracy: 0.3667\n",
      "Epoch 31/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1819 - accuracy: 0.3812 - val_loss: 1.3055 - val_accuracy: 0.3394\n",
      "Epoch 32/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.1791 - accuracy: 0.3898 - val_loss: 1.3025 - val_accuracy: 0.3521\n",
      "Epoch 33/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1806 - accuracy: 0.3870 - val_loss: 1.2513 - val_accuracy: 0.4204\n",
      "Epoch 34/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1779 - accuracy: 0.3867 - val_loss: 1.2530 - val_accuracy: 0.4156\n",
      "Epoch 35/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1771 - accuracy: 0.3824 - val_loss: 1.2616 - val_accuracy: 0.4081\n",
      "Epoch 36/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1650 - accuracy: 0.4001 - val_loss: 1.2367 - val_accuracy: 0.4262\n",
      "Epoch 37/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1673 - accuracy: 0.3937 - val_loss: 1.2761 - val_accuracy: 0.3817\n",
      "Epoch 38/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1672 - accuracy: 0.4024 - val_loss: 1.3136 - val_accuracy: 0.3235\n",
      "Epoch 39/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1566 - accuracy: 0.3911 - val_loss: 1.2682 - val_accuracy: 0.4019\n",
      "Epoch 40/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1566 - accuracy: 0.3915 - val_loss: 1.2270 - val_accuracy: 0.4337\n",
      "Epoch 41/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1652 - accuracy: 0.3946 - val_loss: 1.2490 - val_accuracy: 0.4174\n",
      "Epoch 42/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1606 - accuracy: 0.3864 - val_loss: 1.2490 - val_accuracy: 0.4160\n",
      "Epoch 43/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1471 - accuracy: 0.4026 - val_loss: 1.2651 - val_accuracy: 0.3896\n",
      "Epoch 44/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1510 - accuracy: 0.3994 - val_loss: 1.2616 - val_accuracy: 0.3958\n",
      "Epoch 45/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1470 - accuracy: 0.4044 - val_loss: 1.2381 - val_accuracy: 0.4227\n",
      "Epoch 46/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1356 - accuracy: 0.4027 - val_loss: 1.2554 - val_accuracy: 0.3975\n",
      "Epoch 47/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1417 - accuracy: 0.4065 - val_loss: 1.2246 - val_accuracy: 0.4372\n",
      "Epoch 48/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1399 - accuracy: 0.3975 - val_loss: 1.2340 - val_accuracy: 0.4222\n",
      "Epoch 49/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.1282 - accuracy: 0.4123 - val_loss: 1.2519 - val_accuracy: 0.3980\n",
      "Epoch 50/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1345 - accuracy: 0.3993 - val_loss: 1.2295 - val_accuracy: 0.4196\n",
      "Epoch 51/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1229 - accuracy: 0.4006 - val_loss: 1.2358 - val_accuracy: 0.4138\n",
      "Epoch 52/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1292 - accuracy: 0.4095 - val_loss: 1.2197 - val_accuracy: 0.4354\n",
      "Epoch 53/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1201 - accuracy: 0.4155 - val_loss: 1.2110 - val_accuracy: 0.4407\n",
      "Epoch 54/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1147 - accuracy: 0.4131 - val_loss: 1.2150 - val_accuracy: 0.4350\n",
      "Epoch 55/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1200 - accuracy: 0.4102 - val_loss: 1.2239 - val_accuracy: 0.4174\n",
      "Epoch 56/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1103 - accuracy: 0.4162 - val_loss: 1.2320 - val_accuracy: 0.4077\n",
      "Epoch 57/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1061 - accuracy: 0.4089 - val_loss: 1.1914 - val_accuracy: 0.4544\n",
      "Epoch 58/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1229 - accuracy: 0.4148 - val_loss: 1.2477 - val_accuracy: 0.3781\n",
      "Epoch 59/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1124 - accuracy: 0.4067 - val_loss: 1.2159 - val_accuracy: 0.4209\n",
      "Epoch 60/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1011 - accuracy: 0.4138 - val_loss: 1.1890 - val_accuracy: 0.4601\n",
      "Epoch 61/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.1114 - accuracy: 0.4187 - val_loss: 1.2157 - val_accuracy: 0.4244\n",
      "Epoch 62/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1000 - accuracy: 0.4142 - val_loss: 1.1911 - val_accuracy: 0.4526\n",
      "Epoch 63/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1003 - accuracy: 0.4177 - val_loss: 1.1963 - val_accuracy: 0.4465\n",
      "Epoch 64/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.1013 - accuracy: 0.4197 - val_loss: 1.1867 - val_accuracy: 0.4614\n",
      "Epoch 65/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0895 - accuracy: 0.4257 - val_loss: 1.1869 - val_accuracy: 0.4553\n",
      "Epoch 66/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0828 - accuracy: 0.4287 - val_loss: 1.1761 - val_accuracy: 0.4703\n",
      "Epoch 67/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0800 - accuracy: 0.4177 - val_loss: 1.1907 - val_accuracy: 0.4403\n",
      "Epoch 68/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0686 - accuracy: 0.4376 - val_loss: 1.1798 - val_accuracy: 0.4561\n",
      "Epoch 69/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0703 - accuracy: 0.4232 - val_loss: 1.1945 - val_accuracy: 0.4306\n",
      "Epoch 70/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0776 - accuracy: 0.4310 - val_loss: 1.1678 - val_accuracy: 0.4703\n",
      "Epoch 71/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0737 - accuracy: 0.4353 - val_loss: 1.1716 - val_accuracy: 0.4619\n",
      "Epoch 72/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0699 - accuracy: 0.4252 - val_loss: 1.1504 - val_accuracy: 0.4927\n",
      "Epoch 73/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0774 - accuracy: 0.4312 - val_loss: 1.1417 - val_accuracy: 0.4985\n",
      "Epoch 74/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0620 - accuracy: 0.4420 - val_loss: 1.1548 - val_accuracy: 0.4689\n",
      "Epoch 75/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0590 - accuracy: 0.4340 - val_loss: 1.1335 - val_accuracy: 0.4958\n",
      "Epoch 76/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0478 - accuracy: 0.4613 - val_loss: 1.1350 - val_accuracy: 0.5002\n",
      "Epoch 77/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0625 - accuracy: 0.4290 - val_loss: 1.1520 - val_accuracy: 0.4645\n",
      "Epoch 78/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0697 - accuracy: 0.4422 - val_loss: 1.1227 - val_accuracy: 0.5104\n",
      "Epoch 79/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0597 - accuracy: 0.4552 - val_loss: 1.1402 - val_accuracy: 0.4786\n",
      "Epoch 80/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0637 - accuracy: 0.4407 - val_loss: 1.1426 - val_accuracy: 0.4923\n",
      "Epoch 81/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0475 - accuracy: 0.4466 - val_loss: 1.1300 - val_accuracy: 0.4967\n",
      "Epoch 82/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0474 - accuracy: 0.4490 - val_loss: 1.0948 - val_accuracy: 0.5412\n",
      "Epoch 83/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0460 - accuracy: 0.4543 - val_loss: 1.1178 - val_accuracy: 0.5033\n",
      "Epoch 84/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0477 - accuracy: 0.4629 - val_loss: 1.1036 - val_accuracy: 0.5289\n",
      "Epoch 85/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0428 - accuracy: 0.4517 - val_loss: 1.1167 - val_accuracy: 0.5090\n",
      "Epoch 86/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0320 - accuracy: 0.4601 - val_loss: 1.0999 - val_accuracy: 0.5139\n",
      "Epoch 87/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0351 - accuracy: 0.4520 - val_loss: 1.0975 - val_accuracy: 0.5289\n",
      "Epoch 88/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0224 - accuracy: 0.4727 - val_loss: 1.0819 - val_accuracy: 0.5412\n",
      "Epoch 89/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0360 - accuracy: 0.4677 - val_loss: 1.1078 - val_accuracy: 0.5042\n",
      "Epoch 90/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0391 - accuracy: 0.4641 - val_loss: 1.1085 - val_accuracy: 0.5104\n",
      "Epoch 91/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0238 - accuracy: 0.4633 - val_loss: 1.0721 - val_accuracy: 0.5465\n",
      "Epoch 92/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0174 - accuracy: 0.4769 - val_loss: 1.0591 - val_accuracy: 0.5668\n",
      "Epoch 93/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0199 - accuracy: 0.4663 - val_loss: 1.0655 - val_accuracy: 0.5628\n",
      "Epoch 94/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0128 - accuracy: 0.4791 - val_loss: 1.0706 - val_accuracy: 0.5505\n",
      "Epoch 95/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.0046 - accuracy: 0.4757 - val_loss: 1.0568 - val_accuracy: 0.5461\n",
      "Epoch 96/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0000 - accuracy: 0.4815 - val_loss: 1.0688 - val_accuracy: 0.5394\n",
      "Epoch 97/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0090 - accuracy: 0.4857 - val_loss: 1.0875 - val_accuracy: 0.4993\n",
      "Epoch 98/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 1.0220 - accuracy: 0.4715 - val_loss: 1.0475 - val_accuracy: 0.5663\n",
      "Epoch 99/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0114 - accuracy: 0.4878 - val_loss: 1.0641 - val_accuracy: 0.5500\n",
      "Epoch 100/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0132 - accuracy: 0.4898 - val_loss: 1.0759 - val_accuracy: 0.5143\n",
      "Epoch 101/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 1.0053 - accuracy: 0.4841 - val_loss: 1.0415 - val_accuracy: 0.5738\n",
      "Epoch 102/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9938 - accuracy: 0.4914 - val_loss: 1.0573 - val_accuracy: 0.5394\n",
      "Epoch 103/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9929 - accuracy: 0.4884 - val_loss: 1.0544 - val_accuracy: 0.5522\n",
      "Epoch 104/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9918 - accuracy: 0.4949 - val_loss: 1.0430 - val_accuracy: 0.5527\n",
      "Epoch 105/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9958 - accuracy: 0.4837 - val_loss: 1.0395 - val_accuracy: 0.5535\n",
      "Epoch 106/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9969 - accuracy: 0.4948 - val_loss: 1.0413 - val_accuracy: 0.5483\n",
      "Epoch 107/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9962 - accuracy: 0.4924 - val_loss: 1.0430 - val_accuracy: 0.5474\n",
      "Epoch 108/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9872 - accuracy: 0.5013 - val_loss: 1.0424 - val_accuracy: 0.5646\n",
      "Epoch 109/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9800 - accuracy: 0.4949 - val_loss: 1.0221 - val_accuracy: 0.5628\n",
      "Epoch 110/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9755 - accuracy: 0.5085 - val_loss: 1.0245 - val_accuracy: 0.5562\n",
      "Epoch 111/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9842 - accuracy: 0.5048 - val_loss: 1.0238 - val_accuracy: 0.5773\n",
      "Epoch 112/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9939 - accuracy: 0.5064 - val_loss: 1.0250 - val_accuracy: 0.5840\n",
      "Epoch 113/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9838 - accuracy: 0.5017 - val_loss: 1.0078 - val_accuracy: 0.5862\n",
      "Epoch 114/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9774 - accuracy: 0.5133 - val_loss: 1.0068 - val_accuracy: 0.5765\n",
      "Epoch 115/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9759 - accuracy: 0.5045 - val_loss: 0.9918 - val_accuracy: 0.6069\n",
      "Epoch 116/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9678 - accuracy: 0.5182 - val_loss: 1.0171 - val_accuracy: 0.5685\n",
      "Epoch 117/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9761 - accuracy: 0.5132 - val_loss: 1.0206 - val_accuracy: 0.5721\n",
      "Epoch 118/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9857 - accuracy: 0.5113 - val_loss: 1.0208 - val_accuracy: 0.5822\n",
      "Epoch 119/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9762 - accuracy: 0.5087 - val_loss: 0.9628 - val_accuracy: 0.6271\n",
      "Epoch 120/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9704 - accuracy: 0.5315 - val_loss: 0.9600 - val_accuracy: 0.6254\n",
      "Epoch 121/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9754 - accuracy: 0.5140 - val_loss: 0.9857 - val_accuracy: 0.5919\n",
      "Epoch 122/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9746 - accuracy: 0.5231 - val_loss: 0.9880 - val_accuracy: 0.6086\n",
      "Epoch 123/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9526 - accuracy: 0.5392 - val_loss: 0.9622 - val_accuracy: 0.6113\n",
      "Epoch 124/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9538 - accuracy: 0.5209 - val_loss: 0.9525 - val_accuracy: 0.6360\n",
      "Epoch 125/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9715 - accuracy: 0.5375 - val_loss: 0.9782 - val_accuracy: 0.5840\n",
      "Epoch 126/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9640 - accuracy: 0.5192 - val_loss: 0.9540 - val_accuracy: 0.6148\n",
      "Epoch 127/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.9515 - accuracy: 0.5374 - val_loss: 0.9784 - val_accuracy: 0.5800\n",
      "Epoch 128/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9393 - accuracy: 0.5360 - val_loss: 0.9297 - val_accuracy: 0.6452\n",
      "Epoch 129/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9543 - accuracy: 0.5372 - val_loss: 0.9536 - val_accuracy: 0.6232\n",
      "Epoch 130/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9424 - accuracy: 0.5355 - val_loss: 0.9189 - val_accuracy: 0.6448\n",
      "Epoch 131/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9302 - accuracy: 0.5364 - val_loss: 0.9353 - val_accuracy: 0.6465\n",
      "Epoch 132/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9448 - accuracy: 0.5534 - val_loss: 0.9432 - val_accuracy: 0.6188\n",
      "Epoch 133/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9408 - accuracy: 0.5437 - val_loss: 0.9294 - val_accuracy: 0.6421\n",
      "Epoch 134/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9307 - accuracy: 0.5414 - val_loss: 0.9198 - val_accuracy: 0.6514\n",
      "Epoch 135/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9433 - accuracy: 0.5466 - val_loss: 0.9675 - val_accuracy: 0.5707\n",
      "Epoch 136/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9336 - accuracy: 0.5470 - val_loss: 0.9063 - val_accuracy: 0.6505\n",
      "Epoch 137/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9460 - accuracy: 0.5516 - val_loss: 0.9265 - val_accuracy: 0.6316\n",
      "Epoch 138/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9257 - accuracy: 0.5527 - val_loss: 0.9077 - val_accuracy: 0.6474\n",
      "Epoch 139/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9363 - accuracy: 0.5604 - val_loss: 0.9119 - val_accuracy: 0.6483\n",
      "Epoch 140/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9300 - accuracy: 0.5529 - val_loss: 0.9313 - val_accuracy: 0.6311\n",
      "Epoch 141/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9256 - accuracy: 0.5587 - val_loss: 0.9329 - val_accuracy: 0.6333\n",
      "Epoch 142/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9116 - accuracy: 0.5512 - val_loss: 0.9210 - val_accuracy: 0.6280\n",
      "Epoch 143/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9237 - accuracy: 0.5483 - val_loss: 0.8829 - val_accuracy: 0.6721\n",
      "Epoch 144/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9117 - accuracy: 0.5735 - val_loss: 0.8924 - val_accuracy: 0.6589\n",
      "Epoch 145/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9172 - accuracy: 0.5690 - val_loss: 0.8824 - val_accuracy: 0.6576\n",
      "Epoch 146/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9099 - accuracy: 0.5769 - val_loss: 0.9117 - val_accuracy: 0.6333\n",
      "Epoch 147/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9121 - accuracy: 0.5598 - val_loss: 0.8973 - val_accuracy: 0.6399\n",
      "Epoch 148/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9142 - accuracy: 0.5726 - val_loss: 0.9026 - val_accuracy: 0.6457\n",
      "Epoch 149/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9109 - accuracy: 0.5743 - val_loss: 0.8918 - val_accuracy: 0.6571\n",
      "Epoch 150/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9072 - accuracy: 0.5626 - val_loss: 0.9046 - val_accuracy: 0.6346\n",
      "Epoch 151/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9096 - accuracy: 0.5675 - val_loss: 0.8918 - val_accuracy: 0.6655\n",
      "Epoch 152/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9120 - accuracy: 0.5823 - val_loss: 0.9094 - val_accuracy: 0.6523\n",
      "Epoch 153/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8939 - accuracy: 0.5737 - val_loss: 0.8600 - val_accuracy: 0.6721\n",
      "Epoch 154/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.9017 - accuracy: 0.5757 - val_loss: 0.8938 - val_accuracy: 0.6408\n",
      "Epoch 155/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8926 - accuracy: 0.5836 - val_loss: 0.8540 - val_accuracy: 0.6818\n",
      "Epoch 156/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8947 - accuracy: 0.5821 - val_loss: 0.8656 - val_accuracy: 0.6730\n",
      "Epoch 157/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8981 - accuracy: 0.5756 - val_loss: 0.8426 - val_accuracy: 0.7034\n",
      "Epoch 158/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8886 - accuracy: 0.5931 - val_loss: 0.8465 - val_accuracy: 0.6880\n",
      "Epoch 159/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8868 - accuracy: 0.5867 - val_loss: 0.8515 - val_accuracy: 0.6897\n",
      "Epoch 160/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8841 - accuracy: 0.5957 - val_loss: 0.8517 - val_accuracy: 0.6655\n",
      "Epoch 161/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8717 - accuracy: 0.5989 - val_loss: 0.8341 - val_accuracy: 0.6933\n",
      "Epoch 162/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8965 - accuracy: 0.5878 - val_loss: 0.8831 - val_accuracy: 0.6633\n",
      "Epoch 163/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8795 - accuracy: 0.5864 - val_loss: 0.8544 - val_accuracy: 0.6814\n",
      "Epoch 164/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8839 - accuracy: 0.6020 - val_loss: 0.8602 - val_accuracy: 0.6589\n",
      "Epoch 165/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8850 - accuracy: 0.5987 - val_loss: 0.8280 - val_accuracy: 0.7003\n",
      "Epoch 166/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8615 - accuracy: 0.6058 - val_loss: 0.8310 - val_accuracy: 0.6752\n",
      "Epoch 167/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8735 - accuracy: 0.6081 - val_loss: 0.8309 - val_accuracy: 0.6598\n",
      "Epoch 168/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8793 - accuracy: 0.5956 - val_loss: 0.8474 - val_accuracy: 0.6628\n",
      "Epoch 169/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8668 - accuracy: 0.6003 - val_loss: 0.8337 - val_accuracy: 0.6844\n",
      "Epoch 170/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8446 - accuracy: 0.6105 - val_loss: 0.8081 - val_accuracy: 0.6959\n",
      "Epoch 171/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8575 - accuracy: 0.6177 - val_loss: 0.8090 - val_accuracy: 0.7021\n",
      "Epoch 172/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8679 - accuracy: 0.6171 - val_loss: 0.8243 - val_accuracy: 0.6972\n",
      "Epoch 173/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8577 - accuracy: 0.6141 - val_loss: 0.8121 - val_accuracy: 0.6866\n",
      "Epoch 174/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8611 - accuracy: 0.6084 - val_loss: 0.7854 - val_accuracy: 0.6972\n",
      "Epoch 175/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8527 - accuracy: 0.6133 - val_loss: 0.8054 - val_accuracy: 0.6915\n",
      "Epoch 176/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8404 - accuracy: 0.6226 - val_loss: 0.8001 - val_accuracy: 0.6840\n",
      "Epoch 177/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8468 - accuracy: 0.6114 - val_loss: 0.8013 - val_accuracy: 0.7034\n",
      "Epoch 178/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8642 - accuracy: 0.6125 - val_loss: 0.8371 - val_accuracy: 0.6699\n",
      "Epoch 179/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8410 - accuracy: 0.6254 - val_loss: 0.7863 - val_accuracy: 0.7082\n",
      "Epoch 180/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8561 - accuracy: 0.6136 - val_loss: 0.8167 - val_accuracy: 0.6818\n",
      "Epoch 181/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8467 - accuracy: 0.6186 - val_loss: 0.7958 - val_accuracy: 0.6985\n",
      "Epoch 182/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8317 - accuracy: 0.6233 - val_loss: 0.7884 - val_accuracy: 0.7131\n",
      "Epoch 183/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8439 - accuracy: 0.6308 - val_loss: 0.8021 - val_accuracy: 0.6906\n",
      "Epoch 184/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8347 - accuracy: 0.6283 - val_loss: 0.7663 - val_accuracy: 0.7078\n",
      "Epoch 185/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8280 - accuracy: 0.6396 - val_loss: 0.8000 - val_accuracy: 0.6778\n",
      "Epoch 186/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8353 - accuracy: 0.6252 - val_loss: 0.7719 - val_accuracy: 0.7113\n",
      "Epoch 187/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8337 - accuracy: 0.6320 - val_loss: 0.7580 - val_accuracy: 0.7135\n",
      "Epoch 188/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8280 - accuracy: 0.6381 - val_loss: 0.7677 - val_accuracy: 0.7003\n",
      "Epoch 189/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8305 - accuracy: 0.6381 - val_loss: 0.7562 - val_accuracy: 0.7153\n",
      "Epoch 190/200\n",
      "91/91 [==============================] - 111s 1s/step - loss: 0.8352 - accuracy: 0.6436 - val_loss: 0.7716 - val_accuracy: 0.6968\n",
      "Epoch 191/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8227 - accuracy: 0.6475 - val_loss: 0.7671 - val_accuracy: 0.7025\n",
      "Epoch 192/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8169 - accuracy: 0.6495 - val_loss: 0.7780 - val_accuracy: 0.6977\n",
      "Epoch 193/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8221 - accuracy: 0.6440 - val_loss: 0.7576 - val_accuracy: 0.7087\n",
      "Epoch 194/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8144 - accuracy: 0.6540 - val_loss: 0.7660 - val_accuracy: 0.7140\n",
      "Epoch 195/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8187 - accuracy: 0.6505 - val_loss: 0.7490 - val_accuracy: 0.7113\n",
      "Epoch 196/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8127 - accuracy: 0.6414 - val_loss: 0.7426 - val_accuracy: 0.7228\n",
      "Epoch 197/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8125 - accuracy: 0.6562 - val_loss: 0.7354 - val_accuracy: 0.7237\n",
      "Epoch 198/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8018 - accuracy: 0.6639 - val_loss: 0.7251 - val_accuracy: 0.7290\n",
      "Epoch 199/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8055 - accuracy: 0.6532 - val_loss: 0.7471 - val_accuracy: 0.7201\n",
      "Epoch 200/200\n",
      "91/91 [==============================] - 110s 1s/step - loss: 0.8088 - accuracy: 0.6492 - val_loss: 0.7403 - val_accuracy: 0.7404\n"
     ]
    }
   ],
   "source": [
    "history = model_bert.fit(train_bert_new,lbl_train,\n",
    "                              validation_data=(test_bert_new,lbl_test),\n",
    "                              epochs=200, # 원래 350\n",
    "                              class_weight=class_weight,\n",
    "                              batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "706892d7-0302-4bd6-8533-fb56d4fd0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "# model_bert.save('BERT_embedding_model.h5') # first 100 epochs\n",
    "model_bert.save('BERT_embedding_model2.h5') # next 200 epochs (total 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6621bf9e-e316-4e20-b238-a3cb53b1f76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "from keras.models import load_model\n",
    "import tensorflow_hub\n",
    "\n",
    "with keras.utils.custom_object_scope({'KerasLayer': tensorflow_hub.KerasLayer}):\n",
    "    model_bert = load_model('BERT_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "uOhIEz17oYRT",
   "metadata": {
    "id": "uOhIEz17oYRT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7d4adaa50680> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x7d4adaa50680>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7d4adaa50680> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x7d4adaa50680>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7d4adaa50680> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x7d4adaa50680>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "71/71 [==============================] - 24s 328ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test=model_bert.predict(test_bert_new)\n",
    "lbl_pred_BERT=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real_BERT=np.argmax(lbl_test,axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5cb18e3",
   "metadata": {
    "id": "b5cb18e3"
   },
   "outputs": [],
   "source": [
    "# confusion matrix를 만들어주는 함수\n",
    "\n",
    "def display_confusion_matrix(real, pred):\n",
    "    classfi_report = classification_report(real, pred, output_dict=True)\n",
    "    # save to array\n",
    "    accuracy = accuracy_score(real, pred)\n",
    "    precision = classfi_report['macro avg']['precision']\n",
    "    recall = classfi_report['macro avg']['recall']\n",
    "    f1_score = classfi_report['macro avg']['f1-score']\n",
    "    Con_matrix = confusion_matrix(real, pred)\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=Con_matrix,\n",
    "                                    show_absolute=True,\n",
    "                                    show_normed=True,\n",
    "                                    colorbar=True,\n",
    "                                    figsize=(n_class,n_class) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898c267-2f69-41e5-b383-f8e8f6da4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# CNN_embedding = load_model('CNN_embedding_model.h5')\n",
    "pred_test = CNN_embedding.predict([x_test,x_test_meta])\n",
    "lbl_pred_CNN = np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real_CNN = np.argmax(lbl_test,axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "524f6075",
   "metadata": {
    "id": "524f6075"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lbl_real_LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display_confusion_matrix(\u001b[43mlbl_real_LSTM\u001b[49m, lbl_pred_LSTM)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lbl_real_LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "display_confusion_matrix(lbl_real_LSTM, lbl_pred_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1725e56-e6d8-4592-b314-e8fd1244cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(lbl_real_GRU, lbl_pred_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a132c2-e815-4422-9466-d876690eb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(lbl_real_CNN, lbl_pred_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fb40b05-1143-4a1b-9c47-e938da1603a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC\n",
      "0.919  -0.919\n",
      "0.919 + -0.919\n",
      "f1_score\n",
      "0.601  -0.025\n",
      "0.601 + 0.024\n",
      "accuracy\n",
      "0.74  -0.015\n",
      "0.74 + 0.015\n",
      "precision\n",
      "0.603  -0.025\n",
      "0.603 + 0.025\n",
      "recall\n",
      "0.606  -0.026\n",
      "0.606 + 0.026\n"
     ]
    }
   ],
   "source": [
    "roc_BERT, f1_score_BERT, accuracy_BERT, precision_BERT, recall_BERT =BootstrapR2(pred_test, lbl_test, numboot=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc3330d9-55e0-423d-b974-9dd33f311e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_confusion_matrix(lbl_real_BERT, lbl_pred_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748a413-a3ae-4b13-8d20-21c048514e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
